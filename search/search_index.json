{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RKE2, also known as RKE Government, is Rancher's next-generation Kubernetes distribution. It is a fully conformant Kubernetes distribution that focuses on security and compliance within the U.S. Federal Government sector. To meet these goals, RKE2 does the following: Provides defaults and configuration options that allow clusters to pass the CIS Kubernetes Benchmark v1.6 with minimal operator intervention Enables FIPS 140-2 compliance Regularly scans components for CVEs using trivy in our build pipeline How is this different from RKE or K3s? \u00b6 RKE2 combines the best-of-both-worlds from the 1.x version of RKE (hereafter referred to as RKE1) and K3s. From K3s, it inherits the usability, ease-of-operations, and deployment model. From RKE1, it inherits close alignment with upstream Kubernetes. In places K3s has diverged from upstream Kubernetes in order to optimize for edge deployments, but RKE1 and RKE2 can stay closely aligned with upstream. Importantly, RKE2 does not rely on Docker as RKE1 does. RKE1 leveraged Docker for deploying and managing the control plane components as well as the container runtime for Kubernetes. RKE2 launches control plane components as static pods, managed by the kubelet. The embedded container runtime is containerd. Why two names? \u00b6 It is known as RKE Government in order to convey the primary use cases and sector it currently targets. It is also known as RKE 2 as it is the next iteration of the Rancher Kubernetes Engine for datacenter use cases. Security \u00b6 Rancher Labs supports responsible disclosure and endeavors to resolve security issues in a reasonable timeframe. To report a security vulnerability, email security@rancher.com .","title":"Home"},{"location":"#how-is-this-different-from-rke-or-k3s","text":"RKE2 combines the best-of-both-worlds from the 1.x version of RKE (hereafter referred to as RKE1) and K3s. From K3s, it inherits the usability, ease-of-operations, and deployment model. From RKE1, it inherits close alignment with upstream Kubernetes. In places K3s has diverged from upstream Kubernetes in order to optimize for edge deployments, but RKE1 and RKE2 can stay closely aligned with upstream. Importantly, RKE2 does not rely on Docker as RKE1 does. RKE1 leveraged Docker for deploying and managing the control plane components as well as the container runtime for Kubernetes. RKE2 launches control plane components as static pods, managed by the kubelet. The embedded container runtime is containerd.","title":"How is this different from RKE or K3s?"},{"location":"#why-two-names","text":"It is known as RKE Government in order to convey the primary use cases and sector it currently targets. It is also known as RKE 2 as it is the next iteration of the Rancher Kubernetes Engine for datacenter use cases.","title":"Why two names?"},{"location":"#security","text":"Rancher Labs supports responsible disclosure and endeavors to resolve security issues in a reasonable timeframe. To report a security vulnerability, email security@rancher.com .","title":"Security"},{"location":"advanced/","text":"Advanced Options and Configuration \u00b6 This section contains advanced information describing the different ways you can run and manage RKE2. Certificate Rotation \u00b6 By default, certificates in RKE2 expire in 12 months. If the certificates are expired or have fewer than 90 days remaining before they expire, the certificates are rotated when RKE2 is restarted. As of v1.21.8+rke2r1, certificates can also be rotated manually. To do this, it is best to stop the rke2-server process, rotate the certificates, then start the process up again: systemctl stop rke2-server rke2 certificate rotate systemctl start rke2-server It is also possible to rotate an individual service by passing the --service flag, for example: rke2 certificate rotate --service api-server . See the certificate subcommand for more details. Auto-Deploying Manifests \u00b6 Any file found in /var/lib/rancher/rke2/server/manifests will automatically be deployed to Kubernetes in a manner similar to kubectl apply . For information about deploying Helm charts using the manifests directory, refer to the section about Helm. Configuring containerd \u00b6 RKE2 will generate the config.toml for containerd in /var/lib/rancher/rke2/agent/etc/containerd/config.toml . For advanced customization of this file you can create another file called config.toml.tmpl in the same directory and it will be used instead. The config.toml.tmpl will be treated as a Go template file, and the config.Node structure is being passed to the template. See this template for an example of how to use the structure to customize the configuration file. Configuring an HTTP proxy \u00b6 If you are running RKE2 in an environment, which only has external connectivity through an HTTP proxy, you can configure your proxy settings on the RKE2 systemd service. These proxy settings will then be used in RKE2 and passed down to the embedded containerd and kubelet. Add the necessary HTTP_PROXY , HTTPS_PROXY and NO_PROXY variables to the environment file of your systemd service, usually: /etc/default/rke2-server /etc/default/rke2-agent The NO_PROXY variable must include your internal networks, as well as the cluster pod and service IP ranges. HTTP_PROXY=http://your-proxy.example.com:8888 HTTPS_PROXY=http://your-proxy.example.com:8888 NO_PROXY=127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,.svc,.cluster.local If you want to configure the proxy settings for containerd without affecting RKE2 and the Kubelet, you can prefix the variables with CONTAINERD_ : CONTAINERD_HTTP_PROXY=http://your-proxy.example.com:8888 CONTAINERD_HTTPS_PROXY=http://your-proxy.example.com:8888 CONTAINERD_NO_PROXY=127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,.svc,.cluster.local Node Labels and Taints \u00b6 RKE2 agents can be configured with the options node-label and node-taint which adds a label and taint to the kubelet. The two options only add labels and/or taints at registration time, and can only be added once and not removed after that through rke2 commands. If you want to change node labels and taints after node registration you should use kubectl . Refer to the official Kubernetes documentation for details on how to add taints and node labels . How Agent Node Registration Works \u00b6 Agent nodes are registered via a websocket connection initiated by the rke2 agent process, and the connection is maintained by a client-side load balancer running as part of the agent process. Agents register with the server using the cluster secret portion of the join token, along with a randomly generated node-specific password, which is stored on the agent at /etc/rancher/node/password . The server will store the passwords for individual nodes as Kubernetes secrets, and any subsequent attempts must use the same password. Node password secrets are stored in the kube-system namespace with names using the template <host>.node-password.rke2 . These secrets are deleted when the corresponding Kubernetes node is deleted. Note: Prior to RKE2 v1.20.2 servers stored passwords on disk at /var/lib/rancher/rke2/server/cred/node-passwd . If the /etc/rancher/node directory of an agent is removed, the password file should be recreated for the agent prior to startup, or the entry removed from the server or Kubernetes cluster (depending on the RKE2 version). Starting the Server with the Installation Script \u00b6 The installation script provides units for systemd, but does not enable or start the service by default. When running with systemd, logs will be created in /var/log/syslog and viewed using journalctl -u rke2-server or journalctl -u rke2-agent . An example of installing with the install script: curl -sfL https://get.rke2.io | sh - systemctl enable rke2-server systemctl start rke2-server Disabling Server Charts \u00b6 The server charts bundled with rke2 deployed during cluster bootstrapping can be disabled and replaced with alternatives. A common use case is replacing the bundled rke2-ingress-nginx chart with an alternative. To disable any of the bundled system charts, set the disable parameter in the config file before bootstrapping. The full list of system charts to disable is below: rke2-canal rke2-coredns rke2-ingress-nginx rke2-metrics-server Note that it is the cluster operator's responsibility to ensure that components are disabled or replaced with care, as the server charts play important roles in cluster operability. Refer to the architecture overview for more information on the individual system charts role within the cluster. Installation on classified AWS regions or networks with custom AWS API endpoints \u00b6 In public AWS regions, installing RKE2 with --cloud-provider-name=aws will ensure RKE2 is cloud-enabled, and capable of auto-provisioning certain cloud resources. When installing RKE2 on classified regions (such as SC2S or C2S), there are a few additional pre-requisites to be aware of to ensure RKE2 knows how and where to securely communicate with the appropriate AWS endpoints: Ensure all the common AWS cloud-provider prerequisites are met. These are independent of regions and are always required. Ensure RKE2 knows where to send API requests for ec2 and elasticloadbalancing services by creating a cloud.conf file, the below is an example for the us-iso-east-1 (C2S) region: # /etc/rancher/rke2/cloud.conf [ Global ] [ ServiceOverride \"ec2\" ] Service=ec2 Region=us-iso-east-1 URL=https://ec2.us-iso-east-1.c2s.ic.gov SigningRegion=us-iso-east-1 [ServiceOverride \"elasticloadbalancing\"] Service=elasticloadbalancing Region=us-iso-east-1 URL=https://elasticloadbalancing.us-iso-east-1.c2s.ic.gov SigningRegion=us-iso-east-1 Alternatively, if you are using private AWS endpoints , ensure the appropriate URL is used for each of the private endpoints. Ensure the appropriate AWS CA bundle is loaded into the system's root ca trust store. This may already be done for you depending on the AMI you are using. # on CentOS/RHEL 7/8 cp <ca.pem> /etc/pki/ca-trust/source/anchors/ update-ca-trust configure RKE2 to use the aws cloud-provider with the custom cloud.conf created in step 1: # /etc/rancher/rke2/config.yaml ... cloud-provider-name : aws cloud-provider-config : \"/etc/rancher/rke2/cloud.conf\" ... Install RKE2 normally (most likely in an airgapped capacity) Validate successful installation by confirming the existence of AWS metadata on cluster node labels with kubectl get nodes --show-labels Control Plane Component Resource Requests/Limits \u00b6 The following options are available under the server sub-command for RKE2. The options allow for specifying CPU requests and limits for the control plane components within RKE2. --control-plane-resource-requests value (components) Control Plane resource requests [$RKE2_CONTROL_PLANE_RESOURCE_REQUESTS] --control-plane-resource-limits value (components) Control Plane resource limits [$RKE2_CONTROL_PLANE_RESOURCE_LIMITS] Values are a comma-delimited list of [controlplane-component]-(cpu|memory)=[desired-value] . The possible values for controlplane-component are: kube-apiserver kube-scheduler kube-controller-manager kube-proxy etcd cloud-controller-manager Thus, an example --control-plane-resource-requests or --control-plane-resource-limits value may look like: kube-apiserver-cpu=500m,kube-apiserver-memory=512M,kube-scheduler-cpu=250m,kube-scheduler-memory=512M,etcd-cpu=1000m The unit values for CPU/memory are identical to Kubernetes resource units (See: Resource Limits in Kubernetes ) Extra Control Plane Component Volume Mounts \u00b6 The following options are available under the server sub-command for RKE2. These options specify host-path mounting of directories from the node filesystem into the static pod component that corresponds to the prefixed name. --kube-apiserver-extra-mount value (components) kube-apiserver extra volume mounts [$RKE2_KUBE_APISERVER_EXTRA_MOUNT] --kube-scheduler-extra-mount value (components) kube-scheduler extra volume mounts [$RKE2_KUBE_SCHEDULER_EXTRA_MOUNT] --kube-controller-manager-extra-mount value (components) kube-controller-manager extra volume mounts [$RKE2_KUBE_CONTROLLER_MANAGER_EXTRA_MOUNT] --kube-proxy-extra-mount value (components) kube-proxy extra volume mounts [$RKE2_KUBE_PROXY_EXTRA_MOUNT] --etcd-extra-mount value (components) etcd extra volume mounts [$RKE2_ETCD_EXTRA_MOUNT] --cloud-controller-manager-extra-mount value (components) cloud-controller-manager extra volume mounts [$RKE2_CLOUD_CONTROLLER_MANAGER_EXTRA_MOUNT] RW Host Path Volume Mount \u00b6 /source/volume/path/on/host:/destination/volume/path/in/staticpod RO Host Path Volume Mount \u00b6 In order to mount a volume as read only, append :ro to the end of the volume mount. /source/volume/path/on/host:/destination/volume/path/in/staticpod:ro Multiple volume mounts can be specified for the same component by passing the flag values as an array in the config file. Extra Control Plane Component Environment Variables \u00b6 The following options are available under the server sub-command for RKE2. These options specify additional environment variables in standard format i.e. KEY=VALUE for the static pod component that corresponds to the prefixed name. --kube-apiserver-extra-env value (components) kube-apiserver extra environment variables [$RKE2_KUBE_APISERVER_EXTRA_ENV] --kube-scheduler-extra-env value (components) kube-scheduler extra environment variables [$RKE2_KUBE_SCHEDULER_EXTRA_ENV] --kube-controller-manager-extra-env value (components) kube-controller-manager extra environment variables [$RKE2_KUBE_CONTROLLER_MANAGER_EXTRA_ENV] --kube-proxy-extra-env value (components) kube-proxy extra environment variables [$RKE2_KUBE_PROXY_EXTRA_ENV] --etcd-extra-env value (components) etcd extra environment variables [$RKE2_ETCD_EXTRA_ENV] --cloud-controller-manager-extra-env value (components) cloud-controller-manager extra environment variables [$RKE2_CLOUD_CONTROLLER_MANAGER_EXTRA_ENV] Multiple environment variables can be specified for the same component by passing the flag values as an array in the config file.","title":"Advanced Options and Configuration"},{"location":"advanced/#advanced-options-and-configuration","text":"This section contains advanced information describing the different ways you can run and manage RKE2.","title":"Advanced Options and Configuration"},{"location":"advanced/#certificate-rotation","text":"By default, certificates in RKE2 expire in 12 months. If the certificates are expired or have fewer than 90 days remaining before they expire, the certificates are rotated when RKE2 is restarted. As of v1.21.8+rke2r1, certificates can also be rotated manually. To do this, it is best to stop the rke2-server process, rotate the certificates, then start the process up again: systemctl stop rke2-server rke2 certificate rotate systemctl start rke2-server It is also possible to rotate an individual service by passing the --service flag, for example: rke2 certificate rotate --service api-server . See the certificate subcommand for more details.","title":"Certificate Rotation"},{"location":"advanced/#auto-deploying-manifests","text":"Any file found in /var/lib/rancher/rke2/server/manifests will automatically be deployed to Kubernetes in a manner similar to kubectl apply . For information about deploying Helm charts using the manifests directory, refer to the section about Helm.","title":"Auto-Deploying Manifests"},{"location":"advanced/#configuring-containerd","text":"RKE2 will generate the config.toml for containerd in /var/lib/rancher/rke2/agent/etc/containerd/config.toml . For advanced customization of this file you can create another file called config.toml.tmpl in the same directory and it will be used instead. The config.toml.tmpl will be treated as a Go template file, and the config.Node structure is being passed to the template. See this template for an example of how to use the structure to customize the configuration file.","title":"Configuring containerd"},{"location":"advanced/#configuring-an-http-proxy","text":"If you are running RKE2 in an environment, which only has external connectivity through an HTTP proxy, you can configure your proxy settings on the RKE2 systemd service. These proxy settings will then be used in RKE2 and passed down to the embedded containerd and kubelet. Add the necessary HTTP_PROXY , HTTPS_PROXY and NO_PROXY variables to the environment file of your systemd service, usually: /etc/default/rke2-server /etc/default/rke2-agent The NO_PROXY variable must include your internal networks, as well as the cluster pod and service IP ranges. HTTP_PROXY=http://your-proxy.example.com:8888 HTTPS_PROXY=http://your-proxy.example.com:8888 NO_PROXY=127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,.svc,.cluster.local If you want to configure the proxy settings for containerd without affecting RKE2 and the Kubelet, you can prefix the variables with CONTAINERD_ : CONTAINERD_HTTP_PROXY=http://your-proxy.example.com:8888 CONTAINERD_HTTPS_PROXY=http://your-proxy.example.com:8888 CONTAINERD_NO_PROXY=127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,.svc,.cluster.local","title":"Configuring an HTTP proxy"},{"location":"advanced/#node-labels-and-taints","text":"RKE2 agents can be configured with the options node-label and node-taint which adds a label and taint to the kubelet. The two options only add labels and/or taints at registration time, and can only be added once and not removed after that through rke2 commands. If you want to change node labels and taints after node registration you should use kubectl . Refer to the official Kubernetes documentation for details on how to add taints and node labels .","title":"Node Labels and Taints"},{"location":"advanced/#how-agent-node-registration-works","text":"Agent nodes are registered via a websocket connection initiated by the rke2 agent process, and the connection is maintained by a client-side load balancer running as part of the agent process. Agents register with the server using the cluster secret portion of the join token, along with a randomly generated node-specific password, which is stored on the agent at /etc/rancher/node/password . The server will store the passwords for individual nodes as Kubernetes secrets, and any subsequent attempts must use the same password. Node password secrets are stored in the kube-system namespace with names using the template <host>.node-password.rke2 . These secrets are deleted when the corresponding Kubernetes node is deleted. Note: Prior to RKE2 v1.20.2 servers stored passwords on disk at /var/lib/rancher/rke2/server/cred/node-passwd . If the /etc/rancher/node directory of an agent is removed, the password file should be recreated for the agent prior to startup, or the entry removed from the server or Kubernetes cluster (depending on the RKE2 version).","title":"How Agent Node Registration Works"},{"location":"advanced/#starting-the-server-with-the-installation-script","text":"The installation script provides units for systemd, but does not enable or start the service by default. When running with systemd, logs will be created in /var/log/syslog and viewed using journalctl -u rke2-server or journalctl -u rke2-agent . An example of installing with the install script: curl -sfL https://get.rke2.io | sh - systemctl enable rke2-server systemctl start rke2-server","title":"Starting the Server with the Installation Script"},{"location":"advanced/#disabling-server-charts","text":"The server charts bundled with rke2 deployed during cluster bootstrapping can be disabled and replaced with alternatives. A common use case is replacing the bundled rke2-ingress-nginx chart with an alternative. To disable any of the bundled system charts, set the disable parameter in the config file before bootstrapping. The full list of system charts to disable is below: rke2-canal rke2-coredns rke2-ingress-nginx rke2-metrics-server Note that it is the cluster operator's responsibility to ensure that components are disabled or replaced with care, as the server charts play important roles in cluster operability. Refer to the architecture overview for more information on the individual system charts role within the cluster.","title":"Disabling Server Charts"},{"location":"advanced/#installation-on-classified-aws-regions-or-networks-with-custom-aws-api-endpoints","text":"In public AWS regions, installing RKE2 with --cloud-provider-name=aws will ensure RKE2 is cloud-enabled, and capable of auto-provisioning certain cloud resources. When installing RKE2 on classified regions (such as SC2S or C2S), there are a few additional pre-requisites to be aware of to ensure RKE2 knows how and where to securely communicate with the appropriate AWS endpoints: Ensure all the common AWS cloud-provider prerequisites are met. These are independent of regions and are always required. Ensure RKE2 knows where to send API requests for ec2 and elasticloadbalancing services by creating a cloud.conf file, the below is an example for the us-iso-east-1 (C2S) region: # /etc/rancher/rke2/cloud.conf [ Global ] [ ServiceOverride \"ec2\" ] Service=ec2 Region=us-iso-east-1 URL=https://ec2.us-iso-east-1.c2s.ic.gov SigningRegion=us-iso-east-1 [ServiceOverride \"elasticloadbalancing\"] Service=elasticloadbalancing Region=us-iso-east-1 URL=https://elasticloadbalancing.us-iso-east-1.c2s.ic.gov SigningRegion=us-iso-east-1 Alternatively, if you are using private AWS endpoints , ensure the appropriate URL is used for each of the private endpoints. Ensure the appropriate AWS CA bundle is loaded into the system's root ca trust store. This may already be done for you depending on the AMI you are using. # on CentOS/RHEL 7/8 cp <ca.pem> /etc/pki/ca-trust/source/anchors/ update-ca-trust configure RKE2 to use the aws cloud-provider with the custom cloud.conf created in step 1: # /etc/rancher/rke2/config.yaml ... cloud-provider-name : aws cloud-provider-config : \"/etc/rancher/rke2/cloud.conf\" ... Install RKE2 normally (most likely in an airgapped capacity) Validate successful installation by confirming the existence of AWS metadata on cluster node labels with kubectl get nodes --show-labels","title":"Installation on classified AWS regions or networks with custom AWS API endpoints"},{"location":"advanced/#control-plane-component-resource-requestslimits","text":"The following options are available under the server sub-command for RKE2. The options allow for specifying CPU requests and limits for the control plane components within RKE2. --control-plane-resource-requests value (components) Control Plane resource requests [$RKE2_CONTROL_PLANE_RESOURCE_REQUESTS] --control-plane-resource-limits value (components) Control Plane resource limits [$RKE2_CONTROL_PLANE_RESOURCE_LIMITS] Values are a comma-delimited list of [controlplane-component]-(cpu|memory)=[desired-value] . The possible values for controlplane-component are: kube-apiserver kube-scheduler kube-controller-manager kube-proxy etcd cloud-controller-manager Thus, an example --control-plane-resource-requests or --control-plane-resource-limits value may look like: kube-apiserver-cpu=500m,kube-apiserver-memory=512M,kube-scheduler-cpu=250m,kube-scheduler-memory=512M,etcd-cpu=1000m The unit values for CPU/memory are identical to Kubernetes resource units (See: Resource Limits in Kubernetes )","title":"Control Plane Component Resource Requests/Limits"},{"location":"advanced/#extra-control-plane-component-volume-mounts","text":"The following options are available under the server sub-command for RKE2. These options specify host-path mounting of directories from the node filesystem into the static pod component that corresponds to the prefixed name. --kube-apiserver-extra-mount value (components) kube-apiserver extra volume mounts [$RKE2_KUBE_APISERVER_EXTRA_MOUNT] --kube-scheduler-extra-mount value (components) kube-scheduler extra volume mounts [$RKE2_KUBE_SCHEDULER_EXTRA_MOUNT] --kube-controller-manager-extra-mount value (components) kube-controller-manager extra volume mounts [$RKE2_KUBE_CONTROLLER_MANAGER_EXTRA_MOUNT] --kube-proxy-extra-mount value (components) kube-proxy extra volume mounts [$RKE2_KUBE_PROXY_EXTRA_MOUNT] --etcd-extra-mount value (components) etcd extra volume mounts [$RKE2_ETCD_EXTRA_MOUNT] --cloud-controller-manager-extra-mount value (components) cloud-controller-manager extra volume mounts [$RKE2_CLOUD_CONTROLLER_MANAGER_EXTRA_MOUNT]","title":"Extra Control Plane Component Volume Mounts"},{"location":"advanced/#rw-host-path-volume-mount","text":"/source/volume/path/on/host:/destination/volume/path/in/staticpod","title":"RW Host Path Volume Mount"},{"location":"advanced/#ro-host-path-volume-mount","text":"In order to mount a volume as read only, append :ro to the end of the volume mount. /source/volume/path/on/host:/destination/volume/path/in/staticpod:ro Multiple volume mounts can be specified for the same component by passing the flag values as an array in the config file.","title":"RO Host Path Volume Mount"},{"location":"advanced/#extra-control-plane-component-environment-variables","text":"The following options are available under the server sub-command for RKE2. These options specify additional environment variables in standard format i.e. KEY=VALUE for the static pod component that corresponds to the prefixed name. --kube-apiserver-extra-env value (components) kube-apiserver extra environment variables [$RKE2_KUBE_APISERVER_EXTRA_ENV] --kube-scheduler-extra-env value (components) kube-scheduler extra environment variables [$RKE2_KUBE_SCHEDULER_EXTRA_ENV] --kube-controller-manager-extra-env value (components) kube-controller-manager extra environment variables [$RKE2_KUBE_CONTROLLER_MANAGER_EXTRA_ENV] --kube-proxy-extra-env value (components) kube-proxy extra environment variables [$RKE2_KUBE_PROXY_EXTRA_ENV] --etcd-extra-env value (components) etcd extra environment variables [$RKE2_ETCD_EXTRA_ENV] --cloud-controller-manager-extra-env value (components) cloud-controller-manager extra environment variables [$RKE2_CLOUD_CONTROLLER_MANAGER_EXTRA_ENV] Multiple environment variables can be specified for the same component by passing the flag values as an array in the config file.","title":"Extra Control Plane Component Environment Variables"},{"location":"backup_restore/","text":"Etcd Backup and Restore \u00b6 In this section, you'll learn how to create backups of the rke2 cluster data and to restore the cluster from backup. Note: /var/lib/rancher/rke2 is the default data directory for rke2, it is configurable however via data-dir parameter. Creating Snapshots \u00b6 Snapshots are enabled by default. The snapshot directory defaults to /var/lib/rancher/rke2/server/db/snapshots . To configure the snapshot interval or the number of retained snapshots, refer to the options. In RKE2, snapshots are stored on each etcd node. If you have multiple etcd or etcd + control-plane nodes, you will have multiple copies of local etcd snapshots. You can take a snapshot manually while RKE2 is running with the etcd-snapshot subcommand. For example: rke2 etcd-snapshot save --name pre-upgrade-snapshot . See the full list of etcd-snapshot subcommands at the subcommands page Cluster Reset \u00b6 RKE2 enables a feature to reset the cluster to one member cluster by passing --cluster-reset flag, when passing this flag to rke2 server it will reset the cluster with the same data dir in place, the data directory for etcd exists in /var/lib/rancher/rke2/server/db/etcd , this flag can be passed in the events of quorum loss in the cluster. To pass the reset flag, first you need to stop RKE2 service if its enabled via systemd: systemctl stop rke2-server rke2 server --cluster-reset Result: A message in the logs say that RKE2 can be restarted without the flags. Start rke2 again and it should start rke2 as a 1 member cluster. Restoring a Snapshot to Existing Nodes \u00b6 When RKE2 is restored from backup, the old data directory will be moved to /var/lib/rancher/rke2/server/db/etcd-old-%date%/ . RKE2 will then attempt to restore the snapshot by creating a new data directory and start etcd with a new RKE2 cluster with one etcd member. You must stop RKE2 service on all server nodes if it is enabled via systemd. Use the following command to do so: systemctl stop rke2-server Next, you will initiate the restore from snapshot on the first server node with the following commands: rke2 server \\ --cluster-reset \\ --cluster-reset-restore-path=<PATH-TO-SNAPSHOT> Once the restore process is complete, start the rke2-server service on the first server node as follows: systemctl start rke2-server Remove the rke2 db directory on the other server nodes as follows: rm -rf /var/lib/rancher/rke2/server/db Start the rke2-server service on other server nodes with the following command: systemctl start rke2-server Result: After a successful restore, a message in the logs says that etcd is running, and RKE2 can be restarted without the flags. Start RKE2 again, and it should run successfully and be restored from the specified snapshot. When rke2 resets the cluster, it creates an empty file at /var/lib/rancher/rke2/server/db/reset-flag . This file is harmless to leave in place, but must be removed in order to perform subsequent resets or restores. This file is deleted when rke2 starts normally. Restoring a Snapshot to New Nodes \u00b6 Warning: For all versions of rke2 v.1.20.9 and prior, you will need to back up and restore certificates first due to a known issue in which bootstrap data might not save on restore (Steps 1 - 3 below assume this scenario). See note below for an additional version-specific restore caveat on restore. Back up the following: /var/lib/rancher/rke2/server/cred , /var/lib/rancher/rke2/server/tls , /var/lib/rancher/rke2/server/token , /etc/rancher Restore the certs in Step 1 above to the first new server node. Install rke2 v1.20.8+rke2r1 on the first new server node as in the following example: curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=\"v1.20.8+rke2r1\" sh -` Stop RKE2 service on all server nodes if it is enabled and initiate the restore from snapshot on the first server node with the following commands: systemctl stop rke2-server rke2 server \\ --cluster-reset \\ --cluster-reset-restore-path=<PATH-TO-SNAPSHOT> Once the restore process is complete, start the rke2-server service on the first server node as follows: systemctl start rke2-server You can continue to add new server and worker nodes to cluster per standard RKE2 HA installation documentation . Other Notes on Restoring a Snapshot \u00b6 When performing a restore from backup, users do not need to restore a snapshot using the same version of RKE2 with which the snapshot was created. Users may restore using a more recent version. Be aware when changing versions at restore which etcd version is in use. By default, snapshots are enabled and are scheduled to be taken every 12 hours. The snapshots are written to ${data-dir}/server/db/snapshots with the default ${data-dir} being /var/lib/rancher/rke2 . Version-specific requirement for rke2 v1.20.11+rke2r1 When restoring RKE2 from backup to a new node in rke2 v1.20.11+rke2r1, you should ensure that all pods are stopped following the initial restore by running rke2-killall.sh as follows: curl -sfL https://get.rke2.io | sudo INSTALL_RKE2_VERSION=v1.20.11+rke2r1 rke2 server \\ --cluster-reset \\ --cluster-reset-restore-path=<PATH-TO-SNAPSHOT> \\ --token=<token used in the original cluster> rke2-killall.sh Once the restore process is complete, enable and start the rke2-server service on the first server node as follows: systemctl enable rke2-server systemctl start rke2-server Options \u00b6 These options can be set in the configuration file: Options Description etcd-disable-snapshots Disable automatic etcd snapshots etcd-snapshot-schedule-cron value Snapshot interval time in cron spec. eg. every 4 hours 0 */4 * * * (default: 0 */12 * * * ) etcd-snapshot-retention value Number of snapshots to retain (default: 5) etcd-snapshot-dir value Directory to save db snapshots. (Default location: ${data-dir}/db/snapshots ) cluster-reset Forget all peers and become sole member of a new cluster. This can also be set with the environment variable [$RKE2_CLUSTER_RESET] . cluster-reset-restore-path value Path to snapshot file to be restored S3 Compatible API Support \u00b6 rke2 supports writing etcd snapshots to and restoring etcd snapshots from systems with S3-compatible APIs. S3 support is available for both on-demand and scheduled snapshots. The arguments below have been added to the server subcommand. These flags exist for the etcd-snapshot subcommand as well however the --etcd-s3 portion is removed to avoid redundancy. Options Description --etcd-s3 Enable backup to S3 --etcd-s3-endpoint S3 endpoint url --etcd-s3-endpoint-ca S3 custom CA cert to connect to S3 endpoint --etcd-s3-skip-ssl-verify Disables S3 SSL certificate validation --etcd-s3-access-key S3 access key --etcd-s3-secret-key S3 secret key\" --etcd-s3-bucket S3 bucket name --etcd-s3-region S3 region / bucket location (optional). defaults to us-east-1 --etcd-s3-folder S3 folder To perform an on-demand etcd snapshot and save it to S3: rke2 etcd-snapshot \\ --s3 \\ --s3-bucket=<S3-BUCKET-NAME> \\ --s3-access-key=<S3-ACCESS-KEY> \\ --s3-secret-key=<S3-SECRET-KEY> To perform an on-demand etcd snapshot restore from S3, first make sure that rke2 isn't running. Then run the following commands: rke2 server \\ --cluster-reset \\ --etcd-s3 \\ --cluster-reset-restore-path=<SNAPSHOT-NAME> \\ --etcd-s3-bucket=<S3-BUCKET-NAME> \\ --etcd-s3-access-key=<S3-ACCESS-KEY> \\ --etcd-s3-secret-key=<S3-SECRET-KEY>","title":"Etcd Backup and Restore"},{"location":"backup_restore/#etcd-backup-and-restore","text":"In this section, you'll learn how to create backups of the rke2 cluster data and to restore the cluster from backup. Note: /var/lib/rancher/rke2 is the default data directory for rke2, it is configurable however via data-dir parameter.","title":"Etcd Backup and Restore"},{"location":"backup_restore/#creating-snapshots","text":"Snapshots are enabled by default. The snapshot directory defaults to /var/lib/rancher/rke2/server/db/snapshots . To configure the snapshot interval or the number of retained snapshots, refer to the options. In RKE2, snapshots are stored on each etcd node. If you have multiple etcd or etcd + control-plane nodes, you will have multiple copies of local etcd snapshots. You can take a snapshot manually while RKE2 is running with the etcd-snapshot subcommand. For example: rke2 etcd-snapshot save --name pre-upgrade-snapshot . See the full list of etcd-snapshot subcommands at the subcommands page","title":"Creating Snapshots"},{"location":"backup_restore/#cluster-reset","text":"RKE2 enables a feature to reset the cluster to one member cluster by passing --cluster-reset flag, when passing this flag to rke2 server it will reset the cluster with the same data dir in place, the data directory for etcd exists in /var/lib/rancher/rke2/server/db/etcd , this flag can be passed in the events of quorum loss in the cluster. To pass the reset flag, first you need to stop RKE2 service if its enabled via systemd: systemctl stop rke2-server rke2 server --cluster-reset Result: A message in the logs say that RKE2 can be restarted without the flags. Start rke2 again and it should start rke2 as a 1 member cluster.","title":"Cluster Reset"},{"location":"backup_restore/#restoring-a-snapshot-to-existing-nodes","text":"When RKE2 is restored from backup, the old data directory will be moved to /var/lib/rancher/rke2/server/db/etcd-old-%date%/ . RKE2 will then attempt to restore the snapshot by creating a new data directory and start etcd with a new RKE2 cluster with one etcd member. You must stop RKE2 service on all server nodes if it is enabled via systemd. Use the following command to do so: systemctl stop rke2-server Next, you will initiate the restore from snapshot on the first server node with the following commands: rke2 server \\ --cluster-reset \\ --cluster-reset-restore-path=<PATH-TO-SNAPSHOT> Once the restore process is complete, start the rke2-server service on the first server node as follows: systemctl start rke2-server Remove the rke2 db directory on the other server nodes as follows: rm -rf /var/lib/rancher/rke2/server/db Start the rke2-server service on other server nodes with the following command: systemctl start rke2-server Result: After a successful restore, a message in the logs says that etcd is running, and RKE2 can be restarted without the flags. Start RKE2 again, and it should run successfully and be restored from the specified snapshot. When rke2 resets the cluster, it creates an empty file at /var/lib/rancher/rke2/server/db/reset-flag . This file is harmless to leave in place, but must be removed in order to perform subsequent resets or restores. This file is deleted when rke2 starts normally.","title":"Restoring a Snapshot to Existing Nodes"},{"location":"backup_restore/#restoring-a-snapshot-to-new-nodes","text":"Warning: For all versions of rke2 v.1.20.9 and prior, you will need to back up and restore certificates first due to a known issue in which bootstrap data might not save on restore (Steps 1 - 3 below assume this scenario). See note below for an additional version-specific restore caveat on restore. Back up the following: /var/lib/rancher/rke2/server/cred , /var/lib/rancher/rke2/server/tls , /var/lib/rancher/rke2/server/token , /etc/rancher Restore the certs in Step 1 above to the first new server node. Install rke2 v1.20.8+rke2r1 on the first new server node as in the following example: curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=\"v1.20.8+rke2r1\" sh -` Stop RKE2 service on all server nodes if it is enabled and initiate the restore from snapshot on the first server node with the following commands: systemctl stop rke2-server rke2 server \\ --cluster-reset \\ --cluster-reset-restore-path=<PATH-TO-SNAPSHOT> Once the restore process is complete, start the rke2-server service on the first server node as follows: systemctl start rke2-server You can continue to add new server and worker nodes to cluster per standard RKE2 HA installation documentation .","title":"Restoring a Snapshot to New Nodes"},{"location":"backup_restore/#other-notes-on-restoring-a-snapshot","text":"When performing a restore from backup, users do not need to restore a snapshot using the same version of RKE2 with which the snapshot was created. Users may restore using a more recent version. Be aware when changing versions at restore which etcd version is in use. By default, snapshots are enabled and are scheduled to be taken every 12 hours. The snapshots are written to ${data-dir}/server/db/snapshots with the default ${data-dir} being /var/lib/rancher/rke2 . Version-specific requirement for rke2 v1.20.11+rke2r1 When restoring RKE2 from backup to a new node in rke2 v1.20.11+rke2r1, you should ensure that all pods are stopped following the initial restore by running rke2-killall.sh as follows: curl -sfL https://get.rke2.io | sudo INSTALL_RKE2_VERSION=v1.20.11+rke2r1 rke2 server \\ --cluster-reset \\ --cluster-reset-restore-path=<PATH-TO-SNAPSHOT> \\ --token=<token used in the original cluster> rke2-killall.sh Once the restore process is complete, enable and start the rke2-server service on the first server node as follows: systemctl enable rke2-server systemctl start rke2-server","title":"Other Notes on Restoring a Snapshot"},{"location":"backup_restore/#options","text":"These options can be set in the configuration file: Options Description etcd-disable-snapshots Disable automatic etcd snapshots etcd-snapshot-schedule-cron value Snapshot interval time in cron spec. eg. every 4 hours 0 */4 * * * (default: 0 */12 * * * ) etcd-snapshot-retention value Number of snapshots to retain (default: 5) etcd-snapshot-dir value Directory to save db snapshots. (Default location: ${data-dir}/db/snapshots ) cluster-reset Forget all peers and become sole member of a new cluster. This can also be set with the environment variable [$RKE2_CLUSTER_RESET] . cluster-reset-restore-path value Path to snapshot file to be restored","title":"Options"},{"location":"backup_restore/#s3-compatible-api-support","text":"rke2 supports writing etcd snapshots to and restoring etcd snapshots from systems with S3-compatible APIs. S3 support is available for both on-demand and scheduled snapshots. The arguments below have been added to the server subcommand. These flags exist for the etcd-snapshot subcommand as well however the --etcd-s3 portion is removed to avoid redundancy. Options Description --etcd-s3 Enable backup to S3 --etcd-s3-endpoint S3 endpoint url --etcd-s3-endpoint-ca S3 custom CA cert to connect to S3 endpoint --etcd-s3-skip-ssl-verify Disables S3 SSL certificate validation --etcd-s3-access-key S3 access key --etcd-s3-secret-key S3 secret key\" --etcd-s3-bucket S3 bucket name --etcd-s3-region S3 region / bucket location (optional). defaults to us-east-1 --etcd-s3-folder S3 folder To perform an on-demand etcd snapshot and save it to S3: rke2 etcd-snapshot \\ --s3 \\ --s3-bucket=<S3-BUCKET-NAME> \\ --s3-access-key=<S3-ACCESS-KEY> \\ --s3-secret-key=<S3-SECRET-KEY> To perform an on-demand etcd snapshot restore from S3, first make sure that rke2 isn't running. Then run the following commands: rke2 server \\ --cluster-reset \\ --etcd-s3 \\ --cluster-reset-restore-path=<SNAPSHOT-NAME> \\ --etcd-s3-bucket=<S3-BUCKET-NAME> \\ --etcd-s3-access-key=<S3-ACCESS-KEY> \\ --etcd-s3-secret-key=<S3-SECRET-KEY>","title":"S3 Compatible API Support"},{"location":"cluster_access/","text":"Cluster Access \u00b6 The kubeconfig file stored at /etc/rancher/rke2/rke2.yaml is used to configure access to the Kubernetes cluster. If you have installed upstream Kubernetes command line tools such as kubectl or helm you will need to configure them with the correct kubeconfig path. This can be done by either exporting the KUBECONFIG environment variable or by invoking the --kubeconfig command line flag. Refer to the examples below for details. Note that some tools, such as kubectl, are installed by default into /var/lib/rancher/rke2/bin . Leverage the KUBECONFIG environment variable: export KUBECONFIG=/etc/rancher/rke2/rke2.yaml kubectl get pods --all-namespaces helm ls --all-namespaces Or specify the location of the kubeconfig file in the command: kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get pods --all-namespaces helm --kubeconfig /etc/rancher/rke2/rke2.yaml ls --all-namespaces Accessing the Cluster from Outside with kubectl \u00b6 Copy /etc/rancher/rke2/rke2.yaml on your machine located outside the cluster as ~/.kube/config . Then replace 127.0.0.1 with the IP or hostname of your RKE2 server. kubectl can now manage your RKE2 cluster.","title":"Cluster Access"},{"location":"cluster_access/#cluster-access","text":"The kubeconfig file stored at /etc/rancher/rke2/rke2.yaml is used to configure access to the Kubernetes cluster. If you have installed upstream Kubernetes command line tools such as kubectl or helm you will need to configure them with the correct kubeconfig path. This can be done by either exporting the KUBECONFIG environment variable or by invoking the --kubeconfig command line flag. Refer to the examples below for details. Note that some tools, such as kubectl, are installed by default into /var/lib/rancher/rke2/bin . Leverage the KUBECONFIG environment variable: export KUBECONFIG=/etc/rancher/rke2/rke2.yaml kubectl get pods --all-namespaces helm ls --all-namespaces Or specify the location of the kubeconfig file in the command: kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get pods --all-namespaces helm --kubeconfig /etc/rancher/rke2/rke2.yaml ls --all-namespaces","title":"Cluster Access"},{"location":"cluster_access/#accessing-the-cluster-from-outside-with-kubectl","text":"Copy /etc/rancher/rke2/rke2.yaml on your machine located outside the cluster as ~/.kube/config . Then replace 127.0.0.1 with the IP or hostname of your RKE2 server. kubectl can now manage your RKE2 cluster.","title":"Accessing the Cluster from Outside with kubectl"},{"location":"helm/","text":"Helm Integration \u00b6 Helm is the package management tool of choice for Kubernetes. Helm charts provide templating syntax for Kubernetes YAML manifest documents. With Helm we can create configurable deployments instead of just using static files. For more information about creating your own catalog of deployments, check out the docs at https://helm.sh/docs/intro/quickstart/ . RKE2 does not require any special configuration to use with Helm command-line tools. Just be sure you have properly set up your kubeconfig as per the section about cluster access . RKE2 does include some extra functionality to make deploying both traditional Kubernetes resource manifests and Helm Charts even easier with the rancher/helm-release CRD. This section covers the following topics: Automatically Deploying Manifests and Helm Charts Using the Helm CRD Customizing Packaged Components with HelmChartConfig Automatically Deploying Manifests and Helm Charts \u00b6 Any Kubernetes manifests found in /var/lib/rancher/rke2/server/manifests will automatically be deployed to RKE2 in a manner similar to kubectl apply . Manifests deployed in this manner are managed as AddOn custom resources, and can be viewed by running kubectl get addon -A . You will find AddOns for packaged components such as CoreDNS, Local-Storage, Nginx-Ingress, etc. AddOns are created automatically by the deploy controller, and are named based on their filename in the manifests directory. It is also possible to deploy Helm charts as AddOns. RKE2 includes a Helm Controller that manages Helm charts using a HelmChart Custom Resource Definition (CRD). Using the Helm CRD \u00b6 The HelmChart resource definition captures most of the options you would normally pass to the helm command-line tool. Here's an example of how you might deploy Grafana from the default chart repository, overriding some of the default chart values. Note that the HelmChart resource itself is in the kube-system namespace, but the chart's resources will be deployed to the monitoring namespace. apiVersion : helm.cattle.io/v1 kind : HelmChart metadata : name : grafana namespace : kube-system spec : chart : stable/grafana targetNamespace : monitoring set : adminPassword : \"NotVerySafePassword\" valuesContent : |- image: tag: master env: GF_EXPLORE_ENABLED: true adminUser: admin sidecar: datasources: enabled: true HelmChart Field Definitions \u00b6 Field Default Description Helm Argument / Flag Equivalent name Helm Chart name NAME spec.chart Helm Chart name in repository, or complete HTTPS URL to chart archive (.tgz) CHART spec.targetNamespace default Helm Chart target namespace --namespace spec.version Helm Chart version (when installing from repository) --version spec.repo Helm Chart repository URL --repo spec.helmVersion v3 Helm version to use ( v2 or v3 ) spec.bootstrap False Set to True if this chart is needed to bootstrap the cluster (Cloud Controller Manager, etc) spec.set Override simple default Chart values. These take precedence over options set via valuesContent. --set / --set-string spec.valuesContent Override complex default Chart values via YAML file content --values spec.chartContent Base64-encoded chart archive .tgz - overrides spec.chart CHART Customizing Packaged Components with HelmChartConfig \u00b6 To allow overriding values for packaged components that are deployed as HelmCharts (such as Canal, CoreDNS, Nginx-Ingress, etc), RKE2 supports customizing deployments via a HelmChartConfig resources. The HelmChartConfig resource must match the name and namespace of its corresponding HelmChart, and supports providing additional valuesContent , which is passed to the helm command as an additional value file. Note: HelmChart spec.set values override HelmChart and HelmChartConfig spec.valuesContent settings. For example, to customize the packaged CoreDNS configuration, you can create a file named /var/lib/rancher/rke2/server/manifests/rke2-coredns-config.yaml and populate it with the following content: apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-coredns namespace : kube-system spec : valuesContent : |- image: coredns/coredns imageTag: v1.7.1","title":"Helm Integration"},{"location":"helm/#helm-integration","text":"Helm is the package management tool of choice for Kubernetes. Helm charts provide templating syntax for Kubernetes YAML manifest documents. With Helm we can create configurable deployments instead of just using static files. For more information about creating your own catalog of deployments, check out the docs at https://helm.sh/docs/intro/quickstart/ . RKE2 does not require any special configuration to use with Helm command-line tools. Just be sure you have properly set up your kubeconfig as per the section about cluster access . RKE2 does include some extra functionality to make deploying both traditional Kubernetes resource manifests and Helm Charts even easier with the rancher/helm-release CRD. This section covers the following topics: Automatically Deploying Manifests and Helm Charts Using the Helm CRD Customizing Packaged Components with HelmChartConfig","title":"Helm Integration"},{"location":"helm/#automatically-deploying-manifests-and-helm-charts","text":"Any Kubernetes manifests found in /var/lib/rancher/rke2/server/manifests will automatically be deployed to RKE2 in a manner similar to kubectl apply . Manifests deployed in this manner are managed as AddOn custom resources, and can be viewed by running kubectl get addon -A . You will find AddOns for packaged components such as CoreDNS, Local-Storage, Nginx-Ingress, etc. AddOns are created automatically by the deploy controller, and are named based on their filename in the manifests directory. It is also possible to deploy Helm charts as AddOns. RKE2 includes a Helm Controller that manages Helm charts using a HelmChart Custom Resource Definition (CRD).","title":"Automatically Deploying Manifests and Helm Charts"},{"location":"helm/#using-the-helm-crd","text":"The HelmChart resource definition captures most of the options you would normally pass to the helm command-line tool. Here's an example of how you might deploy Grafana from the default chart repository, overriding some of the default chart values. Note that the HelmChart resource itself is in the kube-system namespace, but the chart's resources will be deployed to the monitoring namespace. apiVersion : helm.cattle.io/v1 kind : HelmChart metadata : name : grafana namespace : kube-system spec : chart : stable/grafana targetNamespace : monitoring set : adminPassword : \"NotVerySafePassword\" valuesContent : |- image: tag: master env: GF_EXPLORE_ENABLED: true adminUser: admin sidecar: datasources: enabled: true","title":"Using the Helm CRD"},{"location":"helm/#helmchart-field-definitions","text":"Field Default Description Helm Argument / Flag Equivalent name Helm Chart name NAME spec.chart Helm Chart name in repository, or complete HTTPS URL to chart archive (.tgz) CHART spec.targetNamespace default Helm Chart target namespace --namespace spec.version Helm Chart version (when installing from repository) --version spec.repo Helm Chart repository URL --repo spec.helmVersion v3 Helm version to use ( v2 or v3 ) spec.bootstrap False Set to True if this chart is needed to bootstrap the cluster (Cloud Controller Manager, etc) spec.set Override simple default Chart values. These take precedence over options set via valuesContent. --set / --set-string spec.valuesContent Override complex default Chart values via YAML file content --values spec.chartContent Base64-encoded chart archive .tgz - overrides spec.chart CHART","title":"HelmChart Field Definitions"},{"location":"helm/#customizing-packaged-components-with-helmchartconfig","text":"To allow overriding values for packaged components that are deployed as HelmCharts (such as Canal, CoreDNS, Nginx-Ingress, etc), RKE2 supports customizing deployments via a HelmChartConfig resources. The HelmChartConfig resource must match the name and namespace of its corresponding HelmChart, and supports providing additional valuesContent , which is passed to the helm command as an additional value file. Note: HelmChart spec.set values override HelmChart and HelmChartConfig spec.valuesContent settings. For example, to customize the packaged CoreDNS configuration, you can create a file named /var/lib/rancher/rke2/server/manifests/rke2-coredns-config.yaml and populate it with the following content: apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-coredns namespace : kube-system spec : valuesContent : |- image: coredns/coredns imageTag: v1.7.1","title":"Customizing Packaged Components with HelmChartConfig"},{"location":"known_issues/","text":"Known Issues and Limitations \u00b6 This section contains current known issues and limitations with rke2. If you come across issues with rke2 not documented here, please open a new issue here . Firewalld conflicts with default networking \u00b6 Firewalld conflicts with RKE2's default Canal (Calico + Flannel) networking stack. To avoid unexpected behavior, firewalld should be disabled on systems running RKE2. NetworkManager \u00b6 NetworkManager manipulates the routing table for interfaces in the default network namespace where many CNIs, including RKE2's default, create veth pairs for connections to containers. This can interfere with the CNI\u2019s ability to route correctly. As such, if installing RKE2 on a NetworkManager enabled system, it is highly recommended to configure NetworkManager to ignore calico/flannel related network interfaces. In order to do this, create a configuration file called rke2-canal.conf in /etc/NetworkManager/conf.d with the contents: [ keyfile ] unmanaged-devices = interface-name:cali* ; interface-name:flannel* If you have not yet installed RKE2, a simple systemctl reload NetworkManager will suffice to install the configuration. If performing this configuration change on a system that already has RKE2 installed, a reboot of the node is necessary to effectively apply the changes. In some operating systems like RHEL 8.4, NetworkManager includes two extra services called nm-cloud-setup.service and nm-cloud-setup.timer . These services add a routing table that interfere with the CNI plugin's configuration. Unfortunately, there is no config that can avoid that as explained in the issue . Therefore, if those services exist, they should be disabled and the node must be rebooted. Istio in Selinux Enforcing System Fails by Default \u00b6 This is due to just-in-time kernel module loading of rke2, which is disallowed under Selinux unless the container is privileged. To allow Istio to run under these conditions, it requires two steps: 1. Enable CNI as part of the Istio install. Please note that this feature is still in Alpha state at the time of this writing. Ensure values.cni.cniBinDir=/opt/cni/bin and values.cni.cniConfDir=/etc/cni/net.d 2. After the install is complete, there should be cni-node pods in a CrashLoopBackoff. Manually edit their daemonset to include securityContext.privileged: true on the install-cni container. This can be performed via a custom overlay as follows: apiVersion : install.istio.io/v1alpha1 kind : IstioOperator spec : components : cni : enabled : true k8s : overlays : - apiVersion : \"apps/v1\" kind : \"DaemonSet\" name : \"istio-cni-node\" patches : - path : spec.template.spec.containers.[name:install-cni].securityContext.privileged value : true values : cni : image : rancher/mirrored-istio-install-cni:1.9.3 excludeNamespaces : - istio-system - kube-system logLevel : info cniBinDir : /opt/cni/bin cniConfDir : /etc/cni/net.d For more information regarding exact failures with detailed logs when not following these steps, please see Issue 504 . Control Groups V2 \u00b6 RKE2 v1.19.5+ ships with containerd v1.4.x or later, hence should run on cgroups v2 capable systems. Older versions (< 1.19.5) is shipped with containerd 1.3.x fork (with back-ported SELinux commits from 1.4.x) which does not support cgroups v2 and requires a little up-front configuration: Assuming a systemd -based system, setting the systemd.unified_cgroup_hierarchy=0 kernel parameter will indicate to systemd that it should run with hybrid (cgroups v1 + v2) support. Combined with the above, setting the systemd.legacy_systemd_cgroup_controller kernel parameter will indicate to systemd that it should run with legacy (cgroups v1) support. As these are kernel command-line arguments they must be set in the system bootloader so that they will be passed to systemd as PID 1 at /sbin/init . See: grub2 manual systemd manual cgroups v2 Calico with vxlan encapsulation \u00b6 Calico hits a kernel bug when using vxlan encapsulation and the checksum offloading of the vxlan interface is on. The issue is described in the calico project and in rke2 project . The workaround we are applying is disabling the checksum offloading by default by applying the value ChecksumOffloadBroken=true in the calico helm chart . This issue has been observed in Ubuntu 18.04, Ubuntu 20.04 and openSUSE Leap 15.3 Wicked \u00b6 Wicked configures the networking settings of the host based on the sysctl configuration files (e.g. under /etc/sysctl.d/ directory). Even though rke2 is setting parameters such as /net/ipv4/conf/all/forwarding to 1, that configuration could be reverted by Wicked whenever it reapplies the network configuration (there are several events that result in reapplying the network configuration as well as rcwicked restart during updates). Consequently, it is very important to enable ipv4 (and ipv6 in case of dual-stack) forwarding in sysctl configuration files. For example, it is recommended to create a file with the name /etc/sysctl.d/90-rke2.conf containing these paratemers (ipv6 only needed in case of dual-stack): net.ipv4.conf.all.forwarding = 1 net.ipv6.conf.all.forwarding = 1 Canal and IP exhaustion \u00b6 There are two possible reasons for this: iptables binary is not installed in the host and there is a pod defining a hostPort. The pod will be given an IP but its creation will fail and Kubernetes will not cease trying to recreate it, consuming one IP every time it tries. Error messages similar to the following will appear in the containerd log. This is the log showing the error: plugin type=\"portmap\" failed (add): failed to open iptables: exec: \"iptables\": executable file not found in $PATH Please install iptables or xtables-nft package to resolve this problem By default Canal keeps track of pod IPs by creating a lock file for each IP in /var/lib/cni/networks/k8s-pod-network . Each IP belongs to a single pod and will be deleted as soon as the pod is removed. However, in the unlikely event that containerd loses track of the running pods, lock files may be leaked and Canal will not be able to reuse those IPs anymore. If this occurs, you may experience IP exhaustion errors, for example: failed to allocate for range 0: no IP addresses available in range set There are two ways to resolve this. You can either manually remove unused IPs from that directory or drain the node, run rke2-killall.sh, start the rke2 systemd service and uncordon the node. If you need to undertake any of these actions, please report the problem via GitHub, making sure to specify how it was triggered. Ingress in CIS Mode \u00b6 By default, when RKE2 is run with the profile: cis-1.6 parameter, it applies network policies that can be restrictive for ingress. This, coupled with the rke2-ingress-nginx chart having hostNetwork: false by default, requires users to set network policies of their own to allow access to the ingress URLs. Below is an example networkpolicy that allows ingress to any workload in the namespace it is applied in. See https://kubernetes.io/docs/concepts/services-networking/network-policies/ for more configuration options. apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : ingress-to-backends spec : podSelector : {} ingress : - from : - namespaceSelector : matchLabels : kubernetes.io/metadata.name : kube-system podSelector : matchLabels : app.kubernetes.io/name : rke2-ingress-nginx policyTypes : - Ingress For more information, refer to comments on https://github.com/rancher/rke2/issues/3195.","title":"Known Issues and Limitations"},{"location":"known_issues/#known-issues-and-limitations","text":"This section contains current known issues and limitations with rke2. If you come across issues with rke2 not documented here, please open a new issue here .","title":"Known Issues and Limitations"},{"location":"known_issues/#firewalld-conflicts-with-default-networking","text":"Firewalld conflicts with RKE2's default Canal (Calico + Flannel) networking stack. To avoid unexpected behavior, firewalld should be disabled on systems running RKE2.","title":"Firewalld conflicts with default networking"},{"location":"known_issues/#networkmanager","text":"NetworkManager manipulates the routing table for interfaces in the default network namespace where many CNIs, including RKE2's default, create veth pairs for connections to containers. This can interfere with the CNI\u2019s ability to route correctly. As such, if installing RKE2 on a NetworkManager enabled system, it is highly recommended to configure NetworkManager to ignore calico/flannel related network interfaces. In order to do this, create a configuration file called rke2-canal.conf in /etc/NetworkManager/conf.d with the contents: [ keyfile ] unmanaged-devices = interface-name:cali* ; interface-name:flannel* If you have not yet installed RKE2, a simple systemctl reload NetworkManager will suffice to install the configuration. If performing this configuration change on a system that already has RKE2 installed, a reboot of the node is necessary to effectively apply the changes. In some operating systems like RHEL 8.4, NetworkManager includes two extra services called nm-cloud-setup.service and nm-cloud-setup.timer . These services add a routing table that interfere with the CNI plugin's configuration. Unfortunately, there is no config that can avoid that as explained in the issue . Therefore, if those services exist, they should be disabled and the node must be rebooted.","title":"NetworkManager"},{"location":"known_issues/#istio-in-selinux-enforcing-system-fails-by-default","text":"This is due to just-in-time kernel module loading of rke2, which is disallowed under Selinux unless the container is privileged. To allow Istio to run under these conditions, it requires two steps: 1. Enable CNI as part of the Istio install. Please note that this feature is still in Alpha state at the time of this writing. Ensure values.cni.cniBinDir=/opt/cni/bin and values.cni.cniConfDir=/etc/cni/net.d 2. After the install is complete, there should be cni-node pods in a CrashLoopBackoff. Manually edit their daemonset to include securityContext.privileged: true on the install-cni container. This can be performed via a custom overlay as follows: apiVersion : install.istio.io/v1alpha1 kind : IstioOperator spec : components : cni : enabled : true k8s : overlays : - apiVersion : \"apps/v1\" kind : \"DaemonSet\" name : \"istio-cni-node\" patches : - path : spec.template.spec.containers.[name:install-cni].securityContext.privileged value : true values : cni : image : rancher/mirrored-istio-install-cni:1.9.3 excludeNamespaces : - istio-system - kube-system logLevel : info cniBinDir : /opt/cni/bin cniConfDir : /etc/cni/net.d For more information regarding exact failures with detailed logs when not following these steps, please see Issue 504 .","title":"Istio in Selinux Enforcing System Fails by Default"},{"location":"known_issues/#control-groups-v2","text":"RKE2 v1.19.5+ ships with containerd v1.4.x or later, hence should run on cgroups v2 capable systems. Older versions (< 1.19.5) is shipped with containerd 1.3.x fork (with back-ported SELinux commits from 1.4.x) which does not support cgroups v2 and requires a little up-front configuration: Assuming a systemd -based system, setting the systemd.unified_cgroup_hierarchy=0 kernel parameter will indicate to systemd that it should run with hybrid (cgroups v1 + v2) support. Combined with the above, setting the systemd.legacy_systemd_cgroup_controller kernel parameter will indicate to systemd that it should run with legacy (cgroups v1) support. As these are kernel command-line arguments they must be set in the system bootloader so that they will be passed to systemd as PID 1 at /sbin/init . See: grub2 manual systemd manual cgroups v2","title":"Control Groups V2"},{"location":"known_issues/#calico-with-vxlan-encapsulation","text":"Calico hits a kernel bug when using vxlan encapsulation and the checksum offloading of the vxlan interface is on. The issue is described in the calico project and in rke2 project . The workaround we are applying is disabling the checksum offloading by default by applying the value ChecksumOffloadBroken=true in the calico helm chart . This issue has been observed in Ubuntu 18.04, Ubuntu 20.04 and openSUSE Leap 15.3","title":"Calico with vxlan encapsulation"},{"location":"known_issues/#wicked","text":"Wicked configures the networking settings of the host based on the sysctl configuration files (e.g. under /etc/sysctl.d/ directory). Even though rke2 is setting parameters such as /net/ipv4/conf/all/forwarding to 1, that configuration could be reverted by Wicked whenever it reapplies the network configuration (there are several events that result in reapplying the network configuration as well as rcwicked restart during updates). Consequently, it is very important to enable ipv4 (and ipv6 in case of dual-stack) forwarding in sysctl configuration files. For example, it is recommended to create a file with the name /etc/sysctl.d/90-rke2.conf containing these paratemers (ipv6 only needed in case of dual-stack): net.ipv4.conf.all.forwarding = 1 net.ipv6.conf.all.forwarding = 1","title":"Wicked"},{"location":"known_issues/#canal-and-ip-exhaustion","text":"There are two possible reasons for this: iptables binary is not installed in the host and there is a pod defining a hostPort. The pod will be given an IP but its creation will fail and Kubernetes will not cease trying to recreate it, consuming one IP every time it tries. Error messages similar to the following will appear in the containerd log. This is the log showing the error: plugin type=\"portmap\" failed (add): failed to open iptables: exec: \"iptables\": executable file not found in $PATH Please install iptables or xtables-nft package to resolve this problem By default Canal keeps track of pod IPs by creating a lock file for each IP in /var/lib/cni/networks/k8s-pod-network . Each IP belongs to a single pod and will be deleted as soon as the pod is removed. However, in the unlikely event that containerd loses track of the running pods, lock files may be leaked and Canal will not be able to reuse those IPs anymore. If this occurs, you may experience IP exhaustion errors, for example: failed to allocate for range 0: no IP addresses available in range set There are two ways to resolve this. You can either manually remove unused IPs from that directory or drain the node, run rke2-killall.sh, start the rke2 systemd service and uncordon the node. If you need to undertake any of these actions, please report the problem via GitHub, making sure to specify how it was triggered.","title":"Canal and IP exhaustion"},{"location":"known_issues/#ingress-in-cis-mode","text":"By default, when RKE2 is run with the profile: cis-1.6 parameter, it applies network policies that can be restrictive for ingress. This, coupled with the rke2-ingress-nginx chart having hostNetwork: false by default, requires users to set network policies of their own to allow access to the ingress URLs. Below is an example networkpolicy that allows ingress to any workload in the namespace it is applied in. See https://kubernetes.io/docs/concepts/services-networking/network-policies/ for more configuration options. apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : ingress-to-backends spec : podSelector : {} ingress : - from : - namespaceSelector : matchLabels : kubernetes.io/metadata.name : kube-system podSelector : matchLabels : app.kubernetes.io/name : rke2-ingress-nginx policyTypes : - Ingress For more information, refer to comments on https://github.com/rancher/rke2/issues/3195.","title":"Ingress in CIS Mode"},{"location":"networking/","text":"Networking \u00b6 This page explains how CoreDNS and the Nginx-Ingress controller work within RKE2. Refer to the Installation Network Options page for details on Canal configuration options, or how to set up your own CNI. For information on which ports need to be opened for RKE2, refer to the Installation Requirements . CoreDNS Nginx Ingress Controller Nodes Without a Hostname CoreDNS \u00b6 CoreDNS is deployed by default when starting the server. To disable, run each server with disable: rke2-coredns option in your configuration file. If you don't install CoreDNS, you will need to install a cluster DNS provider yourself. CoreDNS is deployed with the autoscaler by default. To disable it or change its config, use the HelmChartConfig resource. NodeLocal DNSCache \u00b6 NodeLocal DNSCache improves the performance by running a dns caching agent on each node. To activate this feature, apply the following HelmChartConfig: --- apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-coredns namespace : kube-system spec : valuesContent : |- nodelocal: enabled: true The helm controller will redeploy coredns with the new config. Please be aware that nodelocal modifies the iptables of the node to intercept DNS traffic. Therefore, activating and then deactivating this feature without redeploying, will cause the DNS service to stop working. Note that NodeLocal DNSCache must be deployed in ipvs mode if kube-proxy is using that mode. To deploy it in this mode, apply the following HelmChartConfig: --- apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-coredns namespace : kube-system spec : valuesContent : |- nodelocal: enabled: true ipvs: true Nginx Ingress Controller \u00b6 nginx-ingress is an Ingress controller powered by NGINX that uses a ConfigMap to store the NGINX configuration. nginx-ingress is deployed by default when starting the server. Ports 80 and 443 will be bound by the ingress controller in its default configuration, making these unusable for HostPort or NodePort services in the cluster. Configuration options can be specified by creating a HelmChartConfig manifest to customize the rke2-ingress-nginx HelmChart values. For example, a HelmChartConfig at /var/lib/rancher/rke2/server/manifests/rke2-ingress-nginx-config.yaml with the following contents sets use-forwarded-headers to \"true\" in the ConfigMap storing the NGINX config: # /var/lib/rancher/rke2/server/manifests/rke2-ingress-nginx-config.yaml --- apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-ingress-nginx namespace : kube-system spec : valuesContent : |- controller: config: use-forwarded-headers: \"true\" For more information, refer to the official nginx-ingress Helm Configuration Parameters . To disable the NGINX ingress controller, start each server with the disable: rke2-ingress-nginx option in your configuration file. Nodes Without a Hostname \u00b6 Some cloud providers, such as Linode, will create machines with \"localhost\" as the hostname and others may not have a hostname set at all. This can cause problems with domain name resolution. You can run RKE2 with the node-name parameter and this will pass the node name to resolve this issue.","title":"Networking"},{"location":"networking/#networking","text":"This page explains how CoreDNS and the Nginx-Ingress controller work within RKE2. Refer to the Installation Network Options page for details on Canal configuration options, or how to set up your own CNI. For information on which ports need to be opened for RKE2, refer to the Installation Requirements . CoreDNS Nginx Ingress Controller Nodes Without a Hostname","title":"Networking"},{"location":"networking/#coredns","text":"CoreDNS is deployed by default when starting the server. To disable, run each server with disable: rke2-coredns option in your configuration file. If you don't install CoreDNS, you will need to install a cluster DNS provider yourself. CoreDNS is deployed with the autoscaler by default. To disable it or change its config, use the HelmChartConfig resource.","title":"CoreDNS"},{"location":"networking/#nodelocal-dnscache","text":"NodeLocal DNSCache improves the performance by running a dns caching agent on each node. To activate this feature, apply the following HelmChartConfig: --- apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-coredns namespace : kube-system spec : valuesContent : |- nodelocal: enabled: true The helm controller will redeploy coredns with the new config. Please be aware that nodelocal modifies the iptables of the node to intercept DNS traffic. Therefore, activating and then deactivating this feature without redeploying, will cause the DNS service to stop working. Note that NodeLocal DNSCache must be deployed in ipvs mode if kube-proxy is using that mode. To deploy it in this mode, apply the following HelmChartConfig: --- apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-coredns namespace : kube-system spec : valuesContent : |- nodelocal: enabled: true ipvs: true","title":"NodeLocal DNSCache"},{"location":"networking/#nginx-ingress-controller","text":"nginx-ingress is an Ingress controller powered by NGINX that uses a ConfigMap to store the NGINX configuration. nginx-ingress is deployed by default when starting the server. Ports 80 and 443 will be bound by the ingress controller in its default configuration, making these unusable for HostPort or NodePort services in the cluster. Configuration options can be specified by creating a HelmChartConfig manifest to customize the rke2-ingress-nginx HelmChart values. For example, a HelmChartConfig at /var/lib/rancher/rke2/server/manifests/rke2-ingress-nginx-config.yaml with the following contents sets use-forwarded-headers to \"true\" in the ConfigMap storing the NGINX config: # /var/lib/rancher/rke2/server/manifests/rke2-ingress-nginx-config.yaml --- apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-ingress-nginx namespace : kube-system spec : valuesContent : |- controller: config: use-forwarded-headers: \"true\" For more information, refer to the official nginx-ingress Helm Configuration Parameters . To disable the NGINX ingress controller, start each server with the disable: rke2-ingress-nginx option in your configuration file.","title":"Nginx Ingress Controller"},{"location":"networking/#nodes-without-a-hostname","text":"Some cloud providers, such as Linode, will create machines with \"localhost\" as the hostname and others may not have a hostname set at all. This can cause problems with domain name resolution. You can run RKE2 with the node-name parameter and this will pass the node name to resolve this issue.","title":"Nodes Without a Hostname"},{"location":"subcommands/","text":"Subcommands \u00b6 The rke2 binary comes packaged with multiple subcommands. This page gives information on the options that come with those. etcd-snapshot \u00b6 This subcommand is used to take snapshots manually, list any snapshots currently available, and manually delete any unwanted or older snapshots. NAME: rke2 etcd-snapshot - Trigger an immediate etcd snapshot USAGE: rke2 etcd-snapshot command [command options] [arguments...] COMMANDS: delete Delete given snapshot(s) ls, list, l List snapshots prune Remove snapshots that exceed the configured retention count save Trigger an immediate etcd snapshot OPTIONS: --debug (logging) Turn on debug logs [$RKE2_DEBUG] --config FILE, -c FILE (config) Load configuration from FILE (default: \"/etc/rancher/rke2/config.yaml\") [$RKE2_CONFIG_FILE] --log value, -l value (logging) Log to file --alsologtostderr (logging) Log to standard error as well as file (if set) --node-name value (agent/node) Node name [$RKE2_NODE_NAME] --data-dir value, -d value (data) Folder to hold state (default: \"/var/lib/rancher/rke2\") --dir value, --etcd-snapshot-dir value (db) Directory to save etcd on-demand snapshot. (default: ${data-dir}/db/snapshots) --name value (db) Set the base name of the etcd on-demand snapshot (appended with UNIX timestamp). (default: \"on-demand\") --snapshot-compress, --etcd-snapshot-compress (db) Compress etcd snapshot --s3, --etcd-s3 (db) Enable backup to S3 --s3-endpoint value, --etcd-s3-endpoint value (db) S3 endpoint url (default: \"s3.amazonaws.com\") --s3-endpoint-ca value, --etcd-s3-endpoint-ca value (db) S3 custom CA cert to connect to S3 endpoint --s3-skip-ssl-verify, --etcd-s3-skip-ssl-verify (db) Disables S3 SSL certificate validation --s3-access-key value, --etcd-s3-access-key value (db) S3 access key [$AWS_ACCESS_KEY_ID] --s3-secret-key value, --etcd-s3-secret-key value (db) S3 secret key [$AWS_SECRET_ACCESS_KEY] --s3-bucket value, --etcd-s3-bucket value (db) S3 bucket name --s3-region value, --etcd-s3-region value (db) S3 region / bucket location (optional) (default: \"us-east-1\") --s3-folder value, --etcd-s3-folder value (db) S3 folder --s3-insecure, --etcd-s3-insecure (db) Disables S3 over HTTPS --s3-timeout value, --etcd-s3-timeout value (db) S3 timeout (default: 30s) --help, -h show help certificate \u00b6 This subcommand can be used to rotate the expiry of certificates of the services running in the cluster, such as the kubelet, etcd, and api-server. These are rotated automatically before they expire each year, but this allows for the cases where one might want to rotate them earlier. NAME: rke2 certificate - Certificates management USAGE: rke2 certificate command [command options] [arguments...] COMMANDS: rotate Certificate Rotatation OPTIONS: --debug (logging) Turn on debug logs [$RKE2_DEBUG] --config FILE, -c FILE (config) Load configuration from FILE (default: \"/etc/rancher/rke2/config.yaml\") [$RKE2_CONFIG_FILE] --log value, -l value (logging) Log to file --alsologtostderr (logging) Log to standard error as well as file (if set) --data-dir value, -d value (data) Folder to hold state (default: \"/var/lib/rancher/rke2\") --service value, -s value List of services to rotate certificates for. Options include (admin, api-server, controller-manager, scheduler, rke2-controller, rke2-server, cloud-controller, etcd, auth-proxy, kubelet, kube-proxy) --help, -h show help secrets-encrypt \u00b6 RKE2 has secrets encryption enabled by default. This subcommand allows for disabling that, as well as rotating the encryption key used. NAME: rke2 secrets-encrypt - Control secrets encryption and keys rotation USAGE: rke2 secrets-encrypt command [command options] [arguments...] COMMANDS: status Print current status of secrets encryption enable Enable secrets encryption disable Disable secrets encryption prepare Prepare for encryption keys rotation rotate Rotate secrets encryption keys reencrypt Reencrypt all data with new encryption key OPTIONS: --help, -h show help","title":"Subcommands"},{"location":"subcommands/#subcommands","text":"The rke2 binary comes packaged with multiple subcommands. This page gives information on the options that come with those.","title":"Subcommands"},{"location":"subcommands/#etcd-snapshot","text":"This subcommand is used to take snapshots manually, list any snapshots currently available, and manually delete any unwanted or older snapshots. NAME: rke2 etcd-snapshot - Trigger an immediate etcd snapshot USAGE: rke2 etcd-snapshot command [command options] [arguments...] COMMANDS: delete Delete given snapshot(s) ls, list, l List snapshots prune Remove snapshots that exceed the configured retention count save Trigger an immediate etcd snapshot OPTIONS: --debug (logging) Turn on debug logs [$RKE2_DEBUG] --config FILE, -c FILE (config) Load configuration from FILE (default: \"/etc/rancher/rke2/config.yaml\") [$RKE2_CONFIG_FILE] --log value, -l value (logging) Log to file --alsologtostderr (logging) Log to standard error as well as file (if set) --node-name value (agent/node) Node name [$RKE2_NODE_NAME] --data-dir value, -d value (data) Folder to hold state (default: \"/var/lib/rancher/rke2\") --dir value, --etcd-snapshot-dir value (db) Directory to save etcd on-demand snapshot. (default: ${data-dir}/db/snapshots) --name value (db) Set the base name of the etcd on-demand snapshot (appended with UNIX timestamp). (default: \"on-demand\") --snapshot-compress, --etcd-snapshot-compress (db) Compress etcd snapshot --s3, --etcd-s3 (db) Enable backup to S3 --s3-endpoint value, --etcd-s3-endpoint value (db) S3 endpoint url (default: \"s3.amazonaws.com\") --s3-endpoint-ca value, --etcd-s3-endpoint-ca value (db) S3 custom CA cert to connect to S3 endpoint --s3-skip-ssl-verify, --etcd-s3-skip-ssl-verify (db) Disables S3 SSL certificate validation --s3-access-key value, --etcd-s3-access-key value (db) S3 access key [$AWS_ACCESS_KEY_ID] --s3-secret-key value, --etcd-s3-secret-key value (db) S3 secret key [$AWS_SECRET_ACCESS_KEY] --s3-bucket value, --etcd-s3-bucket value (db) S3 bucket name --s3-region value, --etcd-s3-region value (db) S3 region / bucket location (optional) (default: \"us-east-1\") --s3-folder value, --etcd-s3-folder value (db) S3 folder --s3-insecure, --etcd-s3-insecure (db) Disables S3 over HTTPS --s3-timeout value, --etcd-s3-timeout value (db) S3 timeout (default: 30s) --help, -h show help","title":"etcd-snapshot"},{"location":"subcommands/#certificate","text":"This subcommand can be used to rotate the expiry of certificates of the services running in the cluster, such as the kubelet, etcd, and api-server. These are rotated automatically before they expire each year, but this allows for the cases where one might want to rotate them earlier. NAME: rke2 certificate - Certificates management USAGE: rke2 certificate command [command options] [arguments...] COMMANDS: rotate Certificate Rotatation OPTIONS: --debug (logging) Turn on debug logs [$RKE2_DEBUG] --config FILE, -c FILE (config) Load configuration from FILE (default: \"/etc/rancher/rke2/config.yaml\") [$RKE2_CONFIG_FILE] --log value, -l value (logging) Log to file --alsologtostderr (logging) Log to standard error as well as file (if set) --data-dir value, -d value (data) Folder to hold state (default: \"/var/lib/rancher/rke2\") --service value, -s value List of services to rotate certificates for. Options include (admin, api-server, controller-manager, scheduler, rke2-controller, rke2-server, cloud-controller, etcd, auth-proxy, kubelet, kube-proxy) --help, -h show help","title":"certificate"},{"location":"subcommands/#secrets-encrypt","text":"RKE2 has secrets encryption enabled by default. This subcommand allows for disabling that, as well as rotating the encryption key used. NAME: rke2 secrets-encrypt - Control secrets encryption and keys rotation USAGE: rke2 secrets-encrypt command [command options] [arguments...] COMMANDS: status Print current status of secrets encryption enable Enable secrets encryption disable Disable secrets encryption prepare Prepare for encryption keys rotation rotate Rotate secrets encryption keys reencrypt Reencrypt all data with new encryption key OPTIONS: --help, -h show help","title":"secrets-encrypt"},{"location":"adrs/","text":"Architectural Decision Records (ADRs) \u00b6 ADRs are a record of the arguments and decisions made to change process or software architecture. The idea is for these records to be regularly reviewed, updated, and documented so that new people or external parties to a project can read along and understand the points of a decision, the context, and in general can make educated decisions based on previous discussions. ADRs provide a safer place for individuals who would rather not speak up in face to face communication (which can feel confrontational) or would like to educate themselves on a topic before commenting. Documenting Architecture Decisions \u00b6 In Documenting Architecture Decisions Michael Nygard explains the concept as small, easy to read and update documents which give context to a decision. He further explains the alternative (when the context for decisions is lost) as \"Blindly accept the decision.\" or \"Blindly change it.\". We agree with this point and have thus generated our own process for implementing this concept. Architectural \u00b6 Micheal defines \"Architectural\" as decisions which \"affect the structure, non-functional characteristics, dependencies, interfaces, or construction techniques\" for a project. Our definition is a a bit less structured, essentially if it comes up in a design discussion we want to keep track of it. Versioning \u00b6 Michael states that the file names give no context (are simply numbered) and are never altered once resolved, because \"It's still relevant to know that it was the decision, but is no longer the decision.\". We are choosing to place our ADRs in a version control system where the history of the decision can be queried in the versions of the document itself. This is handy because it lowers the total number of docs that will accumulate and allows us to give the files contextual names. Our file names can summarize the decision they contain so that readers can easily find and reference that decision. Status Types \u00b6 Michael gives four potential statuses for an ADR: \"proposed\", \"accepted\", \"deprecated\", and \"superseded\". Since we are versioning our ADRs and using PRs, we only need \"accepted\", \"rejected\", and \"superseded\", affording us a simpler model. Changes which are in review are handled in PRs, the review is finalized after a period of time with the status of \"accepted\" or \"rejected\". If a decision supersedes another we change the superseded decision's status to \"superseded by doc \" where the doc is a link to the decision which superseded it. Context should be given as to why it is superseded in both the superseded decision in the one superseding. Tools \u00b6 A command line tool for ADRs is available to help manage the complexity of altering or finding a decision. Our model makes such tools a bit less useful, preferring the use of the tools built into the source code management system. Process \u00b6 The process of preparing, documenting, and presenting a decision should be easy and straightforward, we don't want this process to encumber our ability to change, but enable educated decisions. Issue \u00b6 The goal of an issue isn't whether or not to accept or reject a decision. The goal of an issue is to decide whether or not a decision needs to be made. Generate an Issue if: - you have a question or need context about a decision - you have a suggestion which might need a decision to be made - you believe additional context might change a decision Pull Request \u00b6 Generate a PR if: - you have additional context for a decision - a decision was made and there is no ADR for it - you brought up a decision in a design discussion and a change was suggested Pull requests are more closely tied to the file version than Issues, so we prefer conversations about the decision to be in the Pull Request, where it can more easily be found. Step By Step Process for a new ADR \u00b6 You have an idea to improve a process, design, etc Document the idea to present in the design discussion there is a format for this to help you get started create a pull request for the decision with the outcome you would like (approved/rejected) Link the new PR in the discuss-rancher-k3s-rke2 slack channel and notify the team Present the idea in the design discussion meeting Record any context brought forward in the design discussion this should result in a update to the PR Wait one week this gives everyone who would like to comment in the PR the ability to do so add any added context from the comments to the PR Present the outcome of the decision in the next design discussion if new context is brought forward, record it, add it to the PR, and wait another week repeat this process until there is no new context each week you present the ADR make sure to link in slack and notify the team finalize the decision by merging the PR merging should still have the normal requirements (2 approvals and passing CI) Step By Step Process for updating an ADR \u00b6 You have some new context on an existing decision Generate a PR to add the context Merge after the normal requirements (2 approvals and CI passes) this process should not alter the decision, only add context to it any time the decision's status should change, please make sure the team is aware and has had time to discuss. Step By Step Process for revisiting an ADR \u00b6 You disagree with the decision section of an ADR maybe this is due to some new context that you added The decision was made greater than a week ago (the ADR PR should have merged > 7 days ago) this really only applies if there was no new context if you have new context for a decision there should not be any waiting to present a change in the decision this is meant to prevent excessive decision changes, not slow down any necessary change Generate a PR to change the decision section of the ADR Present your point of view in the next design discussion Add any context discovered from the discussion to the PR Highlight the change in slack and notify the team Wait one week Update the PR with any context added in the PR discussion Present the outcome of the decision in the next design discussion Finalize the change by merging the PR if the change is not agreed upon and no context was added then don't merge and close the PR merging should still have the normal requirements (2 approvals and passing CI)","title":"Architectural Decision Records (ADRs)"},{"location":"adrs/#architectural-decision-records-adrs","text":"ADRs are a record of the arguments and decisions made to change process or software architecture. The idea is for these records to be regularly reviewed, updated, and documented so that new people or external parties to a project can read along and understand the points of a decision, the context, and in general can make educated decisions based on previous discussions. ADRs provide a safer place for individuals who would rather not speak up in face to face communication (which can feel confrontational) or would like to educate themselves on a topic before commenting.","title":"Architectural Decision Records (ADRs)"},{"location":"adrs/#documenting-architecture-decisions","text":"In Documenting Architecture Decisions Michael Nygard explains the concept as small, easy to read and update documents which give context to a decision. He further explains the alternative (when the context for decisions is lost) as \"Blindly accept the decision.\" or \"Blindly change it.\". We agree with this point and have thus generated our own process for implementing this concept.","title":"Documenting Architecture Decisions"},{"location":"adrs/#architectural","text":"Micheal defines \"Architectural\" as decisions which \"affect the structure, non-functional characteristics, dependencies, interfaces, or construction techniques\" for a project. Our definition is a a bit less structured, essentially if it comes up in a design discussion we want to keep track of it.","title":"Architectural"},{"location":"adrs/#versioning","text":"Michael states that the file names give no context (are simply numbered) and are never altered once resolved, because \"It's still relevant to know that it was the decision, but is no longer the decision.\". We are choosing to place our ADRs in a version control system where the history of the decision can be queried in the versions of the document itself. This is handy because it lowers the total number of docs that will accumulate and allows us to give the files contextual names. Our file names can summarize the decision they contain so that readers can easily find and reference that decision.","title":"Versioning"},{"location":"adrs/#status-types","text":"Michael gives four potential statuses for an ADR: \"proposed\", \"accepted\", \"deprecated\", and \"superseded\". Since we are versioning our ADRs and using PRs, we only need \"accepted\", \"rejected\", and \"superseded\", affording us a simpler model. Changes which are in review are handled in PRs, the review is finalized after a period of time with the status of \"accepted\" or \"rejected\". If a decision supersedes another we change the superseded decision's status to \"superseded by doc \" where the doc is a link to the decision which superseded it. Context should be given as to why it is superseded in both the superseded decision in the one superseding.","title":"Status Types"},{"location":"adrs/#tools","text":"A command line tool for ADRs is available to help manage the complexity of altering or finding a decision. Our model makes such tools a bit less useful, preferring the use of the tools built into the source code management system.","title":"Tools"},{"location":"adrs/#process","text":"The process of preparing, documenting, and presenting a decision should be easy and straightforward, we don't want this process to encumber our ability to change, but enable educated decisions.","title":"Process"},{"location":"adrs/#issue","text":"The goal of an issue isn't whether or not to accept or reject a decision. The goal of an issue is to decide whether or not a decision needs to be made. Generate an Issue if: - you have a question or need context about a decision - you have a suggestion which might need a decision to be made - you believe additional context might change a decision","title":"Issue"},{"location":"adrs/#pull-request","text":"Generate a PR if: - you have additional context for a decision - a decision was made and there is no ADR for it - you brought up a decision in a design discussion and a change was suggested Pull requests are more closely tied to the file version than Issues, so we prefer conversations about the decision to be in the Pull Request, where it can more easily be found.","title":"Pull Request"},{"location":"adrs/#step-by-step-process-for-a-new-adr","text":"You have an idea to improve a process, design, etc Document the idea to present in the design discussion there is a format for this to help you get started create a pull request for the decision with the outcome you would like (approved/rejected) Link the new PR in the discuss-rancher-k3s-rke2 slack channel and notify the team Present the idea in the design discussion meeting Record any context brought forward in the design discussion this should result in a update to the PR Wait one week this gives everyone who would like to comment in the PR the ability to do so add any added context from the comments to the PR Present the outcome of the decision in the next design discussion if new context is brought forward, record it, add it to the PR, and wait another week repeat this process until there is no new context each week you present the ADR make sure to link in slack and notify the team finalize the decision by merging the PR merging should still have the normal requirements (2 approvals and passing CI)","title":"Step By Step Process for a new ADR"},{"location":"adrs/#step-by-step-process-for-updating-an-adr","text":"You have some new context on an existing decision Generate a PR to add the context Merge after the normal requirements (2 approvals and CI passes) this process should not alter the decision, only add context to it any time the decision's status should change, please make sure the team is aware and has had time to discuss.","title":"Step By Step Process for updating an ADR"},{"location":"adrs/#step-by-step-process-for-revisiting-an-adr","text":"You disagree with the decision section of an ADR maybe this is due to some new context that you added The decision was made greater than a week ago (the ADR PR should have merged > 7 days ago) this really only applies if there was no new context if you have new context for a decision there should not be any waiting to present a change in the decision this is meant to prevent excessive decision changes, not slow down any necessary change Generate a PR to change the decision section of the ADR Present your point of view in the next design discussion Add any context discovered from the discussion to the PR Highlight the change in slack and notify the team Wait one week Update the PR with any context added in the PR discussion Present the outcome of the decision in the next design discussion Finalize the change by merging the PR if the change is not agreed upon and no context was added then don't merge and close the PR merging should still have the normal requirements (2 approvals and passing CI)","title":"Step By Step Process for revisiting an ADR"},{"location":"adrs/001-record-architecture-decisions/","text":"1. Record architecture decisions \u00b6 Date: 2022-01-26 Status \u00b6 Accepted Context \u00b6 We need to record the architectural decisions made on this project. Decision \u00b6 We will use Architecture Decision Records, as described by Michael Nygard . Consequences \u00b6 See Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's adr-tools .","title":"1. Record architecture decisions"},{"location":"adrs/001-record-architecture-decisions/#1-record-architecture-decisions","text":"Date: 2022-01-26","title":"1. Record architecture decisions"},{"location":"adrs/001-record-architecture-decisions/#status","text":"Accepted","title":"Status"},{"location":"adrs/001-record-architecture-decisions/#context","text":"We need to record the architectural decisions made on this project.","title":"Context"},{"location":"adrs/001-record-architecture-decisions/#decision","text":"We will use Architecture Decision Records, as described by Michael Nygard .","title":"Decision"},{"location":"adrs/001-record-architecture-decisions/#consequences","text":"See Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's adr-tools .","title":"Consequences"},{"location":"adrs/002-rke2-rpm-support/","text":"2. RPM support for RKE2 \u00b6 Date: 2022-01-20 Status \u00b6 Accepted Context \u00b6 RKE2 publishes RPMs for distribution of RKE2 through the https://github.com/rancher/rke2-packaging repository. These RPMs are built using automated calls to rpmbuild and corresponding GPG signing/publishing plugins, and publish RPMs to the rpm.rancher.io / rpm-testing.rancher.io S3-backed buckets. Decision \u00b6 Until a more robust RPM building/mechanism is established for RKE2, we will not add any new platforms for RPM publishing beyond the existing CentOS/RHEL 7 and 8 RPMs that are published. We will publish selinux policy RPMs for new platforms as needed, and ensure the selinux RPMs are compatible with the tarball installation method for the platform in question. This decision can be re-evaluated in the future if a more robust RPM publishing technique/platform is developed/made available. Consequences \u00b6 The only supported installation method for all platforms except CentOS 7/8 with selinux support will be a combination of the use of a tarball install in conjunction with an selinux policy RPM.","title":"2. RPM support for RKE2"},{"location":"adrs/002-rke2-rpm-support/#2-rpm-support-for-rke2","text":"Date: 2022-01-20","title":"2. RPM support for RKE2"},{"location":"adrs/002-rke2-rpm-support/#status","text":"Accepted","title":"Status"},{"location":"adrs/002-rke2-rpm-support/#context","text":"RKE2 publishes RPMs for distribution of RKE2 through the https://github.com/rancher/rke2-packaging repository. These RPMs are built using automated calls to rpmbuild and corresponding GPG signing/publishing plugins, and publish RPMs to the rpm.rancher.io / rpm-testing.rancher.io S3-backed buckets.","title":"Context"},{"location":"adrs/002-rke2-rpm-support/#decision","text":"Until a more robust RPM building/mechanism is established for RKE2, we will not add any new platforms for RPM publishing beyond the existing CentOS/RHEL 7 and 8 RPMs that are published. We will publish selinux policy RPMs for new platforms as needed, and ensure the selinux RPMs are compatible with the tarball installation method for the platform in question. This decision can be re-evaluated in the future if a more robust RPM publishing technique/platform is developed/made available.","title":"Decision"},{"location":"adrs/002-rke2-rpm-support/#consequences","text":"The only supported installation method for all platforms except CentOS 7/8 with selinux support will be a combination of the use of a tarball install in conjunction with an selinux policy RPM.","title":"Consequences"},{"location":"adrs/003-rke2-rpm-sle-support/","text":"3. RPM SLE support for RKE2 \u00b6 Date: 2022-01-27 Status \u00b6 Accepted Context \u00b6 RKE2 publishes RPMs for SUSE OS distributions, the rpms will be installed via transactional updates if exists, this will enable two things, the installation of rke2-selinux and the extraction of the binaries in the right /usr paths instead of the alternative tarball installation which will extract the binaries in /opt . Decision \u00b6 We will add support for RPM publishing for SUSE OS distributions in rke2-packaging repo, the rke2-server and rke2-agent packages will require installing rke2-common which will in turn install the rke2-selinux RPM package which is already supported for microos. The decision will involve defaulting to the tarball installation for SUSE OS distribution in the installation script to prevent breaking current compatibility with users who currently installed via tarball installation, the RPM installation will be allowed via passing the environment variable RKE2_INSTALL_METHOD=rpm to the install script. The installation script will also have measures to prevent installation switching from RPM to tarball installation and vice versa, and finally the installation via the tarball method will not allow SELINUX to be enabled unless manually. Consequences \u00b6 The decision will result in some drawbacks: The decision will not enable RPM installation by default. The tarball installation will not enable SELINUX by default.","title":"3. RPM SLE support for RKE2"},{"location":"adrs/003-rke2-rpm-sle-support/#3-rpm-sle-support-for-rke2","text":"Date: 2022-01-27","title":"3. RPM SLE support for RKE2"},{"location":"adrs/003-rke2-rpm-sle-support/#status","text":"Accepted","title":"Status"},{"location":"adrs/003-rke2-rpm-sle-support/#context","text":"RKE2 publishes RPMs for SUSE OS distributions, the rpms will be installed via transactional updates if exists, this will enable two things, the installation of rke2-selinux and the extraction of the binaries in the right /usr paths instead of the alternative tarball installation which will extract the binaries in /opt .","title":"Context"},{"location":"adrs/003-rke2-rpm-sle-support/#decision","text":"We will add support for RPM publishing for SUSE OS distributions in rke2-packaging repo, the rke2-server and rke2-agent packages will require installing rke2-common which will in turn install the rke2-selinux RPM package which is already supported for microos. The decision will involve defaulting to the tarball installation for SUSE OS distribution in the installation script to prevent breaking current compatibility with users who currently installed via tarball installation, the RPM installation will be allowed via passing the environment variable RKE2_INSTALL_METHOD=rpm to the install script. The installation script will also have measures to prevent installation switching from RPM to tarball installation and vice versa, and finally the installation via the tarball method will not allow SELINUX to be enabled unless manually.","title":"Decision"},{"location":"adrs/003-rke2-rpm-sle-support/#consequences","text":"The decision will result in some drawbacks: The decision will not enable RPM installation by default. The tarball installation will not enable SELINUX by default.","title":"Consequences"},{"location":"adrs/adr-template/","text":"ADR Format Template \u00b6 This template provides a canvas for generating ADRs and a standard format so that we can build tools to parse them. - notes are added to this template to help elaborate on the points without a separate document - notes will be prefixed with a dash Established \u00b6 2022-07-20 - this section should contain only the YYYY-MM-DD date of when the decision is considered final - this can be added after context is given, in the PR which will wait for 1 week before merge Revisit by \u00b6 2023-07-15 - this section should contain only the YYYY-MM-DD date of when the decision is considered stale - at the next design discussion we should validate and renew this date Subject \u00b6 Given data , when triggering event , then we do something . the person should be first person plural \"we\" do something not \"I\", \"you\", or \"they\" the tense should be simple present , we \"do\" something not \"does\", \"doing\", \"did\", or \"done\" the mood should be indicative we \"do\" something not \"go do\" Given when then statements should be used as often as possible to get as much context into the subject as possible. Don't force 'given, when, then'; if there is no triggering event or no data given, then leave those parts out. Status \u00b6 Accepted / Rejected / Superseded by #other-issue - accepted is the decision that the subject is appropriate and we will do it. - rejected is the decision that the subject isn't appropriate and we won't do it. - superseded relates that a different decision forces this decision (for instance a decision made at a higher level of abstraction) Context \u00b6 the following is a simple framework for judging a decision, these items are not required, but may be useful to the writer. Strength of doing process \u00b6 Weakness of doing process \u00b6 Threats involved in not doing process \u00b6 Threats involved in doing process \u00b6 Opportunities involved in doing process \u00b6 a different approach to context framework Pros \u00b6 Cons \u00b6","title":"ADR Format Template"},{"location":"adrs/adr-template/#adr-format-template","text":"This template provides a canvas for generating ADRs and a standard format so that we can build tools to parse them. - notes are added to this template to help elaborate on the points without a separate document - notes will be prefixed with a dash","title":"ADR Format Template"},{"location":"adrs/adr-template/#established","text":"2022-07-20 - this section should contain only the YYYY-MM-DD date of when the decision is considered final - this can be added after context is given, in the PR which will wait for 1 week before merge","title":"Established"},{"location":"adrs/adr-template/#revisit-by","text":"2023-07-15 - this section should contain only the YYYY-MM-DD date of when the decision is considered stale - at the next design discussion we should validate and renew this date","title":"Revisit by"},{"location":"adrs/adr-template/#subject","text":"Given data , when triggering event , then we do something . the person should be first person plural \"we\" do something not \"I\", \"you\", or \"they\" the tense should be simple present , we \"do\" something not \"does\", \"doing\", \"did\", or \"done\" the mood should be indicative we \"do\" something not \"go do\" Given when then statements should be used as often as possible to get as much context into the subject as possible. Don't force 'given, when, then'; if there is no triggering event or no data given, then leave those parts out.","title":"Subject"},{"location":"adrs/adr-template/#status","text":"Accepted / Rejected / Superseded by #other-issue - accepted is the decision that the subject is appropriate and we will do it. - rejected is the decision that the subject isn't appropriate and we won't do it. - superseded relates that a different decision forces this decision (for instance a decision made at a higher level of abstraction)","title":"Status"},{"location":"adrs/adr-template/#context","text":"the following is a simple framework for judging a decision, these items are not required, but may be useful to the writer.","title":"Context"},{"location":"adrs/adr-template/#strength-of-doing-process","text":"","title":"Strength of doing process"},{"location":"adrs/adr-template/#weakness-of-doing-process","text":"","title":"Weakness of doing process"},{"location":"adrs/adr-template/#threats-involved-in-not-doing-process","text":"","title":"Threats involved in not doing process"},{"location":"adrs/adr-template/#threats-involved-in-doing-process","text":"","title":"Threats involved in doing process"},{"location":"adrs/adr-template/#opportunities-involved-in-doing-process","text":"a different approach to context framework","title":"Opportunities involved in doing process"},{"location":"adrs/adr-template/#pros","text":"","title":"Pros"},{"location":"adrs/adr-template/#cons","text":"","title":"Cons"},{"location":"architecture/architecture/","text":"Architecture Overview \u00b6 With RKE2 we take lessons learned from developing and maintaining our lightweight Kubernetes distribution, K3s , and apply them to build an enterprise-ready distribution with K3s ease-of-use. What this means is that RKE2 is, at its simplest, a single binary to be installed and configured on all nodes expected to participate in the Kubernetes cluster. Once started, RKE2 is then able to bootstrap and supervise role-appropriate agents per node while sourcing needed content from the network. RKE2 brings together a number of Open Source technologies to make this all work: K3s Helm Controller K8s API Server Controller Manager Kubelet Scheduler Proxy etcd runc containerd / cri CNI : Canal ( Calico & Flannel ), Cilium or Calico CoreDNS NGINX Ingress Controller Metrics Server Helm All of these, except the NGINX Ingress Controller, are compiled and statically linked with Go+BoringCrypto . Process Lifecycle \u00b6 Content Bootstrap \u00b6 RKE2 sources binaries and manifests to run both server and agent nodes from the RKE2 Runtime image. This means RKE2 scans /var/lib/rancher/rke2/agent/images/*.tar for the rancher/rke2-runtime image (with a tag correlating to the output of rke2 --version ) by default and if it cannot be found, attempts to pull it from the network (a.k.a. Docker Hub). RKE2 then extracts /bin/ from the image, flattening it into /var/lib/rancher/rke2/data/${RKE2_DATA_KEY}/bin where ${RKE2_DATA_KEY} represents a unique string identifying the image. For RKE2 to work as expected the runtime image must minimally provide: containerd (the CRI ) containerd-shim (shims wrap runc tasks and do not stop when containerd does) containerd-shim-runc-v1 containerd-shim-runc-v2 kubelet (the Kubernetes node agent) runc (the OCI runtime) The following ops tooling is also provided by the runtime image: ctr (low level containerd maintenance and inspection) crictl (low level CRI maintenance and inspection) kubectl (kubernetes cluster maintenance and inspection) socat (needed by containerd for port-forwarding) After the binaries have been extracted RKE2 will then extract /charts/ from the image into the /var/lib/rancher/rke2/server/manifests directory. Initialize Server \u00b6 In the embedded K3s engine servers are specialized agent processes which means that the following startup will be deferred until the node container runtime has started. Prepare Components \u00b6 kube-apiserver \u00b6 Pull the kube-apiserver image, if not present already, and spin up a goroutine to wait for etcd and then write the static pod definition in /var/lib/rancher/rke2/agent/pod-manifests/ . kube-controller-manager \u00b6 Pull the kube-controller-manager image, if not present already, and spin up a goroutine to wait for kube-apiserver and then write the static pod definition in /var/lib/rancher/rke2/agent/pod-manifests/ . kube-scheduler \u00b6 Pull the kube-scheduler image, if not present already, and spin up a goroutine to wait for kube-apiserver and then write the static pod definition in /var/lib/rancher/rke2/agent/pod-manifests/ . Start Cluster \u00b6 Spin up an HTTP server in a goroutine to listen for other cluster servers/agents then initialize/join the cluster. etcd \u00b6 Pull the etcd image, if not present already, and spin up a goroutine to wait for the kubelet and then write the static pod definition in /var/lib/rancher/rke2/agent/pod-manifests/ . helm-controller \u00b6 Spin up the goroutine to start the embedded helm-controller after waiting for kube-apiserver to be ready. Initialize Agent \u00b6 The agent process entry point. For server processes the embedded K3s engine invokes this directly. Container Runtime \u00b6 containerd \u00b6 Spawn the containerd process and listen for termination. If containerd exits then the rke2 process will also exit. Node Agent \u00b6 kubelet \u00b6 Spawn and supervise the kubelet process. If kubelet exits then rke2 will attempt to restart it. Once the kubelet is running it will start any available static pods. For servers this means that etcd and kube-apiserver will start, in succession, allowing the remaining components started via static pod to connect to the kube-apiserver and begin their processing. Server Charts \u00b6 On server nodes, the helm-controller can now apply to the cluster any charts found in /var/lib/rancher/rke2/server/manifests . rke2-canal.yaml or rke2-cilium.yaml (daemonset, bootstrap) rke2-coredns.yaml (deployment, bootstrap) rke2-ingress-nginx.yaml (deployment) rke2-kube-proxy.yaml (daemonset, bootstrap) rke2-metrics-server.yaml (deployment) Daemon Process \u00b6 The RKE2 process will now run indefinitely until it receives a SIGTERM or SIGKILL or if the containerd process exits.","title":"Anatomy of a Next Generation Kubernetes Distribution"},{"location":"architecture/architecture/#architecture-overview","text":"With RKE2 we take lessons learned from developing and maintaining our lightweight Kubernetes distribution, K3s , and apply them to build an enterprise-ready distribution with K3s ease-of-use. What this means is that RKE2 is, at its simplest, a single binary to be installed and configured on all nodes expected to participate in the Kubernetes cluster. Once started, RKE2 is then able to bootstrap and supervise role-appropriate agents per node while sourcing needed content from the network. RKE2 brings together a number of Open Source technologies to make this all work: K3s Helm Controller K8s API Server Controller Manager Kubelet Scheduler Proxy etcd runc containerd / cri CNI : Canal ( Calico & Flannel ), Cilium or Calico CoreDNS NGINX Ingress Controller Metrics Server Helm All of these, except the NGINX Ingress Controller, are compiled and statically linked with Go+BoringCrypto .","title":"Architecture Overview"},{"location":"architecture/architecture/#process-lifecycle","text":"","title":"Process Lifecycle"},{"location":"architecture/architecture/#content-bootstrap","text":"RKE2 sources binaries and manifests to run both server and agent nodes from the RKE2 Runtime image. This means RKE2 scans /var/lib/rancher/rke2/agent/images/*.tar for the rancher/rke2-runtime image (with a tag correlating to the output of rke2 --version ) by default and if it cannot be found, attempts to pull it from the network (a.k.a. Docker Hub). RKE2 then extracts /bin/ from the image, flattening it into /var/lib/rancher/rke2/data/${RKE2_DATA_KEY}/bin where ${RKE2_DATA_KEY} represents a unique string identifying the image. For RKE2 to work as expected the runtime image must minimally provide: containerd (the CRI ) containerd-shim (shims wrap runc tasks and do not stop when containerd does) containerd-shim-runc-v1 containerd-shim-runc-v2 kubelet (the Kubernetes node agent) runc (the OCI runtime) The following ops tooling is also provided by the runtime image: ctr (low level containerd maintenance and inspection) crictl (low level CRI maintenance and inspection) kubectl (kubernetes cluster maintenance and inspection) socat (needed by containerd for port-forwarding) After the binaries have been extracted RKE2 will then extract /charts/ from the image into the /var/lib/rancher/rke2/server/manifests directory.","title":"Content Bootstrap"},{"location":"architecture/architecture/#initialize-server","text":"In the embedded K3s engine servers are specialized agent processes which means that the following startup will be deferred until the node container runtime has started.","title":"Initialize Server"},{"location":"architecture/architecture/#prepare-components","text":"","title":"Prepare Components"},{"location":"architecture/architecture/#kube-apiserver","text":"Pull the kube-apiserver image, if not present already, and spin up a goroutine to wait for etcd and then write the static pod definition in /var/lib/rancher/rke2/agent/pod-manifests/ .","title":"kube-apiserver"},{"location":"architecture/architecture/#kube-controller-manager","text":"Pull the kube-controller-manager image, if not present already, and spin up a goroutine to wait for kube-apiserver and then write the static pod definition in /var/lib/rancher/rke2/agent/pod-manifests/ .","title":"kube-controller-manager"},{"location":"architecture/architecture/#kube-scheduler","text":"Pull the kube-scheduler image, if not present already, and spin up a goroutine to wait for kube-apiserver and then write the static pod definition in /var/lib/rancher/rke2/agent/pod-manifests/ .","title":"kube-scheduler"},{"location":"architecture/architecture/#start-cluster","text":"Spin up an HTTP server in a goroutine to listen for other cluster servers/agents then initialize/join the cluster.","title":"Start Cluster"},{"location":"architecture/architecture/#etcd","text":"Pull the etcd image, if not present already, and spin up a goroutine to wait for the kubelet and then write the static pod definition in /var/lib/rancher/rke2/agent/pod-manifests/ .","title":"etcd"},{"location":"architecture/architecture/#helm-controller","text":"Spin up the goroutine to start the embedded helm-controller after waiting for kube-apiserver to be ready.","title":"helm-controller"},{"location":"architecture/architecture/#initialize-agent","text":"The agent process entry point. For server processes the embedded K3s engine invokes this directly.","title":"Initialize Agent"},{"location":"architecture/architecture/#container-runtime","text":"","title":"Container Runtime"},{"location":"architecture/architecture/#containerd","text":"Spawn the containerd process and listen for termination. If containerd exits then the rke2 process will also exit.","title":"containerd"},{"location":"architecture/architecture/#node-agent","text":"","title":"Node Agent"},{"location":"architecture/architecture/#kubelet","text":"Spawn and supervise the kubelet process. If kubelet exits then rke2 will attempt to restart it. Once the kubelet is running it will start any available static pods. For servers this means that etcd and kube-apiserver will start, in succession, allowing the remaining components started via static pod to connect to the kube-apiserver and begin their processing.","title":"kubelet"},{"location":"architecture/architecture/#server-charts","text":"On server nodes, the helm-controller can now apply to the cluster any charts found in /var/lib/rancher/rke2/server/manifests . rke2-canal.yaml or rke2-cilium.yaml (daemonset, bootstrap) rke2-coredns.yaml (deployment, bootstrap) rke2-ingress-nginx.yaml (deployment) rke2-kube-proxy.yaml (daemonset, bootstrap) rke2-metrics-server.yaml (deployment)","title":"Server Charts"},{"location":"architecture/architecture/#daemon-process","text":"The RKE2 process will now run indefinitely until it receives a SIGTERM or SIGKILL or if the containerd process exits.","title":"Daemon Process"},{"location":"install/airgap/","text":"Air-Gap Install \u00b6 Important: If your node has NetworkManager installed and enabled, ensure that it is configured to ignore CNI-managed interfaces. RKE2 can be installed in an air-gapped environment with two different methods. You can either deploy via the rke2-airgap-images tarball release artifact, or by using a private registry. All files mentioned in the steps can be obtained from the assets of the desired released rke2 version here . If running on an air-gapped node with SELinux enabled, you must manually install the necessary SELinux policy RPM before performing these steps. See our RPM Documentation to determine what you need. If running on an air-gapped node running SELinux, CentOS, or RHEL 8, with SELinux enabled, the following are required dependencies when doing an RPM install : Installing dependencies: container-selinux iptables libnetfilter_conntrack libnfnetlink libnftnl policycoreutils-python-utils rke2-common rke2-selinux All the steps listed on this document must be run as the root user or through sudo . Tarball Method \u00b6 Download the airgap images tarballs from the RKE release artifacts list for the version and platform of RKE2 you are using. Use rke2-images.linux-amd64.tar.zst , or rke2-images.linux-amd64.tar.gz for releases prior to v1.20. Zstandard offers better compression ratios and faster decompression speeds compared to gzip. If using the default Canal CNI ( --cni=canal ), you can use either the rke2-image legacy archive as described above, or rke2-images-core and rke2-images-canal archives. If using the alternative Cilium CNI ( --cni=cilium ), you must download the rke2-images-core and rke2-images-cilium archives instead. If using your own CNI ( --cni=none ), you can download only the rke2-images-core archive. If enabling the vSphere CPI/CSI charts ( --cloud-provider-name=rancher-vsphere ), you must also download the rke2-images-vsphere archive. Ensure that the /var/lib/rancher/rke2/agent/images/ directory exists on the node. Copy the compressed archive to /var/lib/rancher/rke2/agent/images/ on the node, ensuring that the file extension is retained. Install RKE2 Private Registry Method \u00b6 As of RKE2 v1.20, private registry support honors all settings from the containerd registry configuration . This includes endpoint override and transport protocol (HTTP/HTTPS), authentication, certificate verification, etc. Prior to RKE2 v1.20, private registries must use TLS, with a cert trusted by the host CA bundle. If the registry is using a self-signed cert, you can add the cert to the host CA bundle with update-ca-certificates . The registry must also allow anonymous (unauthenticated) access. Add all the required system images to your private registry. A list of images can be obtained from the .txt file corresponding to each tarball referenced above, or you may docker load the airgap image tarballs, then tag and push the loaded images. If using a private or self-signed certificate on the registry, add the registry's CA cert to the containerd registry configuration, or operating system's trusted certs for releases prior to v1.20. Install RKE2 using the system-default-registry parameter, or use the containerd registry configuration to use your registry as a mirror for docker.io. Install RKE2 \u00b6 The following options to install RKE2 should only be performed after completing one of either the Tarball Method or Private Registry Method . RKE2 can be installed either by running the binary directly or by using the install.sh script . RKE2 Binary Install \u00b6 Obtain the rke2 binary file rke2.linux-amd64 . Ensure the binary is named rke2 and place it in /usr/local/bin . Ensure it is executable. Run the binary with the desired parameters. For example, if using the Private Registry Method, your config file would have the following: system-default-registry : \"registry.example.com:5000\" Note: The system-default-registry parameter must specify only valid RFC 3986 URI authorities, i.e. a host and optional port. RKE2 Install.sh Script Install \u00b6 install.sh may be used in an offline mode by setting the INSTALL_RKE2_ARTIFACT_PATH variable to a path containing pre-downloaded artifacts. This will run though a normal install, including creating systemd units. Download the install script, rke2, rke2-images, and sha256sum archives from the release into a directory, as in the example below: mkdir /root/rke2-artifacts && cd /root/rke2-artifacts/ curl -OLs https://github.com/rancher/rke2/releases/download/v1.21.5%2Brke2r2/rke2-images.linux-amd64.tar.zst curl -OLs https://github.com/rancher/rke2/releases/download/v1.21.5%2Brke2r2/rke2.linux-amd64.tar.gz curl -OLs https://github.com/rancher/rke2/releases/download/v1.21.5%2Brke2r2/sha256sum-amd64.txt curl -sfL https://get.rke2.io --output install.sh Next, run install.sh using the directory, as in the example below: INSTALL_RKE2_ARTIFACT_PATH = /root/rke2-artifacts sh install.sh Enable and run the service as outlined here.","title":"Air-Gap Install"},{"location":"install/airgap/#air-gap-install","text":"Important: If your node has NetworkManager installed and enabled, ensure that it is configured to ignore CNI-managed interfaces. RKE2 can be installed in an air-gapped environment with two different methods. You can either deploy via the rke2-airgap-images tarball release artifact, or by using a private registry. All files mentioned in the steps can be obtained from the assets of the desired released rke2 version here . If running on an air-gapped node with SELinux enabled, you must manually install the necessary SELinux policy RPM before performing these steps. See our RPM Documentation to determine what you need. If running on an air-gapped node running SELinux, CentOS, or RHEL 8, with SELinux enabled, the following are required dependencies when doing an RPM install : Installing dependencies: container-selinux iptables libnetfilter_conntrack libnfnetlink libnftnl policycoreutils-python-utils rke2-common rke2-selinux All the steps listed on this document must be run as the root user or through sudo .","title":"Air-Gap Install"},{"location":"install/airgap/#tarball-method","text":"Download the airgap images tarballs from the RKE release artifacts list for the version and platform of RKE2 you are using. Use rke2-images.linux-amd64.tar.zst , or rke2-images.linux-amd64.tar.gz for releases prior to v1.20. Zstandard offers better compression ratios and faster decompression speeds compared to gzip. If using the default Canal CNI ( --cni=canal ), you can use either the rke2-image legacy archive as described above, or rke2-images-core and rke2-images-canal archives. If using the alternative Cilium CNI ( --cni=cilium ), you must download the rke2-images-core and rke2-images-cilium archives instead. If using your own CNI ( --cni=none ), you can download only the rke2-images-core archive. If enabling the vSphere CPI/CSI charts ( --cloud-provider-name=rancher-vsphere ), you must also download the rke2-images-vsphere archive. Ensure that the /var/lib/rancher/rke2/agent/images/ directory exists on the node. Copy the compressed archive to /var/lib/rancher/rke2/agent/images/ on the node, ensuring that the file extension is retained. Install RKE2","title":"Tarball Method"},{"location":"install/airgap/#private-registry-method","text":"As of RKE2 v1.20, private registry support honors all settings from the containerd registry configuration . This includes endpoint override and transport protocol (HTTP/HTTPS), authentication, certificate verification, etc. Prior to RKE2 v1.20, private registries must use TLS, with a cert trusted by the host CA bundle. If the registry is using a self-signed cert, you can add the cert to the host CA bundle with update-ca-certificates . The registry must also allow anonymous (unauthenticated) access. Add all the required system images to your private registry. A list of images can be obtained from the .txt file corresponding to each tarball referenced above, or you may docker load the airgap image tarballs, then tag and push the loaded images. If using a private or self-signed certificate on the registry, add the registry's CA cert to the containerd registry configuration, or operating system's trusted certs for releases prior to v1.20. Install RKE2 using the system-default-registry parameter, or use the containerd registry configuration to use your registry as a mirror for docker.io.","title":"Private Registry Method"},{"location":"install/airgap/#install-rke2","text":"The following options to install RKE2 should only be performed after completing one of either the Tarball Method or Private Registry Method . RKE2 can be installed either by running the binary directly or by using the install.sh script .","title":"Install RKE2"},{"location":"install/airgap/#rke2-binary-install","text":"Obtain the rke2 binary file rke2.linux-amd64 . Ensure the binary is named rke2 and place it in /usr/local/bin . Ensure it is executable. Run the binary with the desired parameters. For example, if using the Private Registry Method, your config file would have the following: system-default-registry : \"registry.example.com:5000\" Note: The system-default-registry parameter must specify only valid RFC 3986 URI authorities, i.e. a host and optional port.","title":"RKE2 Binary Install"},{"location":"install/airgap/#rke2-installsh-script-install","text":"install.sh may be used in an offline mode by setting the INSTALL_RKE2_ARTIFACT_PATH variable to a path containing pre-downloaded artifacts. This will run though a normal install, including creating systemd units. Download the install script, rke2, rke2-images, and sha256sum archives from the release into a directory, as in the example below: mkdir /root/rke2-artifacts && cd /root/rke2-artifacts/ curl -OLs https://github.com/rancher/rke2/releases/download/v1.21.5%2Brke2r2/rke2-images.linux-amd64.tar.zst curl -OLs https://github.com/rancher/rke2/releases/download/v1.21.5%2Brke2r2/rke2.linux-amd64.tar.gz curl -OLs https://github.com/rancher/rke2/releases/download/v1.21.5%2Brke2r2/sha256sum-amd64.txt curl -sfL https://get.rke2.io --output install.sh Next, run install.sh using the directory, as in the example below: INSTALL_RKE2_ARTIFACT_PATH = /root/rke2-artifacts sh install.sh Enable and run the service as outlined here.","title":"RKE2 Install.sh Script Install"},{"location":"install/containerd_registry_configuration/","text":"Containerd Registry Configuration \u00b6 Containerd can be configured to connect to private registries and use them to pull private images on each node. Upon startup, RKE2 will check to see if a registries.yaml file exists at /etc/rancher/rke2/ and instruct containerd to use any registries defined in the file. If you wish to use a private registry, then you will need to create this file as root on each node that will be using the registry. Note that server nodes are schedulable by default. If you have not tainted the server nodes and will be running workloads on them, please ensure you also create the registries.yaml file on each server as well. Note: Prior to RKE2 v1.20, containerd registry configuration is not honored for the initial RKE2 node bootstrapping, only for Kubernetes workloads that are launched after the node is joined to the cluster. Consult the airgap installation documentation if you plan on using this containerd registry feature to bootstrap nodes. Configuration in containerd can be used to connect to a private registry with a TLS connection and with registries that enable authentication as well. The following section will explain the registries.yaml file and give different examples of using private registry configuration in RKE2. Registries Configuration File \u00b6 The file consists of two main sections: mirrors configs Mirrors \u00b6 Mirrors is a directive that defines the names and endpoints of the private registries. Private registries can be used as a local mirror for the default docker.io registry, or for images where the registry is explicitly specified in the name. For example, the following configuration would pull from the private registry at https://registry.example.com:5000 for both library/busybox:latest and registry.example.com/library/busybox:latest : mirrors : docker.io : endpoint : - \"https://registry.example.com:5000\" registry.example.com : endpoint : - \"https://registry.example.com:5000\" Each mirror must have a name and set of endpoints. When pulling an image from a registry, containerd will try these endpoint URLs one by one, and use the first working one. Note: If no endpoint is configured, containerd assumes that the registry can be accessed anonymously via HTTPS on port 443, and is using a certificate trusted by the host operating system. For more information, you may consult the containerd documentation . Rewrites \u00b6 Each mirror can have a set of rewrites. Rewrites can change the tag of an image based on a regular expression. This is useful if the organization/project structure in the mirror registry is different to the upstream one. For example, the following configuration would transparently pull the image docker.io/rancher/rke2-runtime:v1.23.5-rke2r1 from registry.example.com:5000/mirrorproject/rancher-images/rke2-runtime:v1.23.5-rke2r1 : mirrors : docker.io : endpoint : - \"https://registry.example.com:5000\" rewrite : \"^rancher/(.*)\" : \"mirrorproject/rancher-images/$1\" The image will still be stored under the original name so that a crictl image ls will show docker.io/rancher/rke2-runtime:v1.23.5-rke2r1 as available on the node, even though the image was pulled from the mirrored registry with a different name. Configs \u00b6 The configs section defines the TLS and credential configuration for each mirror. For each mirror you can define auth and/or tls . The TLS part consists of: Directive Description cert_file The client certificate path that will be used to authenticate with the registry key_file The client key path that will be used to authenticate with the registry ca_file Defines the CA certificate path to be used to verify the registry's server cert file insecure_skip_verify Boolean that defines if TLS verification should be skipped for the registry The auth part consists of either username/password or authentication token: Directive Description username user name of the private registry basic auth password user password of the private registry basic auth auth authentication token of the private registry basic auth Below are basic examples of using private registries in different modes: With TLS \u00b6 Below are examples showing how you may configure /etc/rancher/rke2/registries.yaml on each node when using TLS. With Authentication: mirrors : docker.io : endpoint : - \"https://registry.example.com:5000\" configs : \"registry.example.com:5000\" : auth : username : xxxxxx # this is the registry username password : xxxxxx # this is the registry password tls : cert_file : # path to the cert file used to authenticate to the registry key_file : # path to the key file for the certificate used to authenticate to the registry ca_file : # path to the ca file used to verify the registry's certificate insecure_skip_verify : # may be set to true to skip verifying the registry's certificate Without Authentication: mirrors : docker.io : endpoint : - \"https://registry.example.com:5000\" configs : \"registry.example.com:5000\" : tls : cert_file : # path to the cert file used to authenticate to the registry key_file : # path to the key file for the certificate used to authenticate to the registry ca_file : # path to the ca file used to verify the registry's certificate insecure_skip_verify : # may be set to true to skip verifying the registry's certificate Without TLS \u00b6 Below are examples showing how you may configure /etc/rancher/rke2/registries.yaml on each node when not using TLS. Plaintext HTTP With Authentication: mirrors : docker.io : endpoint : - \"http://registry.example.com:5000\" configs : \"registry.example.com:5000\" : auth : username : xxxxxx # this is the registry username password : xxxxxx # this is the registry password Plaintext HTTP Without Authentication: mirrors : docker.io : endpoint : - \"http://registry.example.com:5000\" If using a registry using plaintext HTTP without TLS, you need to specify http:// as the endpoint URI scheme, otherwise it will default to https:// . In order for the registry changes to take effect, you need to either configure this file before starting RKE2 on the node, or restart RKE2 on each configured node.","title":"Containerd Registry Configuration"},{"location":"install/containerd_registry_configuration/#containerd-registry-configuration","text":"Containerd can be configured to connect to private registries and use them to pull private images on each node. Upon startup, RKE2 will check to see if a registries.yaml file exists at /etc/rancher/rke2/ and instruct containerd to use any registries defined in the file. If you wish to use a private registry, then you will need to create this file as root on each node that will be using the registry. Note that server nodes are schedulable by default. If you have not tainted the server nodes and will be running workloads on them, please ensure you also create the registries.yaml file on each server as well. Note: Prior to RKE2 v1.20, containerd registry configuration is not honored for the initial RKE2 node bootstrapping, only for Kubernetes workloads that are launched after the node is joined to the cluster. Consult the airgap installation documentation if you plan on using this containerd registry feature to bootstrap nodes. Configuration in containerd can be used to connect to a private registry with a TLS connection and with registries that enable authentication as well. The following section will explain the registries.yaml file and give different examples of using private registry configuration in RKE2.","title":"Containerd Registry Configuration"},{"location":"install/containerd_registry_configuration/#registries-configuration-file","text":"The file consists of two main sections: mirrors configs","title":"Registries Configuration File"},{"location":"install/containerd_registry_configuration/#mirrors","text":"Mirrors is a directive that defines the names and endpoints of the private registries. Private registries can be used as a local mirror for the default docker.io registry, or for images where the registry is explicitly specified in the name. For example, the following configuration would pull from the private registry at https://registry.example.com:5000 for both library/busybox:latest and registry.example.com/library/busybox:latest : mirrors : docker.io : endpoint : - \"https://registry.example.com:5000\" registry.example.com : endpoint : - \"https://registry.example.com:5000\" Each mirror must have a name and set of endpoints. When pulling an image from a registry, containerd will try these endpoint URLs one by one, and use the first working one. Note: If no endpoint is configured, containerd assumes that the registry can be accessed anonymously via HTTPS on port 443, and is using a certificate trusted by the host operating system. For more information, you may consult the containerd documentation .","title":"Mirrors"},{"location":"install/containerd_registry_configuration/#rewrites","text":"Each mirror can have a set of rewrites. Rewrites can change the tag of an image based on a regular expression. This is useful if the organization/project structure in the mirror registry is different to the upstream one. For example, the following configuration would transparently pull the image docker.io/rancher/rke2-runtime:v1.23.5-rke2r1 from registry.example.com:5000/mirrorproject/rancher-images/rke2-runtime:v1.23.5-rke2r1 : mirrors : docker.io : endpoint : - \"https://registry.example.com:5000\" rewrite : \"^rancher/(.*)\" : \"mirrorproject/rancher-images/$1\" The image will still be stored under the original name so that a crictl image ls will show docker.io/rancher/rke2-runtime:v1.23.5-rke2r1 as available on the node, even though the image was pulled from the mirrored registry with a different name.","title":"Rewrites"},{"location":"install/containerd_registry_configuration/#configs","text":"The configs section defines the TLS and credential configuration for each mirror. For each mirror you can define auth and/or tls . The TLS part consists of: Directive Description cert_file The client certificate path that will be used to authenticate with the registry key_file The client key path that will be used to authenticate with the registry ca_file Defines the CA certificate path to be used to verify the registry's server cert file insecure_skip_verify Boolean that defines if TLS verification should be skipped for the registry The auth part consists of either username/password or authentication token: Directive Description username user name of the private registry basic auth password user password of the private registry basic auth auth authentication token of the private registry basic auth Below are basic examples of using private registries in different modes:","title":"Configs"},{"location":"install/containerd_registry_configuration/#with-tls","text":"Below are examples showing how you may configure /etc/rancher/rke2/registries.yaml on each node when using TLS. With Authentication: mirrors : docker.io : endpoint : - \"https://registry.example.com:5000\" configs : \"registry.example.com:5000\" : auth : username : xxxxxx # this is the registry username password : xxxxxx # this is the registry password tls : cert_file : # path to the cert file used to authenticate to the registry key_file : # path to the key file for the certificate used to authenticate to the registry ca_file : # path to the ca file used to verify the registry's certificate insecure_skip_verify : # may be set to true to skip verifying the registry's certificate Without Authentication: mirrors : docker.io : endpoint : - \"https://registry.example.com:5000\" configs : \"registry.example.com:5000\" : tls : cert_file : # path to the cert file used to authenticate to the registry key_file : # path to the key file for the certificate used to authenticate to the registry ca_file : # path to the ca file used to verify the registry's certificate insecure_skip_verify : # may be set to true to skip verifying the registry's certificate","title":"With TLS"},{"location":"install/containerd_registry_configuration/#without-tls","text":"Below are examples showing how you may configure /etc/rancher/rke2/registries.yaml on each node when not using TLS. Plaintext HTTP With Authentication: mirrors : docker.io : endpoint : - \"http://registry.example.com:5000\" configs : \"registry.example.com:5000\" : auth : username : xxxxxx # this is the registry username password : xxxxxx # this is the registry password Plaintext HTTP Without Authentication: mirrors : docker.io : endpoint : - \"http://registry.example.com:5000\" If using a registry using plaintext HTTP without TLS, you need to specify http:// as the endpoint URI scheme, otherwise it will default to https:// . In order for the registry changes to take effect, you need to either configure this file before starting RKE2 on the node, or restart RKE2 on each configured node.","title":"Without TLS"},{"location":"install/ha/","text":"High Availability \u00b6 This section describes how to install a high availability (HA) RKE2 cluster. An HA RKE2 cluster consists of: A fixed registration address that is placed in front of server nodes to allow other nodes to register with the cluster An odd number (three recommended) of server nodes that will run etcd, the Kubernetes API, and other control plane services Zero or more agent nodes that are designated to run your apps and services Agents register through the fixed registration address. However, when RKE2 launches the kubelet and it must connect to the Kubernetes api-server, it does so through the rke2 agent process, which acts as a client-side load balancer. Setting up an HA cluster requires the following steps: Configure a fixed registration address Launch the first server node Join additional server nodes Join agent nodes 1. Configure the Fixed Registration Address \u00b6 Server nodes beyond the first one and all agent nodes need a URL to register against. This can be the IP or hostname of any of the server nodes, but in many cases those may change over time as nodes are created and destroyed. Therefore, you should have a stable endpoint in front of the server nodes. This endpoint can be set up using any number approaches, such as: A layer 4 (TCP) load balancer Round-robin DNS Virtual or elastic IP addresses This endpoint can also be used for accessing the Kubernetes API. So you can, for example, modify your kubeconfig file to point to it instead of a specific node. Note that the rke2 server process listens on port 9345 for new nodes to register. The Kubernetes API is served on port 6443 , as normal. Configure your load balancer accordingly. 2. Launch the first server node \u00b6 The first server node establishes the secret token that other server or agent nodes will register with when connecting to the cluster. To specify your own pre-shared secret as the token, set the token argument on startup. If you do not specify a pre-shared secret, RKE2 will generate one and place it at /var/lib/rancher/rke2/server/node-token . To avoid certificate errors with the fixed registration address, you should launch the server with the tls-san parameter set. This option adds an additional hostname or IP as a Subject Alternative Name in the server's TLS cert, and it can be specified as a list if you would like to access via both the IP and the hostname. Here is an example of what the RKE2 config file (at /etc/rancher/rke2/config.yaml ) would look like if you are following this guide. Note The RKE2 config file needs to be created manually. You can do that by running touch /etc/rancher/rke2/config.yaml as a privileged user. token : my-shared-secret tls-san : - my-kubernetes-domain.com - another-kubernetes-domain.com 2a. Optional: Consider server node taints \u00b6 By default, server nodes will be schedulable and thus your workloads can get launched on them. If you wish to have a dedicated control plane where no user workloads will run, you can use taints. The node-taint parameter will allow you to configure nodes with taints. Here is an example of adding a node taint to the configuration file: node-taint : - \"CriticalAddonsOnly=true:NoExecute\" Note: The NGINX Ingress and Metrics Server addons will not be deployed when all nodes are tainted with CriticalAddonsOnly . If your server nodes are so tainted, these addons will remain pending until untainted agent nodes are added to the cluster. 3. Launch additional server nodes \u00b6 Additional server nodes are launched much like the first, except that you must specify the server and token parameters so that they can successfully connect to the initial server node. Here is an example of what the RKE2 config file would look like for additional server nodes if you are following this guide: server : https://my-kubernetes-domain.com:9345 token : my-shared-secret tls-san : - my-kubernetes-domain.com - another-kubernetes-domain.com As mentioned previously, you must have an odd number of server nodes in total. 4. Confirm cluster is functional \u00b6 Once you've launched the rke2 server process on all server nodes, ensure that the cluster has come up properly with /var/lib/rancher/rke2/bin/kubectl \\ --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes You should see your server nodes in the Ready state. NOTE : By default, any kubectl command will require root user access, unless RKE2_KUBECONFIG_MODE override is provided. Read more about it in cluster access page . 5. Optional: Join Agent Nodes \u00b6 Because RKE2 server nodes are schedulable by default, the minimum number of nodes for an HA RKE2 server cluster is three server nodes and zero agent nodes. To add nodes designated to run your apps and services, join agent nodes to your cluster. Joining agent nodes in an HA cluster is the same as joining agent nodes in a single server cluster . You just need to specify the URL the agent should register to and the token it should use. server : https://my-kubernetes-domain.com:9345 token : my-shared-secret","title":"High Availability"},{"location":"install/ha/#high-availability","text":"This section describes how to install a high availability (HA) RKE2 cluster. An HA RKE2 cluster consists of: A fixed registration address that is placed in front of server nodes to allow other nodes to register with the cluster An odd number (three recommended) of server nodes that will run etcd, the Kubernetes API, and other control plane services Zero or more agent nodes that are designated to run your apps and services Agents register through the fixed registration address. However, when RKE2 launches the kubelet and it must connect to the Kubernetes api-server, it does so through the rke2 agent process, which acts as a client-side load balancer. Setting up an HA cluster requires the following steps: Configure a fixed registration address Launch the first server node Join additional server nodes Join agent nodes","title":"High Availability"},{"location":"install/ha/#1-configure-the-fixed-registration-address","text":"Server nodes beyond the first one and all agent nodes need a URL to register against. This can be the IP or hostname of any of the server nodes, but in many cases those may change over time as nodes are created and destroyed. Therefore, you should have a stable endpoint in front of the server nodes. This endpoint can be set up using any number approaches, such as: A layer 4 (TCP) load balancer Round-robin DNS Virtual or elastic IP addresses This endpoint can also be used for accessing the Kubernetes API. So you can, for example, modify your kubeconfig file to point to it instead of a specific node. Note that the rke2 server process listens on port 9345 for new nodes to register. The Kubernetes API is served on port 6443 , as normal. Configure your load balancer accordingly.","title":"1. Configure the Fixed Registration Address"},{"location":"install/ha/#2-launch-the-first-server-node","text":"The first server node establishes the secret token that other server or agent nodes will register with when connecting to the cluster. To specify your own pre-shared secret as the token, set the token argument on startup. If you do not specify a pre-shared secret, RKE2 will generate one and place it at /var/lib/rancher/rke2/server/node-token . To avoid certificate errors with the fixed registration address, you should launch the server with the tls-san parameter set. This option adds an additional hostname or IP as a Subject Alternative Name in the server's TLS cert, and it can be specified as a list if you would like to access via both the IP and the hostname. Here is an example of what the RKE2 config file (at /etc/rancher/rke2/config.yaml ) would look like if you are following this guide. Note The RKE2 config file needs to be created manually. You can do that by running touch /etc/rancher/rke2/config.yaml as a privileged user. token : my-shared-secret tls-san : - my-kubernetes-domain.com - another-kubernetes-domain.com","title":"2. Launch the first server node"},{"location":"install/ha/#2a-optional-consider-server-node-taints","text":"By default, server nodes will be schedulable and thus your workloads can get launched on them. If you wish to have a dedicated control plane where no user workloads will run, you can use taints. The node-taint parameter will allow you to configure nodes with taints. Here is an example of adding a node taint to the configuration file: node-taint : - \"CriticalAddonsOnly=true:NoExecute\" Note: The NGINX Ingress and Metrics Server addons will not be deployed when all nodes are tainted with CriticalAddonsOnly . If your server nodes are so tainted, these addons will remain pending until untainted agent nodes are added to the cluster.","title":"2a. Optional: Consider server node taints"},{"location":"install/ha/#3-launch-additional-server-nodes","text":"Additional server nodes are launched much like the first, except that you must specify the server and token parameters so that they can successfully connect to the initial server node. Here is an example of what the RKE2 config file would look like for additional server nodes if you are following this guide: server : https://my-kubernetes-domain.com:9345 token : my-shared-secret tls-san : - my-kubernetes-domain.com - another-kubernetes-domain.com As mentioned previously, you must have an odd number of server nodes in total.","title":"3. Launch additional server nodes"},{"location":"install/ha/#4-confirm-cluster-is-functional","text":"Once you've launched the rke2 server process on all server nodes, ensure that the cluster has come up properly with /var/lib/rancher/rke2/bin/kubectl \\ --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes You should see your server nodes in the Ready state. NOTE : By default, any kubectl command will require root user access, unless RKE2_KUBECONFIG_MODE override is provided. Read more about it in cluster access page .","title":"4. Confirm cluster is functional"},{"location":"install/ha/#5-optional-join-agent-nodes","text":"Because RKE2 server nodes are schedulable by default, the minimum number of nodes for an HA RKE2 server cluster is three server nodes and zero agent nodes. To add nodes designated to run your apps and services, join agent nodes to your cluster. Joining agent nodes in an HA cluster is the same as joining agent nodes in a single server cluster . You just need to specify the URL the agent should register to and the token it should use. server : https://my-kubernetes-domain.com:9345 token : my-shared-secret","title":"5. Optional: Join Agent Nodes"},{"location":"install/linux_uninstall/","text":"Linux Uninstall \u00b6 Note: Uninstalling RKE2 deletes the cluster data and all of the scripts. Depending on the method used to install RKE2, the uninstallation process varies. RPM Method \u00b6 To uninstall RKE2 installed via the RPM method from your system, simply run the commands corresponding to the version of RKE2 you have installed, either as the root user or through sudo . This will shutdown RKE2 process, remove the RKE2 RPMs, and clean up files used by RKE2. RKE2 v1.18.13+rke2r1 and newer RKE2 Prior to v1.18.13+rke2r1 RKE2 Prior to v1.18.11+rke2r1 Starting with RKE2 v1.18.13+rke2r1 , the bundled rke2-uninstall.sh script will remove the corresponding RPM packages during the uninstall process. Simply run the following command: /usr/bin/rke2-uninstall.sh If you are running a version of RKE2 that is older than v1.18.13+rke2r1 , you will need to manually remove the RKE2 RPMs after calling the rke2-uninstall.sh script. /usr/bin/rke2-uninstall.sh yum remove -y 'rke2-*' rm -rf /run/k3s RPM based installs older than and including v1.18.10+rke2r1 did not package the rke2-uninstall.sh script. These instructions provide guidance on how to download and use the necessary scripts. First, remove the corresponding RKE2 packages and /run/k3s directory. yum remove -y 'rke2-*' rm -rf /run/k3s Once those commands are run, the rke2-uninstall.sh and rke2-killall.sh scripts should be downloaded. These two scripts will stop any running containers and processes, clean up used processes, and ultimately remove RKE2 from the system. Run the commands below. curl -sL https://raw.githubusercontent.com/rancher/rke2/master/bundle/bin/rke2-uninstall.sh --output rke2-uninstall.sh chmod +x rke2-uninstall.sh mv rke2-uninstall.sh /usr/local/bin curl -sL https://raw.githubusercontent.com/rancher/rke2/master/bundle/bin/rke2-killall.sh --output rke2-killall.sh chmod +x rke2-killall.sh mv rke2-killall.sh /usr/local/bin Now run the rke2-uninstall.sh script. This will also call the rke2-killall.sh. /usr/local/bin/rke2-uninstall.sh Tarball Method \u00b6 To uninstall RKE2 installed via the Tarball method from your system, simply run the command below. This will shutdown process, remove the RKE2 binary, and clean up files used by RKE2. /usr/local/bin/rke2-uninstall.sh","title":"Linux Uninstall"},{"location":"install/linux_uninstall/#linux-uninstall","text":"Note: Uninstalling RKE2 deletes the cluster data and all of the scripts. Depending on the method used to install RKE2, the uninstallation process varies.","title":"Linux Uninstall"},{"location":"install/linux_uninstall/#rpm-method","text":"To uninstall RKE2 installed via the RPM method from your system, simply run the commands corresponding to the version of RKE2 you have installed, either as the root user or through sudo . This will shutdown RKE2 process, remove the RKE2 RPMs, and clean up files used by RKE2. RKE2 v1.18.13+rke2r1 and newer RKE2 Prior to v1.18.13+rke2r1 RKE2 Prior to v1.18.11+rke2r1 Starting with RKE2 v1.18.13+rke2r1 , the bundled rke2-uninstall.sh script will remove the corresponding RPM packages during the uninstall process. Simply run the following command: /usr/bin/rke2-uninstall.sh If you are running a version of RKE2 that is older than v1.18.13+rke2r1 , you will need to manually remove the RKE2 RPMs after calling the rke2-uninstall.sh script. /usr/bin/rke2-uninstall.sh yum remove -y 'rke2-*' rm -rf /run/k3s RPM based installs older than and including v1.18.10+rke2r1 did not package the rke2-uninstall.sh script. These instructions provide guidance on how to download and use the necessary scripts. First, remove the corresponding RKE2 packages and /run/k3s directory. yum remove -y 'rke2-*' rm -rf /run/k3s Once those commands are run, the rke2-uninstall.sh and rke2-killall.sh scripts should be downloaded. These two scripts will stop any running containers and processes, clean up used processes, and ultimately remove RKE2 from the system. Run the commands below. curl -sL https://raw.githubusercontent.com/rancher/rke2/master/bundle/bin/rke2-uninstall.sh --output rke2-uninstall.sh chmod +x rke2-uninstall.sh mv rke2-uninstall.sh /usr/local/bin curl -sL https://raw.githubusercontent.com/rancher/rke2/master/bundle/bin/rke2-killall.sh --output rke2-killall.sh chmod +x rke2-killall.sh mv rke2-killall.sh /usr/local/bin Now run the rke2-uninstall.sh script. This will also call the rke2-killall.sh. /usr/local/bin/rke2-uninstall.sh","title":"RPM Method"},{"location":"install/linux_uninstall/#tarball-method","text":"To uninstall RKE2 installed via the Tarball method from your system, simply run the command below. This will shutdown process, remove the RKE2 binary, and clean up files used by RKE2. /usr/local/bin/rke2-uninstall.sh","title":"Tarball Method"},{"location":"install/methods/","text":"Important: If your node has NetworkManager installed and enabled, ensure that it is configured to ignore CNI-managed interfaces. RKE2 can be installed to a system in a number of ways, two of which are the preferred and supported methods. Those methods are tarball and RPM. The install script referenced in the Quick Start is a wrapper around these two methods. This document explains these installation methods in greater detail. Tarball \u00b6 To install RKE2 via install you first need to get the install script. This can be done in a number of ways. This gets the script and immediately starts the install process. # curl -sfL https://get.rke2.io | sudo sh - curl -sfL https://get.rke2.io | sh - This will download the install script and make it executable. curl -sfL https://get.rke2.io --output install.sh chmod +x install.sh Installation \u00b6 The install process defaults to the latest RKE2 version and no other qualifiers are necessary. However, if you want specify a version, you should set the INSTALL_RKE2_CHANNEL environment variable. An example below: INSTALL_RKE2_CHANNEL = latest ./install.sh When the install script is executed, it makes a determination of what type of system it is. If it's an OS that uses RPMs (such as CentOS or RHEL), it will perform an RPM based installation, otherwise the script defaults to tarball. RPM based installation is covered below. Next, the installation script downloads the tarball, verifies it by comparing SHA256 hashes, and lastly, extracts the contents to /usr/local . An operator is free to move the files after installation if desired. This operation simply extracts the tarball and no other system modifications are made. Tarball structure / contents bin - contains the RKE2 executable as well as the rke2-killall.sh and rke2-uninstall.sh scripts lib - contains server and agent systemd unit files share - contains the RKE2 license as well as a sysctl configuration file used for when RKE2 is ran in CIS mode To configure the system any further, you'll want to reference the either the server or agent documentation. RPM \u00b6 To start the RPM install process, you need to get the installation script which is covered above. The script will check your system for rpm , yum , or dnf and if any of those exist, it determines that the system is Redhat based and starts the RPM install process. Files are installed with the prefix of /usr rather than /usr/local . Repositories \u00b6 Signed RPMs are published for RKE2 within the rpm-testing.rancher.io and rpm.rancher.io RPM repositories. If you run the https://get.rke2.io script on nodes supporting RPMs, it will use these RPM repos by default. But you can also install them yourself. The RPMs provide systemd units for managing rke2 , but will need to be configured via configuration file before starting the services for the first time. Enterprise Linux 7 \u00b6 In order to use the RPM repository, on a CentOS 7 or RHEL 7 system, run the following bash snippet: cat << EOF > /etc/yum.repos.d/rancher-rke2-1-18-latest.repo [rancher-rke2-common-latest] name=Rancher RKE2 Common Latest baseurl=https://rpm.rancher.io/rke2/latest/common/centos/7/noarch enabled=1 gpgcheck=1 gpgkey=https://rpm.rancher.io/public.key [rancher-rke2-1-18-latest] name=Rancher RKE2 1.18 Latest baseurl=https://rpm.rancher.io/rke2/latest/1.18/centos/7/x86_64 enabled=1 gpgcheck=1 gpgkey=https://rpm.rancher.io/public.key EOF Enterprise Linux 8 \u00b6 In order to use the RPM repository, on a CentOS 8 or RHEL 8 system, run the following bash snippet: cat << EOF > /etc/yum.repos.d/rancher-rke2-1-18-latest.repo [rancher-rke2-common-latest] name=Rancher RKE2 Common Latest baseurl=https://rpm.rancher.io/rke2/latest/common/centos/8/noarch enabled=1 gpgcheck=1 gpgkey=https://rpm.rancher.io/public.key [rancher-rke2-1-18-latest] name=Rancher RKE2 1.18 Latest baseurl=https://rpm.rancher.io/rke2/latest/1.18/centos/8/x86_64 enabled=1 gpgcheck=1 gpgkey=https://rpm.rancher.io/public.key EOF Installation \u00b6 After the repository is configured, you can run either of the following commands: yum -y install rke2-server or yum -y install rke2-agent The RPM will install a corresponding rke2-server.service or rke2-agent.service systemd unit that can be invoked like: systemctl start rke2-server . Make sure that you configure rke2 before you start it, by following the Configuration File instructions below. Manual \u00b6 The RKE2 binary is statically compiled and linked which allows for the RKE2 binary to be portable across Linux distributions without the concern for dependency issues. The simplest installation is to download the binary, make sure it's executable, and copy it into the ${PATH} , generally /usr/local/bin . After first execution, RKE2 will create all necessary directories and files. To configure the system any further, you'll want to reference the config file documentation.","title":"Installation Methods"},{"location":"install/methods/#tarball","text":"To install RKE2 via install you first need to get the install script. This can be done in a number of ways. This gets the script and immediately starts the install process. # curl -sfL https://get.rke2.io | sudo sh - curl -sfL https://get.rke2.io | sh - This will download the install script and make it executable. curl -sfL https://get.rke2.io --output install.sh chmod +x install.sh","title":"Tarball"},{"location":"install/methods/#installation","text":"The install process defaults to the latest RKE2 version and no other qualifiers are necessary. However, if you want specify a version, you should set the INSTALL_RKE2_CHANNEL environment variable. An example below: INSTALL_RKE2_CHANNEL = latest ./install.sh When the install script is executed, it makes a determination of what type of system it is. If it's an OS that uses RPMs (such as CentOS or RHEL), it will perform an RPM based installation, otherwise the script defaults to tarball. RPM based installation is covered below. Next, the installation script downloads the tarball, verifies it by comparing SHA256 hashes, and lastly, extracts the contents to /usr/local . An operator is free to move the files after installation if desired. This operation simply extracts the tarball and no other system modifications are made. Tarball structure / contents bin - contains the RKE2 executable as well as the rke2-killall.sh and rke2-uninstall.sh scripts lib - contains server and agent systemd unit files share - contains the RKE2 license as well as a sysctl configuration file used for when RKE2 is ran in CIS mode To configure the system any further, you'll want to reference the either the server or agent documentation.","title":"Installation"},{"location":"install/methods/#rpm","text":"To start the RPM install process, you need to get the installation script which is covered above. The script will check your system for rpm , yum , or dnf and if any of those exist, it determines that the system is Redhat based and starts the RPM install process. Files are installed with the prefix of /usr rather than /usr/local .","title":"RPM"},{"location":"install/methods/#repositories","text":"Signed RPMs are published for RKE2 within the rpm-testing.rancher.io and rpm.rancher.io RPM repositories. If you run the https://get.rke2.io script on nodes supporting RPMs, it will use these RPM repos by default. But you can also install them yourself. The RPMs provide systemd units for managing rke2 , but will need to be configured via configuration file before starting the services for the first time.","title":"Repositories"},{"location":"install/methods/#enterprise-linux-7","text":"In order to use the RPM repository, on a CentOS 7 or RHEL 7 system, run the following bash snippet: cat << EOF > /etc/yum.repos.d/rancher-rke2-1-18-latest.repo [rancher-rke2-common-latest] name=Rancher RKE2 Common Latest baseurl=https://rpm.rancher.io/rke2/latest/common/centos/7/noarch enabled=1 gpgcheck=1 gpgkey=https://rpm.rancher.io/public.key [rancher-rke2-1-18-latest] name=Rancher RKE2 1.18 Latest baseurl=https://rpm.rancher.io/rke2/latest/1.18/centos/7/x86_64 enabled=1 gpgcheck=1 gpgkey=https://rpm.rancher.io/public.key EOF","title":"Enterprise Linux 7"},{"location":"install/methods/#enterprise-linux-8","text":"In order to use the RPM repository, on a CentOS 8 or RHEL 8 system, run the following bash snippet: cat << EOF > /etc/yum.repos.d/rancher-rke2-1-18-latest.repo [rancher-rke2-common-latest] name=Rancher RKE2 Common Latest baseurl=https://rpm.rancher.io/rke2/latest/common/centos/8/noarch enabled=1 gpgcheck=1 gpgkey=https://rpm.rancher.io/public.key [rancher-rke2-1-18-latest] name=Rancher RKE2 1.18 Latest baseurl=https://rpm.rancher.io/rke2/latest/1.18/centos/8/x86_64 enabled=1 gpgcheck=1 gpgkey=https://rpm.rancher.io/public.key EOF","title":"Enterprise Linux 8"},{"location":"install/methods/#installation_1","text":"After the repository is configured, you can run either of the following commands: yum -y install rke2-server or yum -y install rke2-agent The RPM will install a corresponding rke2-server.service or rke2-agent.service systemd unit that can be invoked like: systemctl start rke2-server . Make sure that you configure rke2 before you start it, by following the Configuration File instructions below.","title":"Installation"},{"location":"install/methods/#manual","text":"The RKE2 binary is statically compiled and linked which allows for the RKE2 binary to be portable across Linux distributions without the concern for dependency issues. The simplest installation is to download the binary, make sure it's executable, and copy it into the ${PATH} , generally /usr/local/bin . After first execution, RKE2 will create all necessary directories and files. To configure the system any further, you'll want to reference the config file documentation.","title":"Manual"},{"location":"install/network_options/","text":"Network Options \u00b6 RKE2 requires a CNI plugin to connect pods and services. The Canal CNI plugin is the default and has been supported since the beginning. Starting with RKE2 v1.21, there are two extra supported CNI plugins: Calico and Cilium. All CNI plugins get installed via a helm chart after the main components are up and running and can be customized by modifying the helm chart options. This page focuses on the network options available when setting up RKE2: Install a CNI plugin Dual-stack configuration Using Multus Install a CNI plugin \u00b6 The next tabs inform how to deploy each CNI plugin and override the default options: Canal CNI plugin Cilium CNI plugin Calico CNI plugin Canal means using Flannel for inter-node traffic and Calico for intra-node traffic and network policies. By default, it will use vxlan encapsulation to create an overlay network among nodes. Canal is deployed by default in RKE2 and thus nothing must be configured to activate it. To override the default Canal options you should create a HelmChartConfig resource. The HelmChartConfig resource must match the name and namespace of its corresponding HelmChart. For example to override the flannel interface, you can apply the following config: apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-canal namespace : kube-system spec : valuesContent : |- flannel: iface: \"eth1\" Starting with RKE2 v1.23 it is possible to use flannel's wireguard backend for in-kernel WireGuard encapsulation and encryption ( Users of kernels < 5.6 need to install a module ). This can be achieved using the following config: apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-canal namespace : kube-system spec : valuesContent : |- flannel: backend: \"wireguard\" After that, please restart the canal daemonset to use the newer config by executing: kubectl rollout restart ds rke2-canal -n kube-system For more information about the full options of the Canal config please refer to the rke2-charts . Note: Canal requires the iptables or xtables-nft package to be installed on the node. Warning: Canal is currently not supported on clusters with Windows nodes. Please check Known issues and Limitations if you experience IP allocation problems Starting with RKE2 v1.21, Cilium can be deployed as the CNI plugin. To do so, pass cilium as the value of the --cni flag. To override the default options, please use a HelmChartConfig resource. The HelmChartConfig resource must match the name and namespace of its corresponding HelmChart. For example, to enable eni: apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-cilium namespace : kube-system spec : valuesContent : |- eni: enabled: true For more information about values available in the Cilium chart, please refer to the rke2-charts repository Cilium includes advanced features to fully replace kube-proxy and implement the routing of services using eBPF instead of iptables. It is not recommended to replace kube-proxy by Cilium if your kernel is not v5.8 or newer, as important bug fixes and features will be missing. To activate this mode, deploy rke2 with the flag --disable-kube-proxy and the following cilium configuration: apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-cilium namespace : kube-system spec : valuesContent : |- kubeProxyReplacement: strict k8sServiceHost: REPLACE_WITH_API_SERVER_IP k8sServicePort: REPLACE_WITH_API_SERVER_PORT For more information, please check the upstream docs Warning: Cilium is currently not supported in the Windows installation of RKE2 Starting with RKE2 v1.21, Calico can be deployed as the CNI plugin. To do so, pass calico as the value of the --cni flag. To override the default options, please use a HelmChartConfig resource. The HelmChartConfig resource must match the name and namespace of its corresponding HelmChart. For example, to change the mtu: apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-calico namespace : kube-system spec : valuesContent : |- installation: calicoNetwork: mtu: 9000 For more information about values available for the Calico chart, please refer to the rke2-charts repository Note: Calico requires the iptables or xtables-nft package to be installed on the node. Dual-stack configuration \u00b6 IPv4/IPv6 dual-stack networking enables the allocation of both IPv4 and IPv6 addresses to Pods and Services. It is supported in RKE2 since v1.21, stable since v1.23 but not activated by default. To activate it correctly, both RKE2 and the chosen CNI plugin must be configured accordingly. To configure RKE2 in dual-stack mode, in the control-plane nodes, you must set a valid IPv4/IPv6 dual-stack cidr for pods and services. Moreover, in both control-plane and worker nodes, you must set a dual-stack node-ip, which includes the IPv4 and IPv6 address of the node. To do so, use the flags --cluster-cidr , --service-cidr and --node-ip for example: ```bash --cluster-cidr 10.42.0.0/16,2001:cafe:42:0::/56 --service-cidr 10.43.0.0/16,2001:cafe:42:1::/112 --node-ip 10.0.10.40,2a02:d091:a6f:4691:58c6:8609:a6d5:d1c3 ``` Each CNI plugin requires a different configuration for dual-stack: Canal CNI plugin Cilium CNI plugin Calico CNI plugin Canal automatically detects the RKE2 configuration for dual-stack and does not need any extra configuration. Dual-stack is currently not supported in the windows installations of RKE2. Cilium automatically detects the RKE2 configuration for dual-stack and does not need any extra configuration Calico automatically detects the RKE2 configuration for dual-stack and does not need any extra configuration. When deployed in dual-stack mode, it creates two different ippool resources. Note that when using dual-stack, calico leverages BGP instead of VXLAN encapsulation. Dual-stack and BGP are currently not supported in the windows installations of RKE2. IPv6 setup \u00b6 In case of IPv6 only configuration RKE2 needs to use localhost to access the liveness URL of the ETCD pod; check that your operating system configures /etc/hosts file correctly: ::1 localhost Using Multus \u00b6 Starting with RKE2 v1.21 it is possible to deploy the Multus CNI meta-plugin. Note that this is for advanced users. Multus CNI is a CNI plugin that enables attaching multiple network interfaces to pods. Multus does not replace CNI plugins, instead it acts as a CNI plugin multiplexer. Multus is useful in certain use cases, especially when pods are network intensive and require extra network interfaces that support dataplane acceleration techniques such as SR-IOV. Multus can not be deployed standalone. It always requires at least one conventional CNI plugin that fulfills the Kubernetes cluster network requirements. That CNI plugin becomes the default for Multus, and will be used to provide the primary interface for all pods. To enable Multus, pass multus as the first value to the --cni flag, followed by the name of the plugin you want to use alongside Multus (or none if you will provide your own default plugin). Note that multus must always be in the first position of the list. For example, to use Multus with canal as the default plugin you could specify --cni=multus,canal or --cni=multus --cni=canal . For more information about Multus, refer to the multus-cni documentation. Using Multus with Cilium \u00b6 To use Cilium with Multus the exclusive config needs to be disabled. You can do this by creating a file named /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yml with the following content: apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-cilium namespace : kube-system spec : valuesContent : |- cni: exclusive: false Using Multus with the containernetworking plugins \u00b6 Any CNI plugin can be used as secondary CNI plugin for Multus to provide additional network interfaces attached to a pod. However, it is most common to use the CNI plugins maintained by the containernetworking team (bridge, host-device, macvlan, etc) as secondary CNI plugins for Multus. These containernetworking plugins are automatically deployed when installing Multus. For more information about these plugins, refer to the containernetworking plugins documentation. To use any of these plugins, a proper NetworkAttachmentDefinition object will need to be created to define the configuration of the secondary network. The definition is then referenced by pod annotations, which Multus will use to provide extra interfaces to that pod. An example using the macvlan cni plugin with Mu is available in the multus-cni repo . Using Multus with the Whereabouts CNI \u00b6 Whereabouts is an IP Address Management (IPAM) CNI plugin that assigns IP addresses cluster-wide. Starting with RKE2 1.22, RKE2 includes the option to use Whereabouts with Multus to manage the IP addresses of the additional interfaces created through Multus. In order to do this, you need to use HelmChartConfig to configure the Multus CNI to use Whereabouts. You can do this by creating a file named /var/lib/rancher/rke2/server/manifests/rke2-multus-config.yml with the following content: apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-multus namespace : kube-system spec : valuesContent : |- rke2-whereabouts: enabled: true This will configure the chart for Multus to use rke2-whereabouts as a dependency. If you want to customize the Whereabouts image, this is possible like this: apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-multus namespace : kube-system spec : valuesContent : |- rke2-whereabouts: enabled: true image: repository: ghcr.io/k8snetworkplumbingwg/whereabouts tag: latest-amd64 NOTE: You should write this file before starting rke2. Using Multus with SR-IOV (experimental) \u00b6 Please note this is an experimental feature introduced with v1.21.2+rke2r1. Using the SR-IOV CNI with Multus can help with data-plane acceleration use cases, providing an extra interface in the pod that can achieve very high throughput. SR-IOV will not work in all environments, and there are several requirements that must be fulfilled to consider the node as SR-IOV capable: Physical NIC must support SR-IOV (e.g. by checking /sys/class/net/$NIC/device/sriov_totalvfs) The host operating system must activate IOMMU virtualization The host operating system includes drivers capable of doing sriov (e.g. i40e, vfio-pci, etc) The SR-IOV CNI plugin cannot be used as the default CNI plugin for Multus; it must be deployed alongside both Multus and a traditional CNI plugin. The SR-IOV CNI helm chart can be found in the rancher-charts Helm repo. For more information see Rancher Helm Charts documentation . After installing the SR-IOV CNI chart, the SR-IOV operator will be deployed. Then, the user must specify what nodes in the cluster are SR-IOV capable by labeling them with feature.node.kubernetes.io/network-sriov.capable=true : kubectl label node $NODE-NAME feature.node.kubernetes.io/network-sriov.capable=true Once labeled, the sriov-network-config Daemonset will deploy a pod to the node to collect information about the network interfaces. That information is available through the sriovnetworknodestates Custom Resource Definition. A couple of minutes after the deployment, there will be one sriovnetworknodestates resource per node, with the name of the node as the resource name. For more information about how to use the SR-IOV operator, please refer to sriov-network-operator","title":"Network Options"},{"location":"install/network_options/#network-options","text":"RKE2 requires a CNI plugin to connect pods and services. The Canal CNI plugin is the default and has been supported since the beginning. Starting with RKE2 v1.21, there are two extra supported CNI plugins: Calico and Cilium. All CNI plugins get installed via a helm chart after the main components are up and running and can be customized by modifying the helm chart options. This page focuses on the network options available when setting up RKE2: Install a CNI plugin Dual-stack configuration Using Multus","title":"Network Options"},{"location":"install/network_options/#install-a-cni-plugin","text":"The next tabs inform how to deploy each CNI plugin and override the default options: Canal CNI plugin Cilium CNI plugin Calico CNI plugin Canal means using Flannel for inter-node traffic and Calico for intra-node traffic and network policies. By default, it will use vxlan encapsulation to create an overlay network among nodes. Canal is deployed by default in RKE2 and thus nothing must be configured to activate it. To override the default Canal options you should create a HelmChartConfig resource. The HelmChartConfig resource must match the name and namespace of its corresponding HelmChart. For example to override the flannel interface, you can apply the following config: apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-canal namespace : kube-system spec : valuesContent : |- flannel: iface: \"eth1\" Starting with RKE2 v1.23 it is possible to use flannel's wireguard backend for in-kernel WireGuard encapsulation and encryption ( Users of kernels < 5.6 need to install a module ). This can be achieved using the following config: apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-canal namespace : kube-system spec : valuesContent : |- flannel: backend: \"wireguard\" After that, please restart the canal daemonset to use the newer config by executing: kubectl rollout restart ds rke2-canal -n kube-system For more information about the full options of the Canal config please refer to the rke2-charts . Note: Canal requires the iptables or xtables-nft package to be installed on the node. Warning: Canal is currently not supported on clusters with Windows nodes. Please check Known issues and Limitations if you experience IP allocation problems Starting with RKE2 v1.21, Cilium can be deployed as the CNI plugin. To do so, pass cilium as the value of the --cni flag. To override the default options, please use a HelmChartConfig resource. The HelmChartConfig resource must match the name and namespace of its corresponding HelmChart. For example, to enable eni: apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-cilium namespace : kube-system spec : valuesContent : |- eni: enabled: true For more information about values available in the Cilium chart, please refer to the rke2-charts repository Cilium includes advanced features to fully replace kube-proxy and implement the routing of services using eBPF instead of iptables. It is not recommended to replace kube-proxy by Cilium if your kernel is not v5.8 or newer, as important bug fixes and features will be missing. To activate this mode, deploy rke2 with the flag --disable-kube-proxy and the following cilium configuration: apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-cilium namespace : kube-system spec : valuesContent : |- kubeProxyReplacement: strict k8sServiceHost: REPLACE_WITH_API_SERVER_IP k8sServicePort: REPLACE_WITH_API_SERVER_PORT For more information, please check the upstream docs Warning: Cilium is currently not supported in the Windows installation of RKE2 Starting with RKE2 v1.21, Calico can be deployed as the CNI plugin. To do so, pass calico as the value of the --cni flag. To override the default options, please use a HelmChartConfig resource. The HelmChartConfig resource must match the name and namespace of its corresponding HelmChart. For example, to change the mtu: apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-calico namespace : kube-system spec : valuesContent : |- installation: calicoNetwork: mtu: 9000 For more information about values available for the Calico chart, please refer to the rke2-charts repository Note: Calico requires the iptables or xtables-nft package to be installed on the node.","title":"Install a CNI plugin"},{"location":"install/network_options/#dual-stack-configuration","text":"IPv4/IPv6 dual-stack networking enables the allocation of both IPv4 and IPv6 addresses to Pods and Services. It is supported in RKE2 since v1.21, stable since v1.23 but not activated by default. To activate it correctly, both RKE2 and the chosen CNI plugin must be configured accordingly. To configure RKE2 in dual-stack mode, in the control-plane nodes, you must set a valid IPv4/IPv6 dual-stack cidr for pods and services. Moreover, in both control-plane and worker nodes, you must set a dual-stack node-ip, which includes the IPv4 and IPv6 address of the node. To do so, use the flags --cluster-cidr , --service-cidr and --node-ip for example: ```bash --cluster-cidr 10.42.0.0/16,2001:cafe:42:0::/56 --service-cidr 10.43.0.0/16,2001:cafe:42:1::/112 --node-ip 10.0.10.40,2a02:d091:a6f:4691:58c6:8609:a6d5:d1c3 ``` Each CNI plugin requires a different configuration for dual-stack: Canal CNI plugin Cilium CNI plugin Calico CNI plugin Canal automatically detects the RKE2 configuration for dual-stack and does not need any extra configuration. Dual-stack is currently not supported in the windows installations of RKE2. Cilium automatically detects the RKE2 configuration for dual-stack and does not need any extra configuration Calico automatically detects the RKE2 configuration for dual-stack and does not need any extra configuration. When deployed in dual-stack mode, it creates two different ippool resources. Note that when using dual-stack, calico leverages BGP instead of VXLAN encapsulation. Dual-stack and BGP are currently not supported in the windows installations of RKE2.","title":"Dual-stack configuration"},{"location":"install/network_options/#ipv6-setup","text":"In case of IPv6 only configuration RKE2 needs to use localhost to access the liveness URL of the ETCD pod; check that your operating system configures /etc/hosts file correctly: ::1 localhost","title":"IPv6 setup"},{"location":"install/network_options/#using-multus","text":"Starting with RKE2 v1.21 it is possible to deploy the Multus CNI meta-plugin. Note that this is for advanced users. Multus CNI is a CNI plugin that enables attaching multiple network interfaces to pods. Multus does not replace CNI plugins, instead it acts as a CNI plugin multiplexer. Multus is useful in certain use cases, especially when pods are network intensive and require extra network interfaces that support dataplane acceleration techniques such as SR-IOV. Multus can not be deployed standalone. It always requires at least one conventional CNI plugin that fulfills the Kubernetes cluster network requirements. That CNI plugin becomes the default for Multus, and will be used to provide the primary interface for all pods. To enable Multus, pass multus as the first value to the --cni flag, followed by the name of the plugin you want to use alongside Multus (or none if you will provide your own default plugin). Note that multus must always be in the first position of the list. For example, to use Multus with canal as the default plugin you could specify --cni=multus,canal or --cni=multus --cni=canal . For more information about Multus, refer to the multus-cni documentation.","title":"Using Multus"},{"location":"install/network_options/#using-multus-with-cilium","text":"To use Cilium with Multus the exclusive config needs to be disabled. You can do this by creating a file named /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yml with the following content: apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-cilium namespace : kube-system spec : valuesContent : |- cni: exclusive: false","title":"Using Multus with Cilium"},{"location":"install/network_options/#using-multus-with-the-containernetworking-plugins","text":"Any CNI plugin can be used as secondary CNI plugin for Multus to provide additional network interfaces attached to a pod. However, it is most common to use the CNI plugins maintained by the containernetworking team (bridge, host-device, macvlan, etc) as secondary CNI plugins for Multus. These containernetworking plugins are automatically deployed when installing Multus. For more information about these plugins, refer to the containernetworking plugins documentation. To use any of these plugins, a proper NetworkAttachmentDefinition object will need to be created to define the configuration of the secondary network. The definition is then referenced by pod annotations, which Multus will use to provide extra interfaces to that pod. An example using the macvlan cni plugin with Mu is available in the multus-cni repo .","title":"Using Multus with the containernetworking plugins"},{"location":"install/network_options/#using-multus-with-the-whereabouts-cni","text":"Whereabouts is an IP Address Management (IPAM) CNI plugin that assigns IP addresses cluster-wide. Starting with RKE2 1.22, RKE2 includes the option to use Whereabouts with Multus to manage the IP addresses of the additional interfaces created through Multus. In order to do this, you need to use HelmChartConfig to configure the Multus CNI to use Whereabouts. You can do this by creating a file named /var/lib/rancher/rke2/server/manifests/rke2-multus-config.yml with the following content: apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-multus namespace : kube-system spec : valuesContent : |- rke2-whereabouts: enabled: true This will configure the chart for Multus to use rke2-whereabouts as a dependency. If you want to customize the Whereabouts image, this is possible like this: apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-multus namespace : kube-system spec : valuesContent : |- rke2-whereabouts: enabled: true image: repository: ghcr.io/k8snetworkplumbingwg/whereabouts tag: latest-amd64 NOTE: You should write this file before starting rke2.","title":"Using Multus with the Whereabouts CNI"},{"location":"install/network_options/#using-multus-with-sr-iov-experimental","text":"Please note this is an experimental feature introduced with v1.21.2+rke2r1. Using the SR-IOV CNI with Multus can help with data-plane acceleration use cases, providing an extra interface in the pod that can achieve very high throughput. SR-IOV will not work in all environments, and there are several requirements that must be fulfilled to consider the node as SR-IOV capable: Physical NIC must support SR-IOV (e.g. by checking /sys/class/net/$NIC/device/sriov_totalvfs) The host operating system must activate IOMMU virtualization The host operating system includes drivers capable of doing sriov (e.g. i40e, vfio-pci, etc) The SR-IOV CNI plugin cannot be used as the default CNI plugin for Multus; it must be deployed alongside both Multus and a traditional CNI plugin. The SR-IOV CNI helm chart can be found in the rancher-charts Helm repo. For more information see Rancher Helm Charts documentation . After installing the SR-IOV CNI chart, the SR-IOV operator will be deployed. Then, the user must specify what nodes in the cluster are SR-IOV capable by labeling them with feature.node.kubernetes.io/network-sriov.capable=true : kubectl label node $NODE-NAME feature.node.kubernetes.io/network-sriov.capable=true Once labeled, the sriov-network-config Daemonset will deploy a pod to the node to collect information about the network interfaces. That information is available through the sriovnetworknodestates Custom Resource Definition. A couple of minutes after the deployment, there will be one sriovnetworknodestates resource per node, with the name of the node as the resource name. For more information about how to use the SR-IOV operator, please refer to sriov-network-operator","title":"Using Multus with SR-IOV (experimental)"},{"location":"install/quickstart/","text":"This guide will help you quickly launch a cluster with default options. New to Kubernetes? The official Kubernetes docs already have some great tutorials outlining the basics here . Prerequisites \u00b6 Make sure your environment fulfills the requirements. If NetworkManager is installed and enabled on your hosts, ensure that it is configured to ignore CNI-managed interfaces. For RKE2 versions 1.21 and higher, if the host kernel supports AppArmor , the AppArmor tools (usually available via the apparmor-parser package) must also be present prior to installing RKE2. The RKE2 installation process must be run as the root user or through sudo . Server Node Installation \u00b6 RKE2 provides an installation script that is a convenient way to install it as a service on systemd based systems. This script is available at https://get.rke2.io. To install RKE2 using this method do the following: 1. Run the installer \u00b6 curl -sfL https://get.rke2.io | sh - This will install the rke2-server service and the rke2 binary onto your machine. Due to its nature, It will fail unless it runs as the root user or through sudo . 2. Enable the rke2-server service \u00b6 systemctl enable rke2-server.service 3. Start the service \u00b6 systemctl start rke2-server.service 4. Follow the logs, if you like \u00b6 journalctl -u rke2-server -f After running this installation: The rke2-server service will be installed. The rke2-server service will be configured to automatically restart after node reboots or if the process crashes or is killed. Additional utilities will be installed at /var/lib/rancher/rke2/bin/ . They include: kubectl , crictl , and ctr . Note that these are not on your path by default. Two cleanup scripts will be installed to the path at /usr/local/bin/rke2 . They are: rke2-killall.sh and rke2-uninstall.sh . A kubeconfig file will be written to /etc/rancher/rke2/rke2.yaml . A token that can be used to register other server or agent nodes will be created at /var/lib/rancher/rke2/server/node-token Note: If you are adding additional server nodes, you must have an odd number in total. An odd number is needed to maintain quorum. See the High Availability documentation for more details. Linux Agent (Worker) Node Installation \u00b6 The steps on this section requires root level access or sudo to work. 1. Run the installer \u00b6 curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE = \"agent\" sh - This will install the rke2-agent service and the rke2 binary onto your machine. Due to its nature, It will fail unless it runs as the root user or through sudo . 2. Enable the rke2-agent service \u00b6 systemctl enable rke2-agent.service 3. Configure the rke2-agent service \u00b6 mkdir -p /etc/rancher/rke2/ vim /etc/rancher/rke2/config.yaml Content for config.yaml: server : https://<server>:9345 token : <token from server node> Note: The rke2 server process listens on port 9345 for new nodes to register. The Kubernetes API is still served on port 6443 , as normal. 4. Start the service \u00b6 systemctl start rke2-agent.service Follow the logs, if you like journalctl -u rke2-agent -f Note: Each machine must have a unique hostname. If your machines do not have unique hostnames, set the node-name parameter in the config.yaml file and provide a value with a valid and unique hostname for each node. To read more about the config.yaml file, see the Install Options documentation. Windows Agent (Worker) Node Installation \u00b6 Windows Support is currently Experimental as of v1.21.3+rke2r1 Windows Support requires choosing Calico as the CNI for the RKE2 cluster 0. Prepare the Windows Agent Node \u00b6 Note The Windows Server Containers feature needs to be enabled for the RKE2 agent to work. Open a new Powershell window with Administrator privileges powershell -Command \"Start-Process PowerShell -Verb RunAs\" In the new Powershell window, run the following command. Enable-WindowsOptionalFeature -Online -FeatureName containers \u2013 All This will require a reboot for the Containers feature to properly function. 1. Download the Install Script \u00b6 Invoke-WebRequest -Uri https :// raw . githubusercontent . com / rancher / rke2 / master / install . ps1 -Outfile install . ps1 This script will download the rke2.exe Windows binary onto your machine. 2. Configure the rke2-agent for Windows \u00b6 New-Item -Type Directory c :/ etc / rancher / rke2 -Force Set-Content -Path c :/ etc / rancher / rke2 / config . yaml -Value @\" server: https://<server>:9345 token: <token from server node> \"@ To read more about the config.yaml file, see the Install Options documentation. 3. Configure PATH \u00b6 $env:PATH += \";c:\\var\\lib\\rancher\\rke2\\bin;c:\\usr\\local\\bin\" [Environment] :: SetEnvironmentVariable ( \"Path\" , [Environment] :: GetEnvironmentVariable ( \"Path\" , [EnvironmentVariableTarget] :: Machine ) + \";c:\\var\\lib\\rancher\\rke2\\bin;c:\\usr\\local\\bin\" , [EnvironmentVariableTarget] :: Machine ) 4. Run the Installer \u00b6 ./ install . ps1 5. Start the Windows RKE2 Service \u00b6 rke2 . exe agent service - -add Note: Each machine must have a unique hostname. If you would prefer to use CLI parameters only instead, run the binary with the desired parameters. rke2 . exe agent - -token <> - -server <>","title":"Quick Start"},{"location":"install/quickstart/#prerequisites","text":"Make sure your environment fulfills the requirements. If NetworkManager is installed and enabled on your hosts, ensure that it is configured to ignore CNI-managed interfaces. For RKE2 versions 1.21 and higher, if the host kernel supports AppArmor , the AppArmor tools (usually available via the apparmor-parser package) must also be present prior to installing RKE2. The RKE2 installation process must be run as the root user or through sudo .","title":"Prerequisites"},{"location":"install/quickstart/#server-node-installation","text":"RKE2 provides an installation script that is a convenient way to install it as a service on systemd based systems. This script is available at https://get.rke2.io. To install RKE2 using this method do the following:","title":"Server Node Installation"},{"location":"install/quickstart/#1-run-the-installer","text":"curl -sfL https://get.rke2.io | sh - This will install the rke2-server service and the rke2 binary onto your machine. Due to its nature, It will fail unless it runs as the root user or through sudo .","title":"1. Run the installer"},{"location":"install/quickstart/#2-enable-the-rke2-server-service","text":"systemctl enable rke2-server.service","title":"2. Enable the rke2-server service"},{"location":"install/quickstart/#3-start-the-service","text":"systemctl start rke2-server.service","title":"3. Start the service"},{"location":"install/quickstart/#4-follow-the-logs-if-you-like","text":"journalctl -u rke2-server -f After running this installation: The rke2-server service will be installed. The rke2-server service will be configured to automatically restart after node reboots or if the process crashes or is killed. Additional utilities will be installed at /var/lib/rancher/rke2/bin/ . They include: kubectl , crictl , and ctr . Note that these are not on your path by default. Two cleanup scripts will be installed to the path at /usr/local/bin/rke2 . They are: rke2-killall.sh and rke2-uninstall.sh . A kubeconfig file will be written to /etc/rancher/rke2/rke2.yaml . A token that can be used to register other server or agent nodes will be created at /var/lib/rancher/rke2/server/node-token Note: If you are adding additional server nodes, you must have an odd number in total. An odd number is needed to maintain quorum. See the High Availability documentation for more details.","title":"4. Follow the logs, if you like"},{"location":"install/quickstart/#linux-agent-worker-node-installation","text":"The steps on this section requires root level access or sudo to work.","title":"Linux Agent (Worker) Node Installation"},{"location":"install/quickstart/#1-run-the-installer_1","text":"curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE = \"agent\" sh - This will install the rke2-agent service and the rke2 binary onto your machine. Due to its nature, It will fail unless it runs as the root user or through sudo .","title":"1. Run the installer"},{"location":"install/quickstart/#2-enable-the-rke2-agent-service","text":"systemctl enable rke2-agent.service","title":"2. Enable the rke2-agent service"},{"location":"install/quickstart/#3-configure-the-rke2-agent-service","text":"mkdir -p /etc/rancher/rke2/ vim /etc/rancher/rke2/config.yaml Content for config.yaml: server : https://<server>:9345 token : <token from server node> Note: The rke2 server process listens on port 9345 for new nodes to register. The Kubernetes API is still served on port 6443 , as normal.","title":"3. Configure the rke2-agent service"},{"location":"install/quickstart/#4-start-the-service","text":"systemctl start rke2-agent.service Follow the logs, if you like journalctl -u rke2-agent -f Note: Each machine must have a unique hostname. If your machines do not have unique hostnames, set the node-name parameter in the config.yaml file and provide a value with a valid and unique hostname for each node. To read more about the config.yaml file, see the Install Options documentation.","title":"4. Start the service"},{"location":"install/quickstart/#windows-agent-worker-node-installation","text":"Windows Support is currently Experimental as of v1.21.3+rke2r1 Windows Support requires choosing Calico as the CNI for the RKE2 cluster","title":"Windows Agent (Worker) Node Installation"},{"location":"install/quickstart/#0-prepare-the-windows-agent-node","text":"Note The Windows Server Containers feature needs to be enabled for the RKE2 agent to work. Open a new Powershell window with Administrator privileges powershell -Command \"Start-Process PowerShell -Verb RunAs\" In the new Powershell window, run the following command. Enable-WindowsOptionalFeature -Online -FeatureName containers \u2013 All This will require a reboot for the Containers feature to properly function.","title":"0. Prepare the Windows Agent Node"},{"location":"install/quickstart/#1-download-the-install-script","text":"Invoke-WebRequest -Uri https :// raw . githubusercontent . com / rancher / rke2 / master / install . ps1 -Outfile install . ps1 This script will download the rke2.exe Windows binary onto your machine.","title":"1. Download the Install Script"},{"location":"install/quickstart/#2-configure-the-rke2-agent-for-windows","text":"New-Item -Type Directory c :/ etc / rancher / rke2 -Force Set-Content -Path c :/ etc / rancher / rke2 / config . yaml -Value @\" server: https://<server>:9345 token: <token from server node> \"@ To read more about the config.yaml file, see the Install Options documentation.","title":"2. Configure the rke2-agent for Windows"},{"location":"install/quickstart/#3-configure-path","text":"$env:PATH += \";c:\\var\\lib\\rancher\\rke2\\bin;c:\\usr\\local\\bin\" [Environment] :: SetEnvironmentVariable ( \"Path\" , [Environment] :: GetEnvironmentVariable ( \"Path\" , [EnvironmentVariableTarget] :: Machine ) + \";c:\\var\\lib\\rancher\\rke2\\bin;c:\\usr\\local\\bin\" , [EnvironmentVariableTarget] :: Machine )","title":"3. Configure PATH"},{"location":"install/quickstart/#4-run-the-installer","text":"./ install . ps1","title":"4. Run the Installer"},{"location":"install/quickstart/#5-start-the-windows-rke2-service","text":"rke2 . exe agent service - -add Note: Each machine must have a unique hostname. If you would prefer to use CLI parameters only instead, run the binary with the desired parameters. rke2 . exe agent - -token <> - -server <>","title":"5. Start the Windows RKE2 Service"},{"location":"install/requirements/","text":"RKE2 is very lightweight, but has some minimum requirements as outlined below. Prerequisites \u00b6 Two nodes cannot have the same hostname. If all your nodes have the same hostname, set the node-name parameter in the RKE2 config file for each node you add to the cluster to have a different node name. Operating Systems \u00b6 Linux \u00b6 RKE2 has been tested and validated on the following operating systems, and their subsequent non-major releases: Ubuntu 18.04 and 20.04 (amd64) CentOS/RHEL 7.8 (amd64) Rocky/RHEL 8.5 (amd64) SLES 15 SP3, OpenSUSE, SLE Micro 5.1 (amd64) Windows \u00b6 Windows Support is currently Experimental as of v1.21.3+rke2r1 Windows Support requires choosing Calico as the CNI for the RKE2 cluster The RKE2 Windows Node (Worker) agent has been tested and validated on the following operating systems, and their subsequent non-major releases: Windows Server 2019 LTSC (amd64) (OS Build 17763.2061) Windows Server 2022 LTSC (amd64) (OS Build 20348.169) Note The Windows Server Containers feature needs to be enabled for the RKE2 Windows agent to work. Open a new Powershell window with Administrator privileges powershell -Command \"Start-Process PowerShell -Verb RunAs\" In the new Powershell window, run the following command. Enable-WindowsOptionalFeature -Online -FeatureName Containers \u2013 All This will require a reboot for the Containers feature to properly function. Hardware \u00b6 Hardware requirements scale based on the size of your deployments. Minimum recommendations are outlined here. Linux/Windows \u00b6 RAM: 4GB Minimum (we recommend at least 8GB) CPU: 2 Minimum (we recommend at least 4CPU) Disks \u00b6 RKE2 performance depends on the performance of the database, and since RKE2 runs etcd embeddedly and it stores the data dir on disk, we recommend using an SSD when possible to ensure optimal performance. Networking \u00b6 Important: If your node has NetworkManager installed and enabled, ensure that it is configured to ignore CNI-managed interfaces. . If your node has Wicked installed and enabled, ensure that the forwarding sysctl config is enabled The RKE2 server needs port 6443 and 9345 to be accessible by other nodes in the cluster. All nodes need to be able to reach other nodes over UDP port 8472 when Flannel VXLAN is used. If you wish to utilize the metrics server, you will need to open port 10250 on each node. Important: The VXLAN port on nodes should not be exposed to the world as it opens up your cluster network to be accessed by anyone. Run your nodes behind a firewall/security group that disables access to port 8472. Inbound Rules for RKE2 Server Nodes Protocol Port Source Description TCP 9345 RKE2 agent nodes Kubernetes API TCP 6443 RKE2 agent nodes Kubernetes API UDP 8472 RKE2 server and agent nodes Required only for Flannel VXLAN TCP 10250 RKE2 server and agent nodes kubelet TCP 2379 RKE2 server nodes etcd client port TCP 2380 RKE2 server nodes etcd peer port TCP 30000-32767 RKE2 server and agent nodes NodePort port range UDP 8472 RKE2 server and agent nodes Cilium CNI VXLAN TCP 4240 RKE2 server and agent nodes Cilium CNI health checks ICMP 8/0 RKE2 server and agent nodes Cilium CNI health checks TCP 179 RKE2 server and agent nodes Calico CNI with BGP UDP 4789 RKE2 server and agent nodes Calico CNI with VXLAN TCP 5473 RKE2 server and agent nodes Calico CNI with Typha TCP 9098 RKE2 server and agent nodes Calico Typha health checks TCP 9099 RKE2 server and agent nodes Calico health checks TCP 5473 RKE2 server and agent nodes Calico CNI with Typha UDP 8472 RKE2 server and agent nodes Canal CNI with VXLAN TCP 9099 RKE2 server and agent nodes Canal CNI health checks UDP 51820 RKE2 server and agent nodes Canal CNI with WireGuard IPv4 UDP 51821 RKE2 server and agent nodes Canal CNI with WireGuard IPv6/dual-stack Inbound Rules for RKE2 Windows Agent Nodes Windows Specific Inbound Network Rules \u00b6 Protocol Port Source Description UDP 4789 RKE2 server nodes Required for Calico and Flannel VXLAN Typically, all outbound traffic will be allowed.","title":"Requirements"},{"location":"install/requirements/#prerequisites","text":"Two nodes cannot have the same hostname. If all your nodes have the same hostname, set the node-name parameter in the RKE2 config file for each node you add to the cluster to have a different node name.","title":"Prerequisites"},{"location":"install/requirements/#operating-systems","text":"","title":"Operating Systems"},{"location":"install/requirements/#linux","text":"RKE2 has been tested and validated on the following operating systems, and their subsequent non-major releases: Ubuntu 18.04 and 20.04 (amd64) CentOS/RHEL 7.8 (amd64) Rocky/RHEL 8.5 (amd64) SLES 15 SP3, OpenSUSE, SLE Micro 5.1 (amd64)","title":"Linux"},{"location":"install/requirements/#windows","text":"Windows Support is currently Experimental as of v1.21.3+rke2r1 Windows Support requires choosing Calico as the CNI for the RKE2 cluster The RKE2 Windows Node (Worker) agent has been tested and validated on the following operating systems, and their subsequent non-major releases: Windows Server 2019 LTSC (amd64) (OS Build 17763.2061) Windows Server 2022 LTSC (amd64) (OS Build 20348.169) Note The Windows Server Containers feature needs to be enabled for the RKE2 Windows agent to work. Open a new Powershell window with Administrator privileges powershell -Command \"Start-Process PowerShell -Verb RunAs\" In the new Powershell window, run the following command. Enable-WindowsOptionalFeature -Online -FeatureName Containers \u2013 All This will require a reboot for the Containers feature to properly function.","title":"Windows"},{"location":"install/requirements/#hardware","text":"Hardware requirements scale based on the size of your deployments. Minimum recommendations are outlined here.","title":"Hardware"},{"location":"install/requirements/#linuxwindows","text":"RAM: 4GB Minimum (we recommend at least 8GB) CPU: 2 Minimum (we recommend at least 4CPU)","title":"Linux/Windows"},{"location":"install/requirements/#disks","text":"RKE2 performance depends on the performance of the database, and since RKE2 runs etcd embeddedly and it stores the data dir on disk, we recommend using an SSD when possible to ensure optimal performance.","title":"Disks"},{"location":"install/requirements/#networking","text":"Important: If your node has NetworkManager installed and enabled, ensure that it is configured to ignore CNI-managed interfaces. . If your node has Wicked installed and enabled, ensure that the forwarding sysctl config is enabled The RKE2 server needs port 6443 and 9345 to be accessible by other nodes in the cluster. All nodes need to be able to reach other nodes over UDP port 8472 when Flannel VXLAN is used. If you wish to utilize the metrics server, you will need to open port 10250 on each node. Important: The VXLAN port on nodes should not be exposed to the world as it opens up your cluster network to be accessed by anyone. Run your nodes behind a firewall/security group that disables access to port 8472. Inbound Rules for RKE2 Server Nodes Protocol Port Source Description TCP 9345 RKE2 agent nodes Kubernetes API TCP 6443 RKE2 agent nodes Kubernetes API UDP 8472 RKE2 server and agent nodes Required only for Flannel VXLAN TCP 10250 RKE2 server and agent nodes kubelet TCP 2379 RKE2 server nodes etcd client port TCP 2380 RKE2 server nodes etcd peer port TCP 30000-32767 RKE2 server and agent nodes NodePort port range UDP 8472 RKE2 server and agent nodes Cilium CNI VXLAN TCP 4240 RKE2 server and agent nodes Cilium CNI health checks ICMP 8/0 RKE2 server and agent nodes Cilium CNI health checks TCP 179 RKE2 server and agent nodes Calico CNI with BGP UDP 4789 RKE2 server and agent nodes Calico CNI with VXLAN TCP 5473 RKE2 server and agent nodes Calico CNI with Typha TCP 9098 RKE2 server and agent nodes Calico Typha health checks TCP 9099 RKE2 server and agent nodes Calico health checks TCP 5473 RKE2 server and agent nodes Calico CNI with Typha UDP 8472 RKE2 server and agent nodes Canal CNI with VXLAN TCP 9099 RKE2 server and agent nodes Canal CNI health checks UDP 51820 RKE2 server and agent nodes Canal CNI with WireGuard IPv4 UDP 51821 RKE2 server and agent nodes Canal CNI with WireGuard IPv6/dual-stack Inbound Rules for RKE2 Windows Agent Nodes","title":"Networking"},{"location":"install/requirements/#windows-specific-inbound-network-rules","text":"Protocol Port Source Description UDP 4789 RKE2 server nodes Required for Calico and Flannel VXLAN Typically, all outbound traffic will be allowed.","title":"Windows Specific Inbound Network Rules"},{"location":"install/windows_airgap/","text":"Windows Air-Gap Install \u00b6 Windows Support is currently Experimental as of v1.21.3+rke2r1 Windows Support requires choosing Calico as the CNI for the RKE2 cluster RKE2 Windows Agent (Worker) Nodes can be used in an air-gapped environment with two different methods. This requires first completing the RKE2 airgap setup You can either deploy using the rke2-windows-<BUILD_VERSION>-amd64-images.tar.gz tarball release artifact, or by using a private registry. There are currently three tarball artifacts released for Windows in accordance with our validated Windows versions . rke2-windows-1809-amd64-images.tar.gz rke2-windows-2004-amd64-images.tar.gz rke2-windows-20H2-amd64-images.tar.gz All files mentioned in the steps can be obtained from the assets of the desired released rke2 version here . Prepare the Windows Agent Node \u00b6 Note The Windows Server Containers feature needs to be enabled for the RKE2 agent to work. Open a new Powershell window with Administrator privileges powershell -Command \"Start-Process PowerShell -Verb RunAs\" In the new Powershell window, run the following command. Enable-WindowsOptionalFeature -Online -FeatureName containers \u2013 All This will require a reboot for the Containers feature to properly function. Windows Tarball Method \u00b6 Download the Windows images tarballs and binary from the RKE2 release artifacts list for the version of RKE2 that you are using. Using tar.gz image tarballs \u00b6 Windows Server 2019 LTSC (amd64) (OS Build 17763.2061) $ProgressPreference = 'SilentlyContinue' Invoke-WebRequest https :// github . com / rancher / rke2 / releases / download / v1 . 21 . 4 % 2Brke2r2 / rke2-windows - 1809-amd64-images . tar . gz -OutFile / var / lib / rancher / rke2 / agent / images / rke2-windows - 1809-amd64-images . tar . gz Windows Server SAC 2004 (amd64) (OS Build 19041.1110) $ProgressPreference = 'SilentlyContinue' Invoke-WebRequest https :// github . com / rancher / rke2 / releases / download / v1 . 21 . 4 % 2Brke2r2 / rke2-windows - 2004-amd64-images . tar . gz -OutFile c :/ var / lib / rancher / rke2 / agent / images / rke2-windows - 2004-amd64-images . tar . gz Windows Server SAC 20H2 (amd64) (OS Build 19042.1110) $ProgressPreference = 'SilentlyContinue' Invoke-WebRequest https :// github . com / rancher / rke2 / releases / download / v1 . 21 . 4 % 2Brke2r2 / rke2-windows - 20H2-amd64-images . tar . gz -OutFile c :/ var / lib / rancher / rke2 / agent / images / rke2-windows - 20H2-amd64-images . tar . gz Using tar.zst image tarballs \u00b6 Windows Server 2019 LTSC (amd64) (OS Build 17763.2061) $ProgressPreference = 'SilentlyContinue' Invoke-WebRequest https :// github . com / rancher / rke2 / releases / download / v1 . 21 . 4 % 2Brke2r2 / rke2-windows - 1809-amd64-images . tar . zst -OutFile / var / lib / rancher / rke2 / agent / images / rke2-windows - 1809-amd64-images . tar . zst Windows Server SAC 2004 (amd64) (OS Build 19041.1110) $ProgressPreference = 'SilentlyContinue' Invoke-WebRequest https :// github . com / rancher / rke2 / releases / download / v1 . 21 . 4 % 2Brke2r2 / rke2-windows - 2004-amd64-images . tar . zst -OutFile c :/ var / lib / rancher / rke2 / agent / images / rke2-windows - 2004-amd64-images . tar . zst Windows Server SAC 20H2 (amd64) (OS Build 19042.1110) $ProgressPreference = 'SilentlyContinue' Invoke-WebRequest hhttps :// github . com / rancher / rke2 / releases / download / v1 . 21 . 4 % 2Brke2r2 / rke2-windows - 20H2-amd64-images . tar . zst -OutFile c :/ var / lib / rancher / rke2 / agent / images / rke2-windows - 20H2-amd64-images . tar . zst Use rke2-windows-<BUILD_VERSION>-amd64.tar.gz or rke2-windows-<BUILD_VERSION>-amd64.tar.zst . Zstandard offers better compression ratios and faster decompression speeds compared to pigz. Ensure that the /var/lib/rancher/rke2/agent/images/ directory exists on the node. New-Item -Type Directory c :\\ usr \\ local \\ bin -Force New-Item -Type Directory c :\\ var \\ lib \\ rancher \\ rke2 \\ bin -Force Copy the compressed archive to /var/lib/rancher/rke2/agent/images/ on the node, ensuring that the file extension is retained. Install RKE2 Private Registry Method \u00b6 As of RKE2 v1.20, private registry support honors all settings from the containerd registry configuration . This includes endpoint override and transport protocol (HTTP/HTTPS), authentication, certificate verification, etc. Prior to RKE2 v1.20, private registries must use TLS, with a cert trusted by the host CA bundle. If the registry is using a self-signed cert, you can add the cert to the host CA bundle with update-ca-certificates . The registry must also allow anonymous (unauthenticated) access. Add all the required system images to your private registry. A list of images can be obtained from the .txt file corresponding to each tarball referenced above, or you may docker load the airgap image tarballs, then tag and push the loaded images. If using a private or self-signed certificate on the registry, add the registry's CA cert to the containerd registry configuration, or operating system's trusted certs for releases prior to v1.20. Install RKE2 using the system-default-registry parameter, or use the containerd registry configuration to use your registry as a mirror for docker.io. Install Windows RKE2 \u00b6 These steps should only be performed after completing one of either the Tarball Method or Private Registry Method . Obtain the Windows RKE2 binary file rke2-windows-amd64.exe . Ensure the binary is named rke2.exe and place it in c:/usr/local/bin . Invoke-WebRequest https :// github . com / rancher / rke2 / releases / download / v1 . 21 . 4 % 2Brke2r2 / rke2-windows-amd64 . exe -OutFile c :/ usr / local / bin / rke2 . exe Configure the rke2-agent for Windows New-Item -Type Directory c :/ etc / rancher / rke2 -Force Set-Content -Path c :/ etc / rancher / rke2 / config . yaml -Value @\" server: https://<server>:9345 token: <token from server node> \"@ To read more about the config.yaml file, see the Install Options documentation. Configure your PATH $env:PATH += \";c:\\var\\lib\\rancher\\rke2\\bin;c:\\usr\\local\\bin\" [Environment] :: SetEnvironmentVariable ( \"Path\" , [Environment] :: GetEnvironmentVariable ( \"Path\" , [EnvironmentVariableTarget] :: Machine ) + \";c:\\var\\lib\\rancher\\rke2\\bin;c:\\usr\\local\\bin\" , [EnvironmentVariableTarget] :: Machine ) Start the RKE2 Windows service by running the binary with the desired parameters. Please see the Windows Agent Configuration reference for additional parameters. c :\\ usr \\ local \\ bin \\ rke2 . exe agent service - -add For example, if using the Private Registry Method, your config file would have the following: system-default-registry : \"registry.example.com:5000\" Note: The system-default-registry parameter must specify only valid RFC 3986 URI authorities, i.e. a host and optional port. If you would prefer to use CLI parameters only instead, run the binary with the desired parameters. c :/ usr / local / bin / rke2 . exe agent - -token <> - -server <>","title":"Windows Air-Gap Install"},{"location":"install/windows_airgap/#windows-air-gap-install","text":"Windows Support is currently Experimental as of v1.21.3+rke2r1 Windows Support requires choosing Calico as the CNI for the RKE2 cluster RKE2 Windows Agent (Worker) Nodes can be used in an air-gapped environment with two different methods. This requires first completing the RKE2 airgap setup You can either deploy using the rke2-windows-<BUILD_VERSION>-amd64-images.tar.gz tarball release artifact, or by using a private registry. There are currently three tarball artifacts released for Windows in accordance with our validated Windows versions . rke2-windows-1809-amd64-images.tar.gz rke2-windows-2004-amd64-images.tar.gz rke2-windows-20H2-amd64-images.tar.gz All files mentioned in the steps can be obtained from the assets of the desired released rke2 version here .","title":"Windows Air-Gap Install"},{"location":"install/windows_airgap/#prepare-the-windows-agent-node","text":"Note The Windows Server Containers feature needs to be enabled for the RKE2 agent to work. Open a new Powershell window with Administrator privileges powershell -Command \"Start-Process PowerShell -Verb RunAs\" In the new Powershell window, run the following command. Enable-WindowsOptionalFeature -Online -FeatureName containers \u2013 All This will require a reboot for the Containers feature to properly function.","title":"Prepare the Windows Agent Node"},{"location":"install/windows_airgap/#windows-tarball-method","text":"Download the Windows images tarballs and binary from the RKE2 release artifacts list for the version of RKE2 that you are using.","title":"Windows Tarball Method"},{"location":"install/windows_airgap/#using-targz-image-tarballs","text":"Windows Server 2019 LTSC (amd64) (OS Build 17763.2061) $ProgressPreference = 'SilentlyContinue' Invoke-WebRequest https :// github . com / rancher / rke2 / releases / download / v1 . 21 . 4 % 2Brke2r2 / rke2-windows - 1809-amd64-images . tar . gz -OutFile / var / lib / rancher / rke2 / agent / images / rke2-windows - 1809-amd64-images . tar . gz Windows Server SAC 2004 (amd64) (OS Build 19041.1110) $ProgressPreference = 'SilentlyContinue' Invoke-WebRequest https :// github . com / rancher / rke2 / releases / download / v1 . 21 . 4 % 2Brke2r2 / rke2-windows - 2004-amd64-images . tar . gz -OutFile c :/ var / lib / rancher / rke2 / agent / images / rke2-windows - 2004-amd64-images . tar . gz Windows Server SAC 20H2 (amd64) (OS Build 19042.1110) $ProgressPreference = 'SilentlyContinue' Invoke-WebRequest https :// github . com / rancher / rke2 / releases / download / v1 . 21 . 4 % 2Brke2r2 / rke2-windows - 20H2-amd64-images . tar . gz -OutFile c :/ var / lib / rancher / rke2 / agent / images / rke2-windows - 20H2-amd64-images . tar . gz","title":"Using tar.gz image tarballs"},{"location":"install/windows_airgap/#using-tarzst-image-tarballs","text":"Windows Server 2019 LTSC (amd64) (OS Build 17763.2061) $ProgressPreference = 'SilentlyContinue' Invoke-WebRequest https :// github . com / rancher / rke2 / releases / download / v1 . 21 . 4 % 2Brke2r2 / rke2-windows - 1809-amd64-images . tar . zst -OutFile / var / lib / rancher / rke2 / agent / images / rke2-windows - 1809-amd64-images . tar . zst Windows Server SAC 2004 (amd64) (OS Build 19041.1110) $ProgressPreference = 'SilentlyContinue' Invoke-WebRequest https :// github . com / rancher / rke2 / releases / download / v1 . 21 . 4 % 2Brke2r2 / rke2-windows - 2004-amd64-images . tar . zst -OutFile c :/ var / lib / rancher / rke2 / agent / images / rke2-windows - 2004-amd64-images . tar . zst Windows Server SAC 20H2 (amd64) (OS Build 19042.1110) $ProgressPreference = 'SilentlyContinue' Invoke-WebRequest hhttps :// github . com / rancher / rke2 / releases / download / v1 . 21 . 4 % 2Brke2r2 / rke2-windows - 20H2-amd64-images . tar . zst -OutFile c :/ var / lib / rancher / rke2 / agent / images / rke2-windows - 20H2-amd64-images . tar . zst Use rke2-windows-<BUILD_VERSION>-amd64.tar.gz or rke2-windows-<BUILD_VERSION>-amd64.tar.zst . Zstandard offers better compression ratios and faster decompression speeds compared to pigz. Ensure that the /var/lib/rancher/rke2/agent/images/ directory exists on the node. New-Item -Type Directory c :\\ usr \\ local \\ bin -Force New-Item -Type Directory c :\\ var \\ lib \\ rancher \\ rke2 \\ bin -Force Copy the compressed archive to /var/lib/rancher/rke2/agent/images/ on the node, ensuring that the file extension is retained. Install RKE2","title":"Using tar.zst image tarballs"},{"location":"install/windows_airgap/#private-registry-method","text":"As of RKE2 v1.20, private registry support honors all settings from the containerd registry configuration . This includes endpoint override and transport protocol (HTTP/HTTPS), authentication, certificate verification, etc. Prior to RKE2 v1.20, private registries must use TLS, with a cert trusted by the host CA bundle. If the registry is using a self-signed cert, you can add the cert to the host CA bundle with update-ca-certificates . The registry must also allow anonymous (unauthenticated) access. Add all the required system images to your private registry. A list of images can be obtained from the .txt file corresponding to each tarball referenced above, or you may docker load the airgap image tarballs, then tag and push the loaded images. If using a private or self-signed certificate on the registry, add the registry's CA cert to the containerd registry configuration, or operating system's trusted certs for releases prior to v1.20. Install RKE2 using the system-default-registry parameter, or use the containerd registry configuration to use your registry as a mirror for docker.io.","title":"Private Registry Method"},{"location":"install/windows_airgap/#install-windows-rke2","text":"These steps should only be performed after completing one of either the Tarball Method or Private Registry Method . Obtain the Windows RKE2 binary file rke2-windows-amd64.exe . Ensure the binary is named rke2.exe and place it in c:/usr/local/bin . Invoke-WebRequest https :// github . com / rancher / rke2 / releases / download / v1 . 21 . 4 % 2Brke2r2 / rke2-windows-amd64 . exe -OutFile c :/ usr / local / bin / rke2 . exe Configure the rke2-agent for Windows New-Item -Type Directory c :/ etc / rancher / rke2 -Force Set-Content -Path c :/ etc / rancher / rke2 / config . yaml -Value @\" server: https://<server>:9345 token: <token from server node> \"@ To read more about the config.yaml file, see the Install Options documentation. Configure your PATH $env:PATH += \";c:\\var\\lib\\rancher\\rke2\\bin;c:\\usr\\local\\bin\" [Environment] :: SetEnvironmentVariable ( \"Path\" , [Environment] :: GetEnvironmentVariable ( \"Path\" , [EnvironmentVariableTarget] :: Machine ) + \";c:\\var\\lib\\rancher\\rke2\\bin;c:\\usr\\local\\bin\" , [EnvironmentVariableTarget] :: Machine ) Start the RKE2 Windows service by running the binary with the desired parameters. Please see the Windows Agent Configuration reference for additional parameters. c :\\ usr \\ local \\ bin \\ rke2 . exe agent service - -add For example, if using the Private Registry Method, your config file would have the following: system-default-registry : \"registry.example.com:5000\" Note: The system-default-registry parameter must specify only valid RFC 3986 URI authorities, i.e. a host and optional port. If you would prefer to use CLI parameters only instead, run the binary with the desired parameters. c :/ usr / local / bin / rke2 . exe agent - -token <> - -server <>","title":"Install Windows RKE2"},{"location":"install/windows_uninstall/","text":"Windows Uninstall \u00b6 Note: Uninstalling the RKE2 Windows Agent deletes all of the node data. Depending on the method used to install RKE2, the uninstallation process varies. Tarball Method \u00b6 To uninstall the RKE2 Windows Agent installed via the tarball method from your system, simply run the command below. This will shutdown all RKE2 Windows processes, remove the RKE2 Windows binary, and clean up the files used by RKE2. c :/ usr / local / bin / rke2-uninstall . ps1","title":"Windows Uninstall"},{"location":"install/windows_uninstall/#windows-uninstall","text":"Note: Uninstalling the RKE2 Windows Agent deletes all of the node data. Depending on the method used to install RKE2, the uninstallation process varies.","title":"Windows Uninstall"},{"location":"install/windows_uninstall/#tarball-method","text":"To uninstall the RKE2 Windows Agent installed via the tarball method from your system, simply run the command below. This will shutdown all RKE2 Windows processes, remove the RKE2 Windows binary, and clean up the files used by RKE2. c :/ usr / local / bin / rke2-uninstall . ps1","title":"Tarball Method"},{"location":"install/install_options/install_options/","text":"Overview \u00b6 This page focuses on the configuration options available when setting up RKE2: Configuring the Linux installation script Configuring the Windows installation script Configuring RKE2 server nodes Configuring Linux RKE2 agent nodes Configuring Windows RKE2 agent nodes Using the configuration file Configuring when running the binary directly The primary way to configure RKE2 is through its config file . Command line arguments and environment variables are also available, but RKE2 is installed as a systemd service and thus these are not as easy to leverage. Configuring the Linux Installation Script \u00b6 As mentioned in the Quick-Start Guide , you can use the installation script available at https://get.rke2.io to install RKE2 as a service. The simplest form of this command is running, as root user or through sudo , as follows: # curl -sfL https://get.rke2.io | sudo sh - curl -sfL https://get.rke2.io | sh - When using this method to install RKE2, the following environment variables can be used to configure the installation: Environment Variable Description INSTALL_RKE2_VERSION Version of RKE2 to download from GitHub. Will attempt to download the latest release from the stable channel if not specified. INSTALL_RKE2_CHANNEL should also be set if installing on an RPM-based system and the desired version does not exist in the stable channel. INSTALL_RKE2_TYPE Type of systemd service to create, can be either \"server\" or \"agent\" Default is \"server\". INSTALL_RKE2_CHANNEL_URL Channel URL for fetching RKE2 download URL. Defaults to https://update.rke2.io/v1-release/channels . INSTALL_RKE2_CHANNEL Channel to use for fetching RKE2 download URL. Defaults to stable . Options include: stable , latest , testing . INSTALL_RKE2_METHOD Method of installation to use. Default is on RPM-based systems rpm , all else tar . INSTALL_RKE2_SKIP_RELOAD Skip reloading the systemctl daemon when doing a tar install. This installation script is straight-forward and will do the following: Obtain the desired version to install based on the above parameters. If no parameters are supplied, the latest official release will be used. Determine and execute the installation method. There are two methods: rpm and tar. If the INSTALL_RKE2_METHOD variable is set, that will be respected, Otherwise, rpm will be used on operating systems that use this package management system. On all other systems, tar will be used. In the case of the tar method, the script will simply unpack the tar archive associated with the desired release. In the case of rpm, a yum repository will be set up and the rpm will be installed using yum. Configuring the Windows Installation Script \u00b6 Windows Support is currently Experimental as of v1.21.3+rke2r1 Windows Support requires choosing Calico as the CNI for the RKE2 cluster As mentioned in the Quick-Start Guide , you can use the installation script available at https://github.com/rancher/rke2/blob/master/install.ps1 to install RKE2 on a Windows Agent Node. The simplest form of this command is as follows: Invoke-WebRequest -Uri https :// raw . githubusercontent . com / rancher / rke2 / master / install . ps1 -Outfile install . ps1 When using this method to install the Windows RKE2 agent, the following parameters can be passed to configure the installation script: SYNTAX install.ps1 [[-Channel] <String>] [[-Method] <String>] [[-Type] <String>] [[-Version] <String>] [[-TarPrefix] <String>] [-Commit] [[-AgentImagesDir] <String>] [[-ArtifactPath] <String>] [[-ChannelUrl] <String>] [<CommonParameters>] OPTIONS -Channel Channel to use for fetching RKE2 download URL (Default: \"stable\") -Method The installation method to use. Currently tar or choco installation supported. (Default: \"tar\") -Type Type of RKE2 service. Only the \"agent\" type is supported on Windows. (Default: \"agent\") -Version Version of rke2 to download from Github -TarPrefix Installation prefix when using the tar installation method. (Default: `C:/usr/local` unless `C:/usr/local` is read-only or has a dedicated mount point, in which case `C:/opt/rke2` is used instead) -Commit (experimental/agent) Commit of RKE2 to download from temporary cloud storage. If set, this forces `--Method=tar`. Intended for development purposes only. -AgentImagesDir Installation path for airgap images when installing from CI commit. (Default: `C:/var/lib/rancher/rke2/agent/images`) -ArtifactPath If set, the install script will use the local path for sourcing the `rke2.windows-$SUFFIX` and `sha256sum-$ARCH.txt` files rather than the downloading the files from GitHub. Disabled by default. Other Windows Installation Script Usage Examples \u00b6 Install the Latest Version Instead of Stable \u00b6 Invoke-WebRequest -Uri https :// raw . githubusercontent . com / rancher / rke2 / master / install . ps1 -Outfile install . ps1 ./ install . ps1 -Channel Latest Install the Latest Version using Tar Installation Method \u00b6 Invoke-WebRequest -Uri https :// raw . githubusercontent . com / rancher / rke2 / master / install . ps1 -Outfile install . ps1 ./ install . ps1 -Channel Latest -Method Tar Configuring RKE2 Server Nodes \u00b6 For details on configuring the RKE2 server, refer to the server configuration reference. Configuring Linux RKE2 Agent Nodes \u00b6 For details on configuring the RKE2 agent, refer to the agent configuration reference. Configuring Windows RKE2 Agent Nodes \u00b6 For details on configuring the RKE2 Windows agent, refer to the Windows agent configuration reference. Configuration File \u00b6 By default, RKE2 will launch with the values present in the YAML file located at /etc/rancher/rke2/config.yaml . An example of a basic server config file is below: write-kubeconfig-mode : \"0644\" tls-san : - \"foo.local\" node-label : - \"foo=bar\" - \"something=amazing\" The configuration file parameters map directly to CLI arguments, with repeatable CLI arguments being represented as YAML lists. An identical configuration using solely CLI arguments is shown below to demonstrate this: rke2 server \\ --write-kubeconfig-mode \"0644\" \\ --tls-san \"foo.local\" \\ --node-label \"foo=bar\" \\ --node-label \"something=amazing\" It is also possible to use both a configuration file and CLI arguments. In these situations, values will be loaded from both sources, but CLI arguments will take precedence. For repeatable arguments such as --node-label , the CLI arguments will overwrite all values in the list. Finally, the location of the config file can be changed either through the cli argument --config FILE, -c FILE , or the environment variable $RKE2_CONFIG_FILE . Configuring when Running the Binary Directly \u00b6 As stated, the installation script is primarily concerned with configuring RKE2 to run as a service. If you choose to not use the script, you can run RKE2 simply by downloading the binary from our release page , placing it on your path, and executing it. The RKE2 binary supports the following commands: Command Description rke2 server Run the RKE2 management server, which will also launch the Kubernetes control plane components such as the API server, controller-manager, and scheduler. Only Supported on Linux. rke2 agent Run the RKE2 node agent. This will cause RKE2 to run as a worker node, launching the Kubernetes node services kubelet and kube-proxy . Supported on Linux and Windows. rke2 help Shows a list of commands or help for one command The rke2 server and rke2 agent commands have additional configuration options that can be viewed with rke2 server --help or rke2 agent --help .","title":"Overview"},{"location":"install/install_options/install_options/#overview","text":"This page focuses on the configuration options available when setting up RKE2: Configuring the Linux installation script Configuring the Windows installation script Configuring RKE2 server nodes Configuring Linux RKE2 agent nodes Configuring Windows RKE2 agent nodes Using the configuration file Configuring when running the binary directly The primary way to configure RKE2 is through its config file . Command line arguments and environment variables are also available, but RKE2 is installed as a systemd service and thus these are not as easy to leverage.","title":"Overview"},{"location":"install/install_options/install_options/#configuring-the-linux-installation-script","text":"As mentioned in the Quick-Start Guide , you can use the installation script available at https://get.rke2.io to install RKE2 as a service. The simplest form of this command is running, as root user or through sudo , as follows: # curl -sfL https://get.rke2.io | sudo sh - curl -sfL https://get.rke2.io | sh - When using this method to install RKE2, the following environment variables can be used to configure the installation: Environment Variable Description INSTALL_RKE2_VERSION Version of RKE2 to download from GitHub. Will attempt to download the latest release from the stable channel if not specified. INSTALL_RKE2_CHANNEL should also be set if installing on an RPM-based system and the desired version does not exist in the stable channel. INSTALL_RKE2_TYPE Type of systemd service to create, can be either \"server\" or \"agent\" Default is \"server\". INSTALL_RKE2_CHANNEL_URL Channel URL for fetching RKE2 download URL. Defaults to https://update.rke2.io/v1-release/channels . INSTALL_RKE2_CHANNEL Channel to use for fetching RKE2 download URL. Defaults to stable . Options include: stable , latest , testing . INSTALL_RKE2_METHOD Method of installation to use. Default is on RPM-based systems rpm , all else tar . INSTALL_RKE2_SKIP_RELOAD Skip reloading the systemctl daemon when doing a tar install. This installation script is straight-forward and will do the following: Obtain the desired version to install based on the above parameters. If no parameters are supplied, the latest official release will be used. Determine and execute the installation method. There are two methods: rpm and tar. If the INSTALL_RKE2_METHOD variable is set, that will be respected, Otherwise, rpm will be used on operating systems that use this package management system. On all other systems, tar will be used. In the case of the tar method, the script will simply unpack the tar archive associated with the desired release. In the case of rpm, a yum repository will be set up and the rpm will be installed using yum.","title":"Configuring the Linux Installation Script"},{"location":"install/install_options/install_options/#configuring-the-windows-installation-script","text":"Windows Support is currently Experimental as of v1.21.3+rke2r1 Windows Support requires choosing Calico as the CNI for the RKE2 cluster As mentioned in the Quick-Start Guide , you can use the installation script available at https://github.com/rancher/rke2/blob/master/install.ps1 to install RKE2 on a Windows Agent Node. The simplest form of this command is as follows: Invoke-WebRequest -Uri https :// raw . githubusercontent . com / rancher / rke2 / master / install . ps1 -Outfile install . ps1 When using this method to install the Windows RKE2 agent, the following parameters can be passed to configure the installation script: SYNTAX install.ps1 [[-Channel] <String>] [[-Method] <String>] [[-Type] <String>] [[-Version] <String>] [[-TarPrefix] <String>] [-Commit] [[-AgentImagesDir] <String>] [[-ArtifactPath] <String>] [[-ChannelUrl] <String>] [<CommonParameters>] OPTIONS -Channel Channel to use for fetching RKE2 download URL (Default: \"stable\") -Method The installation method to use. Currently tar or choco installation supported. (Default: \"tar\") -Type Type of RKE2 service. Only the \"agent\" type is supported on Windows. (Default: \"agent\") -Version Version of rke2 to download from Github -TarPrefix Installation prefix when using the tar installation method. (Default: `C:/usr/local` unless `C:/usr/local` is read-only or has a dedicated mount point, in which case `C:/opt/rke2` is used instead) -Commit (experimental/agent) Commit of RKE2 to download from temporary cloud storage. If set, this forces `--Method=tar`. Intended for development purposes only. -AgentImagesDir Installation path for airgap images when installing from CI commit. (Default: `C:/var/lib/rancher/rke2/agent/images`) -ArtifactPath If set, the install script will use the local path for sourcing the `rke2.windows-$SUFFIX` and `sha256sum-$ARCH.txt` files rather than the downloading the files from GitHub. Disabled by default.","title":"Configuring the Windows Installation Script"},{"location":"install/install_options/install_options/#other-windows-installation-script-usage-examples","text":"","title":"Other Windows Installation Script Usage Examples"},{"location":"install/install_options/install_options/#install-the-latest-version-instead-of-stable","text":"Invoke-WebRequest -Uri https :// raw . githubusercontent . com / rancher / rke2 / master / install . ps1 -Outfile install . ps1 ./ install . ps1 -Channel Latest","title":"Install the Latest Version Instead of Stable"},{"location":"install/install_options/install_options/#install-the-latest-version-using-tar-installation-method","text":"Invoke-WebRequest -Uri https :// raw . githubusercontent . com / rancher / rke2 / master / install . ps1 -Outfile install . ps1 ./ install . ps1 -Channel Latest -Method Tar","title":"Install the Latest Version using Tar Installation Method"},{"location":"install/install_options/install_options/#configuring-rke2-server-nodes","text":"For details on configuring the RKE2 server, refer to the server configuration reference.","title":"Configuring RKE2 Server Nodes"},{"location":"install/install_options/install_options/#configuring-linux-rke2-agent-nodes","text":"For details on configuring the RKE2 agent, refer to the agent configuration reference.","title":"Configuring Linux RKE2 Agent Nodes"},{"location":"install/install_options/install_options/#configuring-windows-rke2-agent-nodes","text":"For details on configuring the RKE2 Windows agent, refer to the Windows agent configuration reference.","title":"Configuring Windows RKE2 Agent Nodes"},{"location":"install/install_options/install_options/#configuration-file","text":"By default, RKE2 will launch with the values present in the YAML file located at /etc/rancher/rke2/config.yaml . An example of a basic server config file is below: write-kubeconfig-mode : \"0644\" tls-san : - \"foo.local\" node-label : - \"foo=bar\" - \"something=amazing\" The configuration file parameters map directly to CLI arguments, with repeatable CLI arguments being represented as YAML lists. An identical configuration using solely CLI arguments is shown below to demonstrate this: rke2 server \\ --write-kubeconfig-mode \"0644\" \\ --tls-san \"foo.local\" \\ --node-label \"foo=bar\" \\ --node-label \"something=amazing\" It is also possible to use both a configuration file and CLI arguments. In these situations, values will be loaded from both sources, but CLI arguments will take precedence. For repeatable arguments such as --node-label , the CLI arguments will overwrite all values in the list. Finally, the location of the config file can be changed either through the cli argument --config FILE, -c FILE , or the environment variable $RKE2_CONFIG_FILE .","title":"Configuration File"},{"location":"install/install_options/install_options/#configuring-when-running-the-binary-directly","text":"As stated, the installation script is primarily concerned with configuring RKE2 to run as a service. If you choose to not use the script, you can run RKE2 simply by downloading the binary from our release page , placing it on your path, and executing it. The RKE2 binary supports the following commands: Command Description rke2 server Run the RKE2 management server, which will also launch the Kubernetes control plane components such as the API server, controller-manager, and scheduler. Only Supported on Linux. rke2 agent Run the RKE2 node agent. This will cause RKE2 to run as a worker node, launching the Kubernetes node services kubelet and kube-proxy . Supported on Linux and Windows. rke2 help Shows a list of commands or help for one command The rke2 server and rke2 agent commands have additional configuration options that can be viewed with rke2 server --help or rke2 agent --help .","title":"Configuring when Running the Binary Directly"},{"location":"install/install_options/linux_agent_config/","text":"This is a reference to all parameters that can be used to configure the rke2 agent. Note that while this is a reference to the command line arguments, the best way to configure RKE2 is using the configuration file . RKE2 Agent CLI Help \u00b6 If an option appears in brackets below, for example [$RKE2_URL] , it means that the option can be passed in as an environment variable of that name. NAME: rke2 agent - Run node agent USAGE: rke2 agent command [command options] [arguments...] COMMANDS: OPTIONS: --config FILE, -c FILE (config) Load configuration from FILE (default: \"/etc/rancher/rke2/config.yaml\") [$RKE2_CONFIG_FILE] --debug (logging) Turn on debug logs [$RKE2_DEBUG] --token value, -t value (cluster) Token to use for authentication [$RKE2_TOKEN] --token-file value (cluster) Token file to use for authentication [$RKE2_TOKEN_FILE] --server value, -s value (cluster) Server to connect to [$RKE2_URL] --data-dir value, -d value (data) Folder to hold state (default: \"/var/lib/rancher/rke2\") --node-name value (agent/node) Node name [$RKE2_NODE_NAME] --node-label value (agent/node) Registering and starting kubelet with set of labels --node-taint value (agent/node) Registering kubelet with set of taints --image-credential-provider-bin-dir value (agent/node) The path to the directory where credential provider plugin binaries are located (default: \"/var/lib/rancher/credentialprovider/bin\") --image-credential-provider-config value (agent/node) The path to the credential provider plugin config file (default: \"/var/lib/rancher/credentialprovider/config.yaml\") --container-runtime-endpoint value (agent/runtime) Disable embedded containerd and use alternative CRI implementation --snapshotter value (agent/runtime) Override default containerd snapshotter (default: \"overlayfs\") --private-registry value (agent/runtime) Private registry configuration file (default: \"/etc/rancher/rke2/registries.yaml\") --node-ip value, -i value (agent/networking) IPv4/IPv6 addresses to advertise for node --node-external-ip value (agent/networking) IPv4/IPv6 external IP addresses to advertise for node --resolv-conf value (agent/networking) Kubelet resolv.conf file [$RKE2_RESOLV_CONF] --kubelet-arg value (agent/flags) Customized flag for kubelet process --kube-proxy-arg value (agent/flags) Customized flag for kube-proxy process --protect-kernel-defaults (agent/node) Kernel tuning behavior. If set, error if kernel tunables are different than kubelet defaults. --selinux (agent/node) Enable SELinux in containerd [$RKE2_SELINUX] --lb-server-port value (agent/node) Local port for supervisor client load-balancer. If the supervisor and apiserver are not colocated an additional port 1 less than this port will also be used for the apiserver client load-balancer. (default: 6444) [$RKE2_LB_SERVER_PORT] --kube-apiserver-image value (image) Override image to use for kube-apiserver [$RKE2_KUBE_APISERVER_IMAGE] --kube-controller-manager-image value (image) Override image to use for kube-controller-manager [$RKE2_KUBE_CONTROLLER_MANAGER_IMAGE] --kube-proxy-image value (image) Override image to use for kube-proxy [$RKE2_KUBE_PROXY_IMAGE] --kube-scheduler-image value (image) Override image to use for kube-scheduler [$RKE2_KUBE_SCHEDULER_IMAGE] --pause-image value (image) Override image to use for pause [$RKE2_PAUSE_IMAGE] --runtime-image value (image) Override image to use for runtime binaries (containerd, kubectl, crictl, etc) [$RKE2_RUNTIME_IMAGE] --etcd-image value (image) Override image to use for etcd [$RKE2_ETCD_IMAGE] --kubelet-path value (experimental/agent) Override kubelet binary path [$RKE2_KUBELET_PATH] --cloud-provider-name value (cloud provider) Cloud provider name [$RKE2_CLOUD_PROVIDER_NAME] --cloud-provider-config value (cloud provider) Cloud provider configuration file path [$RKE2_CLOUD_PROVIDER_CONFIG] --profile value (security) Validate system configuration against the selected benchmark (valid items: cis-1.5, cis-1.6 ) [$RKE2_CIS_PROFILE] --audit-policy-file value (security) Path to the file that defines the audit policy configuration [$RKE2_AUDIT_POLICY_FILE] --control-plane-resource-requests value (components) Control Plane resource requests [$RKE2_CONTROL_PLANE_RESOURCE_REQUESTS] --control-plane-resource-limits value (components) Control Plane resource limits [$RKE2_CONTROL_PLANE_RESOURCE_LIMITS] --kube-apiserver-extra-mount value (components) kube-apiserver extra volume mounts [$RKE2_KUBE_APISERVER_EXTRA_MOUNT] --kube-scheduler-extra-mount value (components) kube-scheduler extra volume mounts [$RKE2_KUBE_SCHEDULER_EXTRA_MOUNT] --kube-controller-manager-extra-mount value (components) kube-controller-manager extra volume mounts [$RKE2_KUBE_CONTROLLER_MANAGER_EXTRA_MOUNT] --kube-proxy-extra-mount value (components) kube-proxy extra volume mounts [$RKE2_KUBE_PROXY_EXTRA_MOUNT] --etcd-extra-mount value (components) etcd extra volume mounts [$RKE2_ETCD_EXTRA_MOUNT] --cloud-controller-manager-extra-mount value (components) cloud-controller-manager extra volume mounts [$RKE2_CLOUD_CONTROLLER_MANAGER_EXTRA_MOUNT] --kube-apiserver-extra-env value (components) kube-apiserver extra environment variables [$RKE2_KUBE_APISERVER_EXTRA_ENV] --kube-scheduler-extra-env value (components) kube-scheduler extra environment variables [$RKE2_KUBE_SCHEDULER_EXTRA_ENV] --kube-controller-manager-extra-env value (components) kube-controller-manager extra environment variables [$RKE2_KUBE_CONTROLLER_MANAGER_EXTRA_ENV] --kube-proxy-extra-env value (components) kube-proxy extra environment variables [$RKE2_KUBE_PROXY_EXTRA_ENV] --etcd-extra-env value (components) etcd extra environment variables [$RKE2_ETCD_EXTRA_ENV] --cloud-controller-manager-extra-env value (components) cloud-controller-manager extra environment variables [$RKE2_CLOUD_CONTROLLER_MANAGER_EXTRA_ENV] --help, -h show help","title":"Agent Configuration Reference"},{"location":"install/install_options/linux_agent_config/#rke2-agent-cli-help","text":"If an option appears in brackets below, for example [$RKE2_URL] , it means that the option can be passed in as an environment variable of that name. NAME: rke2 agent - Run node agent USAGE: rke2 agent command [command options] [arguments...] COMMANDS: OPTIONS: --config FILE, -c FILE (config) Load configuration from FILE (default: \"/etc/rancher/rke2/config.yaml\") [$RKE2_CONFIG_FILE] --debug (logging) Turn on debug logs [$RKE2_DEBUG] --token value, -t value (cluster) Token to use for authentication [$RKE2_TOKEN] --token-file value (cluster) Token file to use for authentication [$RKE2_TOKEN_FILE] --server value, -s value (cluster) Server to connect to [$RKE2_URL] --data-dir value, -d value (data) Folder to hold state (default: \"/var/lib/rancher/rke2\") --node-name value (agent/node) Node name [$RKE2_NODE_NAME] --node-label value (agent/node) Registering and starting kubelet with set of labels --node-taint value (agent/node) Registering kubelet with set of taints --image-credential-provider-bin-dir value (agent/node) The path to the directory where credential provider plugin binaries are located (default: \"/var/lib/rancher/credentialprovider/bin\") --image-credential-provider-config value (agent/node) The path to the credential provider plugin config file (default: \"/var/lib/rancher/credentialprovider/config.yaml\") --container-runtime-endpoint value (agent/runtime) Disable embedded containerd and use alternative CRI implementation --snapshotter value (agent/runtime) Override default containerd snapshotter (default: \"overlayfs\") --private-registry value (agent/runtime) Private registry configuration file (default: \"/etc/rancher/rke2/registries.yaml\") --node-ip value, -i value (agent/networking) IPv4/IPv6 addresses to advertise for node --node-external-ip value (agent/networking) IPv4/IPv6 external IP addresses to advertise for node --resolv-conf value (agent/networking) Kubelet resolv.conf file [$RKE2_RESOLV_CONF] --kubelet-arg value (agent/flags) Customized flag for kubelet process --kube-proxy-arg value (agent/flags) Customized flag for kube-proxy process --protect-kernel-defaults (agent/node) Kernel tuning behavior. If set, error if kernel tunables are different than kubelet defaults. --selinux (agent/node) Enable SELinux in containerd [$RKE2_SELINUX] --lb-server-port value (agent/node) Local port for supervisor client load-balancer. If the supervisor and apiserver are not colocated an additional port 1 less than this port will also be used for the apiserver client load-balancer. (default: 6444) [$RKE2_LB_SERVER_PORT] --kube-apiserver-image value (image) Override image to use for kube-apiserver [$RKE2_KUBE_APISERVER_IMAGE] --kube-controller-manager-image value (image) Override image to use for kube-controller-manager [$RKE2_KUBE_CONTROLLER_MANAGER_IMAGE] --kube-proxy-image value (image) Override image to use for kube-proxy [$RKE2_KUBE_PROXY_IMAGE] --kube-scheduler-image value (image) Override image to use for kube-scheduler [$RKE2_KUBE_SCHEDULER_IMAGE] --pause-image value (image) Override image to use for pause [$RKE2_PAUSE_IMAGE] --runtime-image value (image) Override image to use for runtime binaries (containerd, kubectl, crictl, etc) [$RKE2_RUNTIME_IMAGE] --etcd-image value (image) Override image to use for etcd [$RKE2_ETCD_IMAGE] --kubelet-path value (experimental/agent) Override kubelet binary path [$RKE2_KUBELET_PATH] --cloud-provider-name value (cloud provider) Cloud provider name [$RKE2_CLOUD_PROVIDER_NAME] --cloud-provider-config value (cloud provider) Cloud provider configuration file path [$RKE2_CLOUD_PROVIDER_CONFIG] --profile value (security) Validate system configuration against the selected benchmark (valid items: cis-1.5, cis-1.6 ) [$RKE2_CIS_PROFILE] --audit-policy-file value (security) Path to the file that defines the audit policy configuration [$RKE2_AUDIT_POLICY_FILE] --control-plane-resource-requests value (components) Control Plane resource requests [$RKE2_CONTROL_PLANE_RESOURCE_REQUESTS] --control-plane-resource-limits value (components) Control Plane resource limits [$RKE2_CONTROL_PLANE_RESOURCE_LIMITS] --kube-apiserver-extra-mount value (components) kube-apiserver extra volume mounts [$RKE2_KUBE_APISERVER_EXTRA_MOUNT] --kube-scheduler-extra-mount value (components) kube-scheduler extra volume mounts [$RKE2_KUBE_SCHEDULER_EXTRA_MOUNT] --kube-controller-manager-extra-mount value (components) kube-controller-manager extra volume mounts [$RKE2_KUBE_CONTROLLER_MANAGER_EXTRA_MOUNT] --kube-proxy-extra-mount value (components) kube-proxy extra volume mounts [$RKE2_KUBE_PROXY_EXTRA_MOUNT] --etcd-extra-mount value (components) etcd extra volume mounts [$RKE2_ETCD_EXTRA_MOUNT] --cloud-controller-manager-extra-mount value (components) cloud-controller-manager extra volume mounts [$RKE2_CLOUD_CONTROLLER_MANAGER_EXTRA_MOUNT] --kube-apiserver-extra-env value (components) kube-apiserver extra environment variables [$RKE2_KUBE_APISERVER_EXTRA_ENV] --kube-scheduler-extra-env value (components) kube-scheduler extra environment variables [$RKE2_KUBE_SCHEDULER_EXTRA_ENV] --kube-controller-manager-extra-env value (components) kube-controller-manager extra environment variables [$RKE2_KUBE_CONTROLLER_MANAGER_EXTRA_ENV] --kube-proxy-extra-env value (components) kube-proxy extra environment variables [$RKE2_KUBE_PROXY_EXTRA_ENV] --etcd-extra-env value (components) etcd extra environment variables [$RKE2_ETCD_EXTRA_ENV] --cloud-controller-manager-extra-env value (components) cloud-controller-manager extra environment variables [$RKE2_CLOUD_CONTROLLER_MANAGER_EXTRA_ENV] --help, -h show help","title":"RKE2 Agent CLI Help"},{"location":"install/install_options/server_config/","text":"This is a reference to all parameters that can be used to configure the rke2 server. Note that while this is a reference to the command line arguments, the best way to configure RKE2 is using the configuration file . RKE2 Server CLI Help \u00b6 If an option appears in brackets below, for example [$RKE2_TOKEN] , it means that the option can be passed in as an environment variable of that name. NAME: rke2 server - Run management server USAGE: rke2 server [OPTIONS] OPTIONS: --config FILE, -c FILE (config) Load configuration from FILE (default: \"/etc/rancher/rke2/config.yaml\") [$RKE2_CONFIG_FILE] --debug (logging) Turn on debug logs [$RKE2_DEBUG] --bind-address value (listener) rke2 bind address (default: 0.0.0.0) --advertise-address value (listener) IPv4 address that apiserver uses to advertise to members of the cluster (default: node-external-ip/node-ip) --tls-san value (listener) Add additional hostnames or IPv4/IPv6 addresses as Subject Alternative Names on the server TLS cert --data-dir value, -d value (data) Folder to hold state (default: \"/var/lib/rancher/rke2\") --cluster-cidr value (networking) IPv4/IPv6 network CIDRs to use for pod IPs (default: 10.42.0.0/16) --service-cidr value (networking) IPv4/IPv6 network CIDRs to use for service IPs (default: 10.43.0.0/16) --service-node-port-range value (networking) Port range to reserve for services with NodePort visibility (default: \"30000-32767\") --cluster-dns value (networking) IPv4 Cluster IP for coredns service. Should be in your service-cidr range (default: 10.43.0.10) --cluster-domain value (networking) Cluster Domain (default: \"cluster.local\") --token value, -t value (cluster) Shared secret used to join a server or agent to a cluster [$RKE2_TOKEN] --token-file value (cluster) File containing the cluster-secret/token [$RKE2_TOKEN_FILE] --write-kubeconfig value, -o value (client) Write kubeconfig for admin client to this file [$RKE2_KUBECONFIG_OUTPUT] --write-kubeconfig-mode value (client) Write kubeconfig with this mode [$RKE2_KUBECONFIG_MODE] --kube-apiserver-arg value (flags) Customized flag for kube-apiserver process --etcd-arg value (flags) Customized flag for etcd process --kube-controller-manager-arg value (flags) Customized flag for kube-controller-manager process --kube-scheduler-arg value (flags) Customized flag for kube-scheduler process --etcd-expose-metrics (db) Expose etcd metrics to client interface. (Default false) --etcd-disable-snapshots (db) Disable automatic etcd snapshots --etcd-snapshot-name value (db) Set the base name of etcd snapshots. Default: etcd-snapshot-<unix-timestamp> (default: \"etcd-snapshot\") --etcd-snapshot-schedule-cron value (db) Snapshot interval time in cron spec. eg. every 5 hours '* */5 * * *' (default: \"0 */12 * * *\") --etcd-snapshot-retention value (db) Number of snapshots to retain Default: 5 (default: 5) --etcd-snapshot-dir value (db) Directory to save db snapshots. (Default location: ${data-dir}/db/snapshots) --etcd-s3 (db) Enable backup to S3 --etcd-s3-endpoint value (db) S3 endpoint url (default: \"s3.amazonaws.com\") --etcd-s3-endpoint-ca value (db) S3 custom CA cert to connect to S3 endpoint --etcd-s3-skip-ssl-verify (db) Disables S3 SSL certificate validation --etcd-s3-access-key value (db) S3 access key [$AWS_ACCESS_KEY_ID] --etcd-s3-secret-key value (db) S3 secret key [$AWS_SECRET_ACCESS_KEY] --etcd-s3-bucket value (db) S3 bucket name --etcd-s3-region value (db) S3 region / bucket location (optional) (default: \"us-east-1\") --etcd-s3-folder value (db) S3 folder --disable value (components) Do not deploy packaged components and delete any deployed components (valid items: rke2-coredns, rke2-ingress-nginx, rke2-metrics-server) --disable-scheduler (components) Disable Kubernetes default scheduler --disable-cloud-controller (components) Disable rke2 default cloud controller manager --disable-kube-proxy (components) Disable running kube-proxy --node-name value (agent/node) Node name [$RKE2_NODE_NAME] --node-label value (agent/node) Registering and starting kubelet with set of labels --node-taint value (agent/node) Registering kubelet with set of taints --image-credential-provider-bin-dir value (agent/node) The path to the directory where credential provider plugin binaries are located (default: \"/var/lib/rancher/credentialprovider/bin\") --image-credential-provider-config value (agent/node) The path to the credential provider plugin config file (default: \"/var/lib/rancher/credentialprovider/config.yaml\") --container-runtime-endpoint value (agent/runtime) Disable embedded containerd and use alternative CRI implementation --snapshotter value (agent/runtime) Override default containerd snapshotter (default: \"overlayfs\") --private-registry value (agent/runtime) Private registry configuration file (default: \"/etc/rancher/rke2/registries.yaml\") --node-ip value, -i value (agent/networking) IPv4/IPv6 addresses to advertise for node --node-external-ip value (agent/networking) IPv4/IPv6 external IP addresses to advertise for node --resolv-conf value (agent/networking) Kubelet resolv.conf file [$RKE2_RESOLV_CONF] --kubelet-arg value (agent/flags) Customized flag for kubelet process --kube-proxy-arg value (agent/flags) Customized flag for kube-proxy process --protect-kernel-defaults (agent/node) Kernel tuning behavior. If set, error if kernel tunables are different than kubelet defaults. --agent-token value (experimental/cluster) Shared secret used to join agents to the cluster, but not servers [$RKE2_AGENT_TOKEN] --agent-token-file value (experimental/cluster) File containing the agent secret [$RKE2_AGENT_TOKEN_FILE] --server value, -s value (experimental/cluster) Server to connect to, used to join a cluster [$RKE2_URL] --cluster-reset (experimental/cluster) Forget all peers and become sole member of a new cluster [$RKE2_CLUSTER_RESET] --cluster-reset-restore-path value (db) Path to snapshot file to be restored --system-default-registry value (image) Private registry to be used for all system images [$RKE2_SYSTEM_DEFAULT_REGISTRY] --selinux (agent/node) Enable SELinux in containerd [$RKE2_SELINUX] --lb-server-port value (agent/node) Local port for supervisor client load-balancer. If the supervisor and apiserver are not colocated an additional port 1 less than this port will also be used for the apiserver client load-balancer. (default: 6444) [$RKE2_LB_SERVER_PORT] --cni value (networking) CNI Plugins to deploy, one of none, calico, canal, cilium; optionally with multus as the first value to enable the multus meta-plugin (default: canal) [$RKE2_CNI] --kube-apiserver-image value (image) Override image to use for kube-apiserver [$RKE2_KUBE_APISERVER_IMAGE] --kube-controller-manager-image value (image) Override image to use for kube-controller-manager [$RKE2_KUBE_CONTROLLER_MANAGER_IMAGE] --kube-proxy-image value (image) Override image to use for kube-proxy [$RKE2_KUBE_PROXY_IMAGE] --kube-scheduler-image value (image) Override image to use for kube-scheduler [$RKE2_KUBE_SCHEDULER_IMAGE] --pause-image value (image) Override image to use for pause [$RKE2_PAUSE_IMAGE] --runtime-image value (image) Override image to use for runtime binaries (containerd, kubectl, crictl, etc) [$RKE2_RUNTIME_IMAGE] --etcd-image value (image) Override image to use for etcd [$RKE2_ETCD_IMAGE] --kubelet-path value (experimental/agent) Override kubelet binary path [$RKE2_KUBELET_PATH] --cloud-provider-name value (cloud provider) Cloud provider name [$RKE2_CLOUD_PROVIDER_NAME] --cloud-provider-config value (cloud provider) Cloud provider configuration file path [$RKE2_CLOUD_PROVIDER_CONFIG] --profile value (security) Validate system configuration against the selected benchmark (valid items: cis-1.5, cis-1.6 ) [$RKE2_CIS_PROFILE] --audit-policy-file value (security) Path to the file that defines the audit policy configuration [$RKE2_AUDIT_POLICY_FILE] --control-plane-resource-requests value (components) Control Plane resource requests [$RKE2_CONTROL_PLANE_RESOURCE_REQUESTS] --control-plane-resource-limits value (components) Control Plane resource limits [$RKE2_CONTROL_PLANE_RESOURCE_LIMITS] --kube-apiserver-extra-mount value (components) kube-apiserver extra volume mounts [$RKE2_KUBE_APISERVER_EXTRA_MOUNT] --kube-scheduler-extra-mount value (components) kube-scheduler extra volume mounts [$RKE2_KUBE_SCHEDULER_EXTRA_MOUNT] --kube-controller-manager-extra-mount value (components) kube-controller-manager extra volume mounts [$RKE2_KUBE_CONTROLLER_MANAGER_EXTRA_MOUNT] --kube-proxy-extra-mount value (components) kube-proxy extra volume mounts [$RKE2_KUBE_PROXY_EXTRA_MOUNT] --etcd-extra-mount value (components) etcd extra volume mounts [$RKE2_ETCD_EXTRA_MOUNT] --cloud-controller-manager-extra-mount value (components) cloud-controller-manager extra volume mounts [$RKE2_CLOUD_CONTROLLER_MANAGER_EXTRA_MOUNT] --kube-apiserver-extra-env value (components) kube-apiserver extra environment variables [$RKE2_KUBE_APISERVER_EXTRA_ENV] --kube-scheduler-extra-env value (components) kube-scheduler extra environment variables [$RKE2_KUBE_SCHEDULER_EXTRA_ENV] --kube-controller-manager-extra-env value (components) kube-controller-manager extra environment variables [$RKE2_KUBE_CONTROLLER_MANAGER_EXTRA_ENV] --kube-proxy-extra-env value (components) kube-proxy extra environment variables [$RKE2_KUBE_PROXY_EXTRA_ENV] --etcd-extra-env value (components) etcd extra environment variables [$RKE2_ETCD_EXTRA_ENV] --cloud-controller-manager-extra-env value (components) cloud-controller-manager extra environment variables [$RKE2_CLOUD_CONTROLLER_MANAGER_EXTRA_ENV]```","title":"Server Configuration Reference"},{"location":"install/install_options/server_config/#rke2-server-cli-help","text":"If an option appears in brackets below, for example [$RKE2_TOKEN] , it means that the option can be passed in as an environment variable of that name. NAME: rke2 server - Run management server USAGE: rke2 server [OPTIONS] OPTIONS: --config FILE, -c FILE (config) Load configuration from FILE (default: \"/etc/rancher/rke2/config.yaml\") [$RKE2_CONFIG_FILE] --debug (logging) Turn on debug logs [$RKE2_DEBUG] --bind-address value (listener) rke2 bind address (default: 0.0.0.0) --advertise-address value (listener) IPv4 address that apiserver uses to advertise to members of the cluster (default: node-external-ip/node-ip) --tls-san value (listener) Add additional hostnames or IPv4/IPv6 addresses as Subject Alternative Names on the server TLS cert --data-dir value, -d value (data) Folder to hold state (default: \"/var/lib/rancher/rke2\") --cluster-cidr value (networking) IPv4/IPv6 network CIDRs to use for pod IPs (default: 10.42.0.0/16) --service-cidr value (networking) IPv4/IPv6 network CIDRs to use for service IPs (default: 10.43.0.0/16) --service-node-port-range value (networking) Port range to reserve for services with NodePort visibility (default: \"30000-32767\") --cluster-dns value (networking) IPv4 Cluster IP for coredns service. Should be in your service-cidr range (default: 10.43.0.10) --cluster-domain value (networking) Cluster Domain (default: \"cluster.local\") --token value, -t value (cluster) Shared secret used to join a server or agent to a cluster [$RKE2_TOKEN] --token-file value (cluster) File containing the cluster-secret/token [$RKE2_TOKEN_FILE] --write-kubeconfig value, -o value (client) Write kubeconfig for admin client to this file [$RKE2_KUBECONFIG_OUTPUT] --write-kubeconfig-mode value (client) Write kubeconfig with this mode [$RKE2_KUBECONFIG_MODE] --kube-apiserver-arg value (flags) Customized flag for kube-apiserver process --etcd-arg value (flags) Customized flag for etcd process --kube-controller-manager-arg value (flags) Customized flag for kube-controller-manager process --kube-scheduler-arg value (flags) Customized flag for kube-scheduler process --etcd-expose-metrics (db) Expose etcd metrics to client interface. (Default false) --etcd-disable-snapshots (db) Disable automatic etcd snapshots --etcd-snapshot-name value (db) Set the base name of etcd snapshots. Default: etcd-snapshot-<unix-timestamp> (default: \"etcd-snapshot\") --etcd-snapshot-schedule-cron value (db) Snapshot interval time in cron spec. eg. every 5 hours '* */5 * * *' (default: \"0 */12 * * *\") --etcd-snapshot-retention value (db) Number of snapshots to retain Default: 5 (default: 5) --etcd-snapshot-dir value (db) Directory to save db snapshots. (Default location: ${data-dir}/db/snapshots) --etcd-s3 (db) Enable backup to S3 --etcd-s3-endpoint value (db) S3 endpoint url (default: \"s3.amazonaws.com\") --etcd-s3-endpoint-ca value (db) S3 custom CA cert to connect to S3 endpoint --etcd-s3-skip-ssl-verify (db) Disables S3 SSL certificate validation --etcd-s3-access-key value (db) S3 access key [$AWS_ACCESS_KEY_ID] --etcd-s3-secret-key value (db) S3 secret key [$AWS_SECRET_ACCESS_KEY] --etcd-s3-bucket value (db) S3 bucket name --etcd-s3-region value (db) S3 region / bucket location (optional) (default: \"us-east-1\") --etcd-s3-folder value (db) S3 folder --disable value (components) Do not deploy packaged components and delete any deployed components (valid items: rke2-coredns, rke2-ingress-nginx, rke2-metrics-server) --disable-scheduler (components) Disable Kubernetes default scheduler --disable-cloud-controller (components) Disable rke2 default cloud controller manager --disable-kube-proxy (components) Disable running kube-proxy --node-name value (agent/node) Node name [$RKE2_NODE_NAME] --node-label value (agent/node) Registering and starting kubelet with set of labels --node-taint value (agent/node) Registering kubelet with set of taints --image-credential-provider-bin-dir value (agent/node) The path to the directory where credential provider plugin binaries are located (default: \"/var/lib/rancher/credentialprovider/bin\") --image-credential-provider-config value (agent/node) The path to the credential provider plugin config file (default: \"/var/lib/rancher/credentialprovider/config.yaml\") --container-runtime-endpoint value (agent/runtime) Disable embedded containerd and use alternative CRI implementation --snapshotter value (agent/runtime) Override default containerd snapshotter (default: \"overlayfs\") --private-registry value (agent/runtime) Private registry configuration file (default: \"/etc/rancher/rke2/registries.yaml\") --node-ip value, -i value (agent/networking) IPv4/IPv6 addresses to advertise for node --node-external-ip value (agent/networking) IPv4/IPv6 external IP addresses to advertise for node --resolv-conf value (agent/networking) Kubelet resolv.conf file [$RKE2_RESOLV_CONF] --kubelet-arg value (agent/flags) Customized flag for kubelet process --kube-proxy-arg value (agent/flags) Customized flag for kube-proxy process --protect-kernel-defaults (agent/node) Kernel tuning behavior. If set, error if kernel tunables are different than kubelet defaults. --agent-token value (experimental/cluster) Shared secret used to join agents to the cluster, but not servers [$RKE2_AGENT_TOKEN] --agent-token-file value (experimental/cluster) File containing the agent secret [$RKE2_AGENT_TOKEN_FILE] --server value, -s value (experimental/cluster) Server to connect to, used to join a cluster [$RKE2_URL] --cluster-reset (experimental/cluster) Forget all peers and become sole member of a new cluster [$RKE2_CLUSTER_RESET] --cluster-reset-restore-path value (db) Path to snapshot file to be restored --system-default-registry value (image) Private registry to be used for all system images [$RKE2_SYSTEM_DEFAULT_REGISTRY] --selinux (agent/node) Enable SELinux in containerd [$RKE2_SELINUX] --lb-server-port value (agent/node) Local port for supervisor client load-balancer. If the supervisor and apiserver are not colocated an additional port 1 less than this port will also be used for the apiserver client load-balancer. (default: 6444) [$RKE2_LB_SERVER_PORT] --cni value (networking) CNI Plugins to deploy, one of none, calico, canal, cilium; optionally with multus as the first value to enable the multus meta-plugin (default: canal) [$RKE2_CNI] --kube-apiserver-image value (image) Override image to use for kube-apiserver [$RKE2_KUBE_APISERVER_IMAGE] --kube-controller-manager-image value (image) Override image to use for kube-controller-manager [$RKE2_KUBE_CONTROLLER_MANAGER_IMAGE] --kube-proxy-image value (image) Override image to use for kube-proxy [$RKE2_KUBE_PROXY_IMAGE] --kube-scheduler-image value (image) Override image to use for kube-scheduler [$RKE2_KUBE_SCHEDULER_IMAGE] --pause-image value (image) Override image to use for pause [$RKE2_PAUSE_IMAGE] --runtime-image value (image) Override image to use for runtime binaries (containerd, kubectl, crictl, etc) [$RKE2_RUNTIME_IMAGE] --etcd-image value (image) Override image to use for etcd [$RKE2_ETCD_IMAGE] --kubelet-path value (experimental/agent) Override kubelet binary path [$RKE2_KUBELET_PATH] --cloud-provider-name value (cloud provider) Cloud provider name [$RKE2_CLOUD_PROVIDER_NAME] --cloud-provider-config value (cloud provider) Cloud provider configuration file path [$RKE2_CLOUD_PROVIDER_CONFIG] --profile value (security) Validate system configuration against the selected benchmark (valid items: cis-1.5, cis-1.6 ) [$RKE2_CIS_PROFILE] --audit-policy-file value (security) Path to the file that defines the audit policy configuration [$RKE2_AUDIT_POLICY_FILE] --control-plane-resource-requests value (components) Control Plane resource requests [$RKE2_CONTROL_PLANE_RESOURCE_REQUESTS] --control-plane-resource-limits value (components) Control Plane resource limits [$RKE2_CONTROL_PLANE_RESOURCE_LIMITS] --kube-apiserver-extra-mount value (components) kube-apiserver extra volume mounts [$RKE2_KUBE_APISERVER_EXTRA_MOUNT] --kube-scheduler-extra-mount value (components) kube-scheduler extra volume mounts [$RKE2_KUBE_SCHEDULER_EXTRA_MOUNT] --kube-controller-manager-extra-mount value (components) kube-controller-manager extra volume mounts [$RKE2_KUBE_CONTROLLER_MANAGER_EXTRA_MOUNT] --kube-proxy-extra-mount value (components) kube-proxy extra volume mounts [$RKE2_KUBE_PROXY_EXTRA_MOUNT] --etcd-extra-mount value (components) etcd extra volume mounts [$RKE2_ETCD_EXTRA_MOUNT] --cloud-controller-manager-extra-mount value (components) cloud-controller-manager extra volume mounts [$RKE2_CLOUD_CONTROLLER_MANAGER_EXTRA_MOUNT] --kube-apiserver-extra-env value (components) kube-apiserver extra environment variables [$RKE2_KUBE_APISERVER_EXTRA_ENV] --kube-scheduler-extra-env value (components) kube-scheduler extra environment variables [$RKE2_KUBE_SCHEDULER_EXTRA_ENV] --kube-controller-manager-extra-env value (components) kube-controller-manager extra environment variables [$RKE2_KUBE_CONTROLLER_MANAGER_EXTRA_ENV] --kube-proxy-extra-env value (components) kube-proxy extra environment variables [$RKE2_KUBE_PROXY_EXTRA_ENV] --etcd-extra-env value (components) etcd extra environment variables [$RKE2_ETCD_EXTRA_ENV] --cloud-controller-manager-extra-env value (components) cloud-controller-manager extra environment variables [$RKE2_CLOUD_CONTROLLER_MANAGER_EXTRA_ENV]```","title":"RKE2 Server CLI Help"},{"location":"install/install_options/windows_agent_config/","text":"This is a reference to all parameters that can be used to configure the Windows RKE2 agent. Windows Support is currently Experimental as of v1.21.3+rke2r1 and requires choosing Calico as the CNI for the RKE2 cluster Windows RKE2 Agent CLI Help \u00b6 NAME: rke2-windows-amd64.exe agent - Run node agent USAGE: rke2-windows-amd64.exe agent command [command options] [arguments...] COMMANDS: service Manage RKE2 as a Windows Service OPTIONS: --config FILE, -c FILE (config) Load configuration from FILE (default: \"/etc/rancher/rke2/config.yaml\") [%RKE2_CONFIG_FILE%] --debug (logging) Turn on debug logs [%RKE2_DEBUG%] --token value, -t value (cluster) Token to use for authentication [%RKE2_TOKEN%] --token-file value (cluster) Token file to use for authentication [%RKE2_TOKEN_FILE%] --server value, -s value (cluster) Server to connect to [%RKE2_URL%] --data-dir value, -d value (data) Folder to hold state (default: \"/var/lib/rancher/rke2\") --node-name value (agent/node) Node name [%RKE2_NODE_NAME%] --node-label value (agent/node) Registering and starting kubelet with set of labels --node-taint value (agent/node) Registering kubelet with set of taints --image-credential-provider-bin-dir value (agent/node) The path to the directory where credential provider plugin binaries are located (default: \"/var/lib/rancher/credentialprovider/bin\") --image-credential-provider-config value (agent/node) The path to the credential provider plugin config file (default: \"/var/lib/rancher/credentialprovider/config.yaml\") --container-runtime-endpoint value (agent/runtime) Disable embedded containerd and use alternative CRI implementation --snapshotter value (agent/runtime) Override default containerd snapshotter (default: \"native\") --private-registry value (agent/runtime) Private registry configuration file (default: \"/etc/rancher/rke2/registries.yaml\") --node-ip value, -i value (agent/networking) IPv4/IPv6 addresses to advertise for node --node-external-ip value (agent/networking) IPv4/IPv6 external IP addresses to advertise for node --resolv-conf value (agent/networking) Kubelet resolv.conf file [%RKE2_RESOLV_CONF%] --kubelet-arg value (agent/flags) Customized flag for kubelet process --kube-proxy-arg value (agent/flags) Customized flag for kube-proxy process --protect-kernel-defaults (agent/node) Kernel tuning behavior. If set, error if kernel tunables are different than kubelet defaults. --selinux (agent/node) Enable SELinux in containerd [%RKE2_SELINUX%] --lb-server-port value (agent/node) Local port for supervisor client load-balancer. If the supervisor and apiserver are not colocated an additional port 1 less than this port w ill also be used for the apiserver client load-balancer. (default: 6444) [%RKE2_LB_SERVER_PORT%] --kube-apiserver-image value (image) Override image to use for kube-apiserver [%RKE2_KUBE_APISERVER_IMAGE%] --kube-controller-manager-image value (image) Override image to use for kube-controller-manager [%RKE2_KUBE_CONTROLLER_MANAGER_IMAGE%] --kube-proxy-image value (image) Override image to use for kube-proxy [%RKE2_KUBE_PROXY_IMAGE%] --kube-scheduler-image value (image) Override image to use for kube-scheduler [%RKE2_KUBE_SCHEDULER_IMAGE%] --pause-image value (image) Override image to use for pause [%RKE2_PAUSE_IMAGE%] --runtime-image value (image) Override image to use for runtime binaries (containerd, kubectl, crictl, etc) [%RKE2_RUNTIME_IMAGE%] --etcd-image value (image) Override image to use for etcd [%RKE2_ETCD_IMAGE%] --kubelet-path value (experimental/agent) Override kubelet binary path [%RKE2_KUBELET_PATH%] --cloud-provider-name value (cloud provider) Cloud provider name [%RKE2_CLOUD_PROVIDER_NAME%] --cloud-provider-config value (cloud provider) Cloud provider configuration file path [%RKE2_CLOUD_PROVIDER_CONFIG%] --profile value (security) Validate system configuration against the selected benchmark (valid items: cis-1.5, cis-1.6 ) [%RKE2_CIS_PROFILE%] --audit-policy-file value (security) Path to the file that defines the audit policy configuration [%RKE2_AUDIT_POLICY_FILE%] --control-plane-resource-requests value (components) Control Plane resource requests [%RKE2_CONTROL_PLANE_RESOURCE_REQUESTS%] --control-plane-resource-limits value (components) Control Plane resource limits [%RKE2_CONTROL_PLANE_RESOURCE_LIMITS%] --kube-apiserver-extra-mount value (components) kube-apiserver extra volume mounts [%RKE2_KUBE_APISERVER_EXTRA_MOUNT%] --kube-scheduler-extra-mount value (components) kube-scheduler extra volume mounts [%RKE2_KUBE_SCHEDULER_EXTRA_MOUNT%] --kube-controller-manager-extra-mount value (components) kube-controller-manager extra volume mounts [%RKE2_KUBE_CONTROLLER_MANAGER_EXTRA_MOUNT%] --kube-proxy-extra-mount value (components) kube-proxy extra volume mounts [%RKE2_KUBE_PROXY_EXTRA_MOUNT%] --etcd-extra-mount value (components) etcd extra volume mounts [%RKE2_ETCD_EXTRA_MOUNT%] --cloud-controller-manager-extra-mount value (components) cloud-controller-manager extra volume mounts [%RKE2_CLOUD_CONTROLLER_MANAGER_EXTRA_MOUNT%] --kube-apiserver-extra-env value (components) kube-apiserver extra environment variables [%RKE2_KUBE_APISERVER_EXTRA_ENV%] --kube-scheduler-extra-env value (components) kube-scheduler extra environment variables [%RKE2_KUBE_SCHEDULER_EXTRA_ENV%] --kube-controller-manager-extra-env value (components) kube-controller-manager extra environment variables [%RKE2_KUBE_CONTROLLER_MANAGER_EXTRA_ENV%] --kube-proxy-extra-env value (components) kube-proxy extra environment variables [%RKE2_KUBE_PROXY_EXTRA_ENV%] --etcd-extra-env value (components) etcd extra environment variables [%RKE2_ETCD_EXTRA_ENV%] --cloud-controller-manager-extra-env value (components) cloud-controller-manager extra environment variables [%RKE2_CLOUD_CONTROLLER_MANAGER_EXTRA_ENV%] --help, -h show help This Windows Agent Configuration Reference was last updated using the v1.22.5+rke2r2 release \u00b6 rke2-windows-amd64.exe version v1.22.5+rke2r2 (b61d4b3cb989b0380aae97fceb9a3e45a35ee2b9) go version go1.16.10b7","title":"Windows Agent Configuration Reference"},{"location":"install/install_options/windows_agent_config/#windows-rke2-agent-cli-help","text":"NAME: rke2-windows-amd64.exe agent - Run node agent USAGE: rke2-windows-amd64.exe agent command [command options] [arguments...] COMMANDS: service Manage RKE2 as a Windows Service OPTIONS: --config FILE, -c FILE (config) Load configuration from FILE (default: \"/etc/rancher/rke2/config.yaml\") [%RKE2_CONFIG_FILE%] --debug (logging) Turn on debug logs [%RKE2_DEBUG%] --token value, -t value (cluster) Token to use for authentication [%RKE2_TOKEN%] --token-file value (cluster) Token file to use for authentication [%RKE2_TOKEN_FILE%] --server value, -s value (cluster) Server to connect to [%RKE2_URL%] --data-dir value, -d value (data) Folder to hold state (default: \"/var/lib/rancher/rke2\") --node-name value (agent/node) Node name [%RKE2_NODE_NAME%] --node-label value (agent/node) Registering and starting kubelet with set of labels --node-taint value (agent/node) Registering kubelet with set of taints --image-credential-provider-bin-dir value (agent/node) The path to the directory where credential provider plugin binaries are located (default: \"/var/lib/rancher/credentialprovider/bin\") --image-credential-provider-config value (agent/node) The path to the credential provider plugin config file (default: \"/var/lib/rancher/credentialprovider/config.yaml\") --container-runtime-endpoint value (agent/runtime) Disable embedded containerd and use alternative CRI implementation --snapshotter value (agent/runtime) Override default containerd snapshotter (default: \"native\") --private-registry value (agent/runtime) Private registry configuration file (default: \"/etc/rancher/rke2/registries.yaml\") --node-ip value, -i value (agent/networking) IPv4/IPv6 addresses to advertise for node --node-external-ip value (agent/networking) IPv4/IPv6 external IP addresses to advertise for node --resolv-conf value (agent/networking) Kubelet resolv.conf file [%RKE2_RESOLV_CONF%] --kubelet-arg value (agent/flags) Customized flag for kubelet process --kube-proxy-arg value (agent/flags) Customized flag for kube-proxy process --protect-kernel-defaults (agent/node) Kernel tuning behavior. If set, error if kernel tunables are different than kubelet defaults. --selinux (agent/node) Enable SELinux in containerd [%RKE2_SELINUX%] --lb-server-port value (agent/node) Local port for supervisor client load-balancer. If the supervisor and apiserver are not colocated an additional port 1 less than this port w ill also be used for the apiserver client load-balancer. (default: 6444) [%RKE2_LB_SERVER_PORT%] --kube-apiserver-image value (image) Override image to use for kube-apiserver [%RKE2_KUBE_APISERVER_IMAGE%] --kube-controller-manager-image value (image) Override image to use for kube-controller-manager [%RKE2_KUBE_CONTROLLER_MANAGER_IMAGE%] --kube-proxy-image value (image) Override image to use for kube-proxy [%RKE2_KUBE_PROXY_IMAGE%] --kube-scheduler-image value (image) Override image to use for kube-scheduler [%RKE2_KUBE_SCHEDULER_IMAGE%] --pause-image value (image) Override image to use for pause [%RKE2_PAUSE_IMAGE%] --runtime-image value (image) Override image to use for runtime binaries (containerd, kubectl, crictl, etc) [%RKE2_RUNTIME_IMAGE%] --etcd-image value (image) Override image to use for etcd [%RKE2_ETCD_IMAGE%] --kubelet-path value (experimental/agent) Override kubelet binary path [%RKE2_KUBELET_PATH%] --cloud-provider-name value (cloud provider) Cloud provider name [%RKE2_CLOUD_PROVIDER_NAME%] --cloud-provider-config value (cloud provider) Cloud provider configuration file path [%RKE2_CLOUD_PROVIDER_CONFIG%] --profile value (security) Validate system configuration against the selected benchmark (valid items: cis-1.5, cis-1.6 ) [%RKE2_CIS_PROFILE%] --audit-policy-file value (security) Path to the file that defines the audit policy configuration [%RKE2_AUDIT_POLICY_FILE%] --control-plane-resource-requests value (components) Control Plane resource requests [%RKE2_CONTROL_PLANE_RESOURCE_REQUESTS%] --control-plane-resource-limits value (components) Control Plane resource limits [%RKE2_CONTROL_PLANE_RESOURCE_LIMITS%] --kube-apiserver-extra-mount value (components) kube-apiserver extra volume mounts [%RKE2_KUBE_APISERVER_EXTRA_MOUNT%] --kube-scheduler-extra-mount value (components) kube-scheduler extra volume mounts [%RKE2_KUBE_SCHEDULER_EXTRA_MOUNT%] --kube-controller-manager-extra-mount value (components) kube-controller-manager extra volume mounts [%RKE2_KUBE_CONTROLLER_MANAGER_EXTRA_MOUNT%] --kube-proxy-extra-mount value (components) kube-proxy extra volume mounts [%RKE2_KUBE_PROXY_EXTRA_MOUNT%] --etcd-extra-mount value (components) etcd extra volume mounts [%RKE2_ETCD_EXTRA_MOUNT%] --cloud-controller-manager-extra-mount value (components) cloud-controller-manager extra volume mounts [%RKE2_CLOUD_CONTROLLER_MANAGER_EXTRA_MOUNT%] --kube-apiserver-extra-env value (components) kube-apiserver extra environment variables [%RKE2_KUBE_APISERVER_EXTRA_ENV%] --kube-scheduler-extra-env value (components) kube-scheduler extra environment variables [%RKE2_KUBE_SCHEDULER_EXTRA_ENV%] --kube-controller-manager-extra-env value (components) kube-controller-manager extra environment variables [%RKE2_KUBE_CONTROLLER_MANAGER_EXTRA_ENV%] --kube-proxy-extra-env value (components) kube-proxy extra environment variables [%RKE2_KUBE_PROXY_EXTRA_ENV%] --etcd-extra-env value (components) etcd extra environment variables [%RKE2_ETCD_EXTRA_ENV%] --cloud-controller-manager-extra-env value (components) cloud-controller-manager extra environment variables [%RKE2_CLOUD_CONTROLLER_MANAGER_EXTRA_ENV%] --help, -h show help","title":"Windows RKE2 Agent CLI Help"},{"location":"install/install_options/windows_agent_config/#this-windows-agent-configuration-reference-was-last-updated-using-the-v1225rke2r2-release","text":"rke2-windows-amd64.exe version v1.22.5+rke2r2 (b61d4b3cb989b0380aae97fceb9a3e45a35ee2b9) go version go1.16.10b7","title":"This Windows Agent Configuration Reference was last updated using the v1.22.5+rke2r2 release"},{"location":"security/about_hardened_images/","text":"About Hardened Images \u00b6 RKE2 hardened images are scanned for vulnerabilities at build time, and additional security protections have been added to decrease potential weaknesses: Images are not simply mirrored from upstream builds. The images are built from source on top of a hardened minimal base image, which is currently Red Hat Universal Base Image (UBI). Any binaries that are written in Go are compiled using a FIPS 140-2 compliant build process. For more information on this compiler, refer here . You will know if an image has been hardened as above by the image name. RKE2 publishes image lists with each release. Refer here for an example of a published image list. Note Currently, RKE2 hardened images are multi-architecture. Only the Linux AMD64 architecture is FIPS compliant. Windows and s390x are not FIPS compliant.","title":"About Hardened Images"},{"location":"security/about_hardened_images/#about-hardened-images","text":"RKE2 hardened images are scanned for vulnerabilities at build time, and additional security protections have been added to decrease potential weaknesses: Images are not simply mirrored from upstream builds. The images are built from source on top of a hardened minimal base image, which is currently Red Hat Universal Base Image (UBI). Any binaries that are written in Go are compiled using a FIPS 140-2 compliant build process. For more information on this compiler, refer here . You will know if an image has been hardened as above by the image name. RKE2 publishes image lists with each release. Refer here for an example of a published image list. Note Currently, RKE2 hardened images are multi-architecture. Only the Linux AMD64 architecture is FIPS compliant. Windows and s390x are not FIPS compliant.","title":"About Hardened Images"},{"location":"security/cis_self_assessment16/","text":"CIS Kubernetes Benchmark v1.6 - RKE2 \u00b6 Overview \u00b6 This document is a companion to the RKE2 security hardening guide. The hardening guide provides prescriptive guidance for hardening a production installation of RKE2, and this benchmark guide is meant to help you evaluate the level of security of the hardened cluster against each control in the CIS Kubernetes benchmark. It is to be used by RKE2 operators, security teams, auditors, and decision makers. This guide is specific to the v1.21 and v1.22 release line of RKE2 and the v1.6.1 release of the CIS Kubernetes Benchmark. For more details about each control, including detailed descriptions and remediations for failing tests, you can refer to the corresponding section of the CIS Kubernetes Benchmark v1.6.1. You can download the benchmark after logging in to CISecurity.org . Testing controls methodology \u00b6 Each control in the CIS Kubernetes Benchmark was evaluated against an RKE2 cluster that was configured according to the accompanying hardening guide. Where control audits differ from the original CIS benchmark, the audit commands specific to RKE2 are provided for testing. These are the possible results for each control: Pass - The RKE2 cluster under test passed the audit outlined in the benchmark. Not Applicable - The control is not applicable to RKE2 because of how it is designed to operate. The remediation section will explain why this is so. Manual - Operator Dependent - The control is Manual in the CIS benchmark and it depends on the cluster's use case or some other factor that must be determined by the cluster operator. These controls have been evaluated to ensure RKE2 does not prevent their implementation, but no further configuration or auditing of the cluster under test has been performed. Controls \u00b6 1 Master Node Security Configuration \u00b6 1.1 Master Node Configuration Files \u00b6 1.1.1 \u00b6 Ensure that the API server pod specification file permissions are set to 644 or more restrictive (Automated) Rationale The API server pod specification file controls various parameters that set the behavior of the API server. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/agent/pod-manifests/kube-apiserver.yaml 644 Remediation: By default, RKE2 creates these files with 644 permissions. No manual remediation needed. 1.1.2 \u00b6 Ensure that the API server pod specification file ownership is set to root:root (Automated) Rationale The API server pod specification file controls various parameters that set the behavior of the API server. You should set its file ownership to maintain the integrity of the file. The file should be owned by `root:root`. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/agent/pod-manifests/kube-apiserver.yaml root:root Remediation: By default, RKE2 creates these files with root:root ownership. No manual remediation needed. 1.1.3 \u00b6 Ensure that the controller manager pod specification file permissions are set to 644 or more restrictive (Automated) Rationale The controller manager pod specification file controls various parameters that set the behavior of the Controller Manager on the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/agent/pod-manifests/kube-controller-manager.yaml 644 Remediation: By default, RKE2 creates these files with 644 permissions. No manual remediation needed. 1.1.4 \u00b6 Ensure that the controller manager pod specification file ownership is set to root:root (Automated) Rationale The controller manager pod specification file controls various parameters that set the behavior of various components of the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/agent/pod-manifests/kube-controller-manager.yaml root:root Remediation: By default, RKE2 creates these files with root:root ownership. No manual remediation needed. 1.1.5 \u00b6 Ensure that the scheduler pod specification file permissions are set to 644 or more restrictive (Automated) Rationale The scheduler pod specification file controls various parameters that set the behavior of the Scheduler service in the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/agent/pod-manifests/kube-scheduler.yaml 644 Remediation: By default, RKE2 creates these files with 644 permissions. No manual remediation needed. 1.1.6 \u00b6 Ensure that the scheduler pod specification file ownership is set to root:root (Automated) Rationale The scheduler pod specification file controls various parameters that set the behavior of the kube-scheduler service in the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/agent/pod-manifests/kube-scheduler.yaml root:root Remediation: By default, RKE2 creates these files with root:root ownership. No manual remediation needed. 1.1.7 \u00b6 Ensure that the etcd pod specification file permissions are set to 644 or more restrictive (Automated) Rationale The etcd pod specification file /var/lib/rancher/rke2/agent/pod-manifests/etcd.yaml controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly-available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/agent/pod-manifests/etcd.yaml 644 Remediation: By default, RKE2 creates these files with 644 permissions. No manual remediation needed. 1.1.8 \u00b6 Ensure that the etcd pod specification file ownership is set to root:root (Automated) Rationale The etcd pod specification file /var/lib/rancher/rke2/agent/pod-manifests/etcd.yaml controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly-available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/agent/pod-manifests/etcd.yaml root:root Remediation: By default, RKE2 creates these files with root:root ownership. No manual remediation needed. 1.1.9 \u00b6 Ensure that the Container Network Interface file permissions are set to 644 or more restrictive (Manual) Rationale Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be writable by only the administrators on the system. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/server/manifests/rke2-canal.yml 644 Remediation: RKE2 deploys the default CNI, Canal, using a Helm chart. The chart is defined as a custom resource in a file with 644 permissions. No manual remediation needed. 1.1.10 \u00b6 Ensure that the Container Network Interface file ownership is set to root:root (Manual) Rationale Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be owned by root:root. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/manifests/rke2-canal.yml root:root Remediation: RKE2 deploys the default CNI, Canal, using a Helm chart. The chart is defined as a custom resource in a file with root:root ownership. No manual remediation needed. 1.1.11 \u00b6 Ensure that the etcd data directory permissions are set to 700 or more restrictive (Automated) Rationale etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should not be readable or writable by any group members or the world. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/server/db/etcd 700 Remediation: RKE2 manages the etcd data directory and sets its permissions to 700. No manual remediation needed. 1.1.12 \u00b6 Ensure that the etcd data directory ownership is set to etcd:etcd (Automated) Rationale etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should be owned by etcd:etcd. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/db/etcd etcd:etcd Remediation: When running RKE2 with the profile flag set to cis-1.6 , RKE2 will refuse to start if the etcd user and group doesn't exist on the host. If it does exist, RKE2 will automatically set the ownership of the etcd data directory to etcd:etcd and ensure the etcd static pod is started with that user and group. 1.1.13 \u00b6 Ensure that the admin.conf file permissions are set to 644 or more restrictive (Automated) Rationale The admin.conf is the administrator kubeconfig file defining various settings for the administration of the cluster. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. In RKE2, this file is located at `/var/lib/rancher/rke2/server/cred/admin.kubeconfig`. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/server/cred/admin.kubeconfig 644 Remediation: By default, RKE2 creates this file at /var/lib/rancher/rke2/server/cred/admin.kubeconfig and automatically sets its permissions to 644 . No manual remediation needed. 1.1.14 \u00b6 Ensure that the admin.conf file ownership is set to root:root (Automated) Rationale The admin.conf file contains the admin credentials for the cluster. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. In RKE2, this file is located at `/var/lib/rancher/rke2/server/cred/admin.kubeconfig`. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/cred/admin.kubeconfig root:root Remediation: By default, RKE2 creates this file at stat -c %U:%G /var/lib/rancher/rke2/server/cred/admin.kubeconfig and automatically sets its ownership to root:root . 1.1.15 \u00b6 Ensure that the scheduler.conf file permissions are set to 644 or more restrictive (Automated) Rationale The scheduler.conf file is the kubeconfig file for the Scheduler. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. In RKE2, this file is located at `/var/lib/rancher/rke2/server/cred/scheduler.kubeconfig`. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig 644 Remediation: By default, RKE2 creates this file at /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig and automatically sets its permissions to 644 . No manual remediation needed. 1.1.16 \u00b6 Ensure that the scheduler.conf file ownership is set to root:root (Automated) Rationale The scheduler.conf file is the kubeconfig file for the Scheduler. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. In RKE2, this file is located at `/var/lib/rancher/rke2/server/cred/scheduler.kubeconfig`. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig root:root Remediation: By default, RKE2 creates this file at /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig and automatically sets its ownership to root:root . 1.1.17 \u00b6 Ensure that the controller.kubeconfig file permissions are set to 644 or more restrictive (Automated) Rationale The controller.kubeconfig file is the kubeconfig file for the Scheduler. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. In RKE2, this file is located at `/var/lib/rancher/rke2/server/cred/controller.kubeconfig`. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/server/cred/controller.kubeconfig 644 Remediation: By default, RKE2 creates this file at /var/lib/rancher/rke2/server/cred/controller.kubeconfig and automatically sets its permissions to 644 . No manual remediation needed. 1.1.18 \u00b6 Ensure that the controller.kubeconfig file ownership is set to root:root (Automated) Rationale The controller.kubeconfig file is the kubeconfig file for the Scheduler. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. In RKE2, this file is located at `/var/lib/rancher/rke2/server/cred/controller.kubeconfig`. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/cred/controller.kubeconfig root:root Remediation: By default, RKE2 creates this file at /var/lib/rancher/rke2/server/cred/controller.kubeconfig and automatically sets its ownership to root:root . 1.1.19 \u00b6 Ensure that the Kubernetes PKI directory and file ownership is set to root:root (Automated) Rationale Kubernetes makes use of a number of certificates as part of its operation. You should set the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity. The directory and files should be owned by root:root. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/tls root:root Remediation: By default, RKE2 creates the directory and files with the expected ownership of root:root . No manual remediation should be necessary. 1.1.20 \u00b6 Ensure that the Kubernetes PKI certificate file permissions are set to 644 or more restrictive (Automated) Rationale Kubernetes makes use of a number of certificate files as part of the operation of its components. The permissions on these files should be set to 644 or more restrictive to protect their integrity. Result: Pass Audit: Run the below command on the master node. stat -c %n \\ %a /var/lib/rancher/rke2/server/tls/*.crt Verify that the permissions are 644 or more restrictive. Remediation: By default, RKE2 creates the files with the expected permissions of 644 . No manual remediation is needed. 1.1.21 \u00b6 Ensure that the Kubernetes PKI key file permissions are set to 600 (Automated) Rationale Kubernetes makes use of a number of key files as part of the operation of its components. The permissions on these files should be set to 600 to protect their integrity and confidentiality. Result: Pass Audit: Run the below command on the master node. stat -c %n \\ %a /var/lib/rancher/rke2/server/tls/*.key Verify that the permissions are 600 or more restrictive. Remediation: By default, RKE2 creates the files with the expected permissions of 600 . No manual remediation is needed. 1.2 API Server \u00b6 This section contains recommendations relating to API server configuration flags 1.2.1 \u00b6 Ensure that the --anonymous-auth argument is set to false (Manual) Rationale When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the API server. You should rely on authentication to authorize access and disallow anonymous requests. If you are using RBAC authorization, it is generally considered reasonable to allow anonymous access to the API Server for health checks and discovery purposes, and hence this recommendation is Manual. However, you should consider whether anonymous discovery is an acceptable risk for your purposes. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that --anonymous-auth=false is present. Remediation: By default, RKE2 kube-apiserver is configured to run with this flag and value. No manual remediation is needed. 1.2.2 \u00b6 Ensure that the --basic-auth-file argument is not set (Automated) Rationale Basic authentication uses plaintext credentials for authentication. Currently, the basic authentication credentials last indefinitely, and the password cannot be changed without restarting the API server. The basic authentication is currently supported for convenience. Hence, basic authentication should not be used. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --basic-auth-file argument does not exist. Remediation: By default, RKE2 does not run with basic authentication enabled. No manual remediation is needed. 1.2.3 \u00b6 Ensure that the --token-auth-file parameter is not set (Automated) Rationale The token-based authentication utilizes static tokens to authenticate requests to the apiserver. The tokens are stored in clear-text in a file on the apiserver, and cannot be revoked or rotated without restarting the apiserver. Hence, do not use static token-based authentication. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --token-auth-file argument does not exist. Remediation: By default, RKE2 does not run with basic authentication enabled. No manual remediation is needed. 1.2.4 \u00b6 Ensure that the --kubelet-https argument is set to true (Automated) Rationale Connections from apiserver to kubelets could potentially carry sensitive data such as secrets and keys. It is thus important to use in-transit encryption for any communication between the apiserver and kubelets. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --kubelet-https argument does not exist. Remediation: By default, RKE2 kube-apiserver doesn't run with the --kubelet-https parameter as it runs with TLS. No manual remediation is needed. 1.2.5 \u00b6 Ensure that the --kubelet-client-certificate and --kubelet-client-key arguments are set as appropriate (Automated) Rationale The apiserver, by default, does not authenticate itself to the kubelet's HTTPS endpoints. The requests from the apiserver are treated anonymously. You should set up certificate- based kubelet authentication to ensure that the apiserver authenticates itself to kubelets when submitting requests. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --kubelet-client-certificate and --kubelet-client-key arguments exist and they are set as appropriate. Remediation: By default, RKE2 kube-apiserver is ran with these arguments for secure communication with kubelet. No manual remediation is needed. 1.2.6 \u00b6 Ensure that the --kubelet-certificate-authority argument is set as appropriate (Automated) Rationale The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet\u2019s port-forwarding functionality. These connections terminate at the kubelet\u2019s HTTPS endpoint. By default, the apiserver does not verify the kubelet\u2019s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --kubelet-certificate-authority argument exists and is set as appropriate. Remediation: By default, RKE2 kube-apiserver is ran with this argument for secure communication with kubelet. No manual remediation is needed. 1.2.7 \u00b6 Ensure that the --authorization-mode argument is not set to AlwaysAllow (Automated) Rationale The API Server, can be configured to allow all requests. This mode should not be used on any production cluster. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the argument value doesn't contain AlwaysAllow . Remediation: By default, RKE2 sets Node,RBAC as the parameter to the --authorization-mode argument. No manual remediation is needed. 1.2.8 \u00b6 Ensure that the --authorization-mode argument includes Node (Automated) Rationale The Node authorization mode only allows kubelets to read Secret, ConfigMap, PersistentVolume, and PersistentVolumeClaim objects associated with their nodes. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify Node exists as a parameter to the argument. Remediation: By default, RKE2 sets Node,RBAC as the parameter to the --authorization-mode argument. No manual remediation is needed. 1.2.9 \u00b6 Ensure that the --authorization-mode argument includes RBAC (Automated) Rationale Role Based Access Control (RBAC) allows fine-grained control over the operations that different entities can perform on different objects in the cluster. It is recommended to use the RBAC authorization mode. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify RBAC exists as a parameter to the argument. Remediation: By default, RKE2 sets Node,RBAC as the parameter to the --authorization-mode argument. No manual remediation is needed. 1.2.10 \u00b6 Ensure that the admission control plugin EventRateLimit is set (Manual) Rationale Using `EventRateLimit` admission control enforces a limit on the number of events that the API Server will accept in a given time slice. A misbehaving workload could overwhelm and DoS the API Server, making it unavailable. This particularly applies to a multi-tenant cluster, where there might be a small percentage of misbehaving tenants which could have a significant impact on the performance of the cluster overall. Hence, it is recommended to limit the rate of events that the API server will accept. Note: This is an Alpha feature in the Kubernetes 1.15 release. Result: Manual - Operator Dependent Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --enable-admission-plugins argument is set to a value that includes EventRateLimit. Remediation: By default, RKE2 only sets NodeRestriction,PodSecurityPolicy as the parameter to the --enable-admission-plugins argument. To configure this, follow the Kubernetes documentation and set the desired limits in a configuration file. Then refer to RKE2's documentation to see how to supply additional api server configuration via the kube-apiserver-arg parameter. 1.2.11 \u00b6 Ensure that the admission control plugin AlwaysAdmit is not set (Automated) Rationale Setting admission control plugin AlwaysAdmit allows all requests and do not filter any requests. The AlwaysAdmit admission controller was deprecated in Kubernetes v1.13. Its behavior was equivalent to turning off all admission controllers. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that if the --enable-admission-plugins argument is set, its value does not include AlwaysAdmit . Remediation: By default, RKE2 only sets NodeRestriction,PodSecurityPolicy as the parameter to the --enable-admission-plugins argument. No manual remediation needed. 1.2.12 \u00b6 Ensure that the admission control plugin AlwaysPullImages is set (Manual) Rationale Setting admission control policy to `AlwaysPullImages` forces every new pod to pull the required images every time. In a multi-tenant cluster users can be assured that their private images can only be used by those who have the credentials to pull them. Without this admission control policy, once an image has been pulled to a node, any pod from any user can use it simply by knowing the image\u2019s name, without any authorization check against the image ownership. When this plug-in is enabled, images are always pulled prior to starting containers, which means valid credentials are required. Result: Manual - Operator Dependent Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --enable-admission-plugins argument is set to a value that includes AlwaysPullImages . Remediation: By default, RKE2 only sets NodeRestriction,PodSecurityPolicy as the parameter to the --enable-admission-plugins argument. To configure this, follow the Kubernetes documentation and set the desired limits in a configuration file. Then refer to RKE2's documentation to see how to supply additional api server configuration via the kube-apiserver-arg parameter. 1.2.13 \u00b6 Ensure that the admission control plugin SecurityContextDeny is set if PodSecurityPolicy is not used (Manual) Rationale SecurityContextDeny can be used to provide a layer of security for clusters which do not have PodSecurityPolicies enabled. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --enable-admission-plugins argument is set to a value that includes SecurityContextDeny , if PodSecurityPolicy is not included. Remediation: By default, RKE2 automatically enables the PodSecurityPolicy admission plugin. Therefore, the SecurityContextDeny plugin need not be enabled. No manual remediation needed. 1.2.14 \u00b6 Ensure that the admission control plugin ServiceAccount is set (Automated) Rationale When you create a pod, if you do not specify a service account, it is automatically assigned the `default` service account in the same namespace. You should create your own service account and let the API server manage its security tokens. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --disable-admission-plugins argument is set to a value that does not includes ServiceAccount . Remediation: By default, RKE2 does not use this argument. If there's a desire to use this argument, follow the documentation and create ServiceAccount objects as per your environment. Then refer to RKE2's documentation to see how to supply additional api server configuration via the kube-apiserver-arg parameter. 1.2.15 \u00b6 Ensure that the admission control plugin NamespaceLifecycle is set (Automated) Rationale Setting admission control policy to `NamespaceLifecycle` ensures that objects cannot be created in non-existent namespaces, and that namespaces undergoing termination are not used for creating the new objects. This is recommended to enforce the integrity of the namespace termination process and also for the availability of the newer objects. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --disable-admission-plugins argument is set to a value that does not include NamespaceLifecycle . Remediation: By default, RKE2 does not use this argument. No manual remediation needed. 1.2.16 \u00b6 Ensure that the admission control plugin PodSecurityPolicy is set (Automated) Rationale A Pod Security Policy is a cluster-level resource that controls the actions that a pod can perform and what it has the ability to access. The `PodSecurityPolicy` objects define a set of conditions that a pod must run with in order to be accepted into the system. Pod Security Policies are comprised of settings and strategies that control the security features a pod has access to and hence this must be used to control pod access permissions. **Note:** When the PodSecurityPolicy admission plugin is in use, there needs to be at least one PodSecurityPolicy in place for ANY pods to be admitted. See section 1.7 for recommendations on PodSecurityPolicy settings. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --enable-admission-plugins argument is set to a value that includes PodSecurityPolicy . Remediation: By default, RKE2 only sets NodeRestriction,PodSecurityPolicy as the parameter to the --enable-admission-plugins argument. No manual remediation needed. 1.2.17 \u00b6 Ensure that the admission control plugin NodeRestriction is set (Automated) Rationale Using the `NodeRestriction` plug-in ensures that the kubelet is restricted to the `Node` and `Pod` objects that it could modify as defined. Such kubelets will only be allowed to modify their own `Node` API object, and only modify `Pod` API objects that are bound to their node. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Remediation: By default, RKE2 only sets NodeRestriction,PodSecurityPolicy as the parameter to the --enable-admission-plugins argument. No manual remediation needed. 1.2.18 \u00b6 Ensure that the --insecure-bind-address argument is not set (Automated) Rationale If you bind the apiserver to an insecure address, basically anyone who could connect to it over the insecure port, would have unauthenticated and unencrypted access to your master node. The apiserver doesn't do any authentication checking for insecure binds and traffic to the Insecure API port is not encrpyted, allowing attackers to potentially read sensitive data in transit. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --insecure-bind-address argument does not exist. Remediation: By default, RKE2 explicitly excludes the use of the --insecure-bind-address parameter. No manual remediation is needed. 1.2.19 \u00b6 Ensure that the --insecure-port argument is set to 0 (Automated) Rationale Setting up the apiserver to serve on an insecure port would allow unauthenticated and unencrypted access to your master node. This would allow attackers who could access this port, to easily take control of the cluster. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --insecure-port argument is set to 0. Remediation: By default, RKE2 starts the kube-apiserver process with this argument's parameter set to 0. No manual remediation is needed. 1.2.20 \u00b6 Ensure that the --secure-port argument is not set to 0 (Automated) Rationale The secure port is used to serve https with authentication and authorization. If you disable it, no https traffic is served and all traffic is served unencrypted. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --secure-port argument is either not set or is set to an integer value between 1 and 65535. Remediation: By default, RKE2 sets the parameter of 6443 for the --secure-port argument. No manual remediation is needed. 1.2.21 \u00b6 Ensure that the --profiling argument is set to false (Automated) Rationale Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --profiling argument is set to false. Remediation: By default, RKE2 sets the --profiling flag parameter to false. No manual remediation needed. 1.2.22 \u00b6 Ensure that the --audit-log-path argument is set (Automated) Rationale Auditing the Kubernetes API Server provides a security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators or other components of the system. Even though currently, Kubernetes provides only basic audit capabilities, it should be enabled. You can enable it by setting an appropriate audit log path. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --audit-log-path argument is set as appropriate. Remediation: By default, RKE2 sets the --audit-log-path argument and parameter. No manual remediation needed. 1.2.23 \u00b6 Ensure that the --audit-log-maxage argument is set to 30 or as appropriate (Automated) Rationale Retaining logs for at least 30 days ensures that you can go back in time and investigate or correlate any events. Set your audit log retention period to 30 days or as per your business requirements. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --audit-log-maxage argument is set to 30 or as appropriate. Remediation: By default, RKE2 sets the --audit-log-maxage argument parameter to 30. No manual remediation needed. 1.2.24 \u00b6 Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate (Automated) Rationale Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. For example, if you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --audit-log-maxbackup argument is set to 10 or as appropriate. Remediation: By default, RKE2 sets the --audit-log-maxbackup argument parameter to 10. No manual remediation needed. 1.2.25 \u00b6 Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate (Automated) Rationale Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. If you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --audit-log-maxsize argument is set to 100 or as appropriate. Remediation: By default, RKE2 sets the --audit-log-maxsize argument parameter to 100. No manual remediation needed. 1.2.26 \u00b6 Ensure that the --request-timeout argument is set as appropriate (Automated) Rationale Setting global request timeout allows extending the API server request timeout limit to a duration appropriate to the user's connection speed. By default, it is set to 60 seconds which might be problematic on slower connections making cluster resources inaccessible once the data volume for requests exceeds what can be transmitted in 60 seconds. But, setting this timeout limit to be too large can exhaust the API server resources making it prone to Denial-of-Service attack. Hence, it is recommended to set this limit as appropriate and change the default limit of 60 seconds only if needed. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --request-timeout argument is either not set or set to an appropriate value. Remediation: By default, RKE2 does not set the --request-timeout argument. No manual remediation needed. 1.2.27 \u00b6 Ensure that the --service-account-lookup argument is set to true (Automated) Rationale If `--service-account-lookup` is not enabled, the apiserver only verifies that the authentication token is valid, and does not validate that the service account token mentioned in the request is actually present in etcd. This allows using a service account token even after the corresponding service account is deleted. This is an example of time of check to time of use security issue. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that if the --service-account-lookup argument exists it is set to true. Remediation: By default, RKE2 doesn't set this argument in favor of taking the default effect. No manual remediation needed. 1.2.28 \u00b6 Ensure that the --service-account-key-file argument is set as appropriate (Automated) Rationale By default, if no `--service-account-key-file` is specified to the apiserver, it uses the private key from the TLS serving certificate to verify service account tokens. To ensure that the keys for service account tokens could be rotated as needed, a separate public/private key pair should be used for signing service account tokens. Hence, the public key should be specified to the apiserver with `--service-account-key-file`. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --service-account-key-file argument exists and is set as appropriate. Remediation: By default, RKE2 sets the --service-account-key-file explicitly. No manual remediation needed. 1.2.29 \u00b6 Ensure that the --etcd-certfile and --etcd-keyfile arguments are set as appropriate (Automated) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a client certificate and key. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --etcd-certfile and --etcd-keyfile arguments exist and they are set as appropriate. Remediation: By default, RKE2 sets the --etcd-certfile and --etcd-keyfile arguments explicitly. No manual remediation needed. 1.2.30 \u00b6 Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Automated) Rationale API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --tls-cert-file and --tls-private-key-file arguments exist and they are set as appropriate. Remediation: By default, RKE2 sets the --tls-cert-file and --tls-private-key-file arguments explicitly. No manual remediation needed. 1.2.31 \u00b6 Ensure that the --client-ca-file argument is set as appropriate (Automated) Rationale API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. If `--client-ca-file` argument is set, any request presenting a client certificate signed by one of the authorities in the `client-ca-file` is authenticated with an identity corresponding to the CommonName of the client certificate. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --client-ca-file argument exists and it is set as appropriate. Remediation: By default, RKE2 sets the --client-ca-file argument explicitly. No manual remediation needed. 1.2.32 \u00b6 Ensure that the --etcd-cafile argument is set as appropriate (Automated) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a SSL Certificate Authority file. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --etcd-cafile argument exists and it is set as appropriate. Remediation: By default, RKE2 sets the --etcd-cafile argument explicitly. No manual remediation needed. 1.2.33 \u00b6 Ensure that the --encryption-provider-config argument is set as appropriate (Automated) Rationale etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted at rest to avoid any disclosures. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --encryption-provider-config argument is set to a EncryptionConfigfile. Additionally, ensure that the EncryptionConfigfile has all the desired resources covered especially any secrets. Remediation: By default, RKE2 sets the --encryption-provider-config argument explicitly. No manual remediation needed. RKE2's default encryption provider config file is located at /var/lib/rancher/rke2/server/cred/encryption-config.json and is configured to encrypt secrets. 1.2.34 \u00b6 Ensure that encryption providers are appropriately configured (Automated) Rationale Where `etcd` encryption is used, it is important to ensure that the appropriate set of encryption providers is used. Currently, the `aescbc`, `kms` and `secretbox` are likely to be appropriate options. Result: Pass Remediation: Follow the Kubernetes documentation and configure a EncryptionConfig file. In this file, choose aescbc , kms or secretbox as the encryption provider. Audit: Run the below command on the master node. grep aescbc /var/lib/rancher/rke2/server/cred/encryption-config.json Run the below command on the master node. Verify that aescbc is set as the encryption provider for all the desired resources. Remediation By default, RKE2 sets the argument --encryption-provider-config and parameter. The contents of the config file indicates the use of aescbc. No manual remediation needed. 1.2.35 \u00b6 Ensure that the API Server only makes use of Strong Cryptographic Ciphers (Manual) Rationale TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided. Result: Manual - Operator Dependent Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --tls-cipher-suites argument is set as outlined in the remediation procedure below. Remediation: By default, RKE2 explicitly doesn't set this flag. No manual remediation needed. 1.3 Controller Manager \u00b6 1.3.1 \u00b6 Ensure that the --terminated-pod-gc-threshold argument is set as appropriate (Manual) Rationale Garbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. The current setting for garbage collection is 12,500 terminated pods which might be too high for your system to sustain. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection. Result: Manual - Operator Dependent Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --terminated-pod-gc-threshold argument is set as appropriate. Remediation: By default, RKE2 sets the --terminated-pod-gc-threshold argument with a value of 1000. No manual remediation needed. 1.3.2 \u00b6 Ensure that the --profiling argument is set to false (Automated) Rationale Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --profiling argument is set to false. Remediation: By default, RKE2 sets the --profiling flag parameter to false. No manual remediation needed. 1.3.3 \u00b6 Ensure that the --use-service-account-credentials argument is set to true (Automated) Rationale The controller manager creates a service account per controller in the `kube-system` namespace, generates a credential for it, and builds a dedicated API client with that service account credential for each controller loop to use. Setting the `--use-service-account-credentials` to `true` runs each control loop within the controller manager using a separate service account credential. When used in combination with RBAC, this ensures that the control loops run with the minimum permissions required to perform their intended tasks. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --use-service-account-credentials argument is set to true. Remediation: By default, RKE2 sets the --use-service-account-credentials argument to true. No manual remediation needed. 1.3.4 \u00b6 Ensure that the --service-account-private-key-file argument is set as appropriate (Automated) Rationale To ensure that keys for service account tokens can be rotated as needed, a separate public/private key pair should be used for signing service account tokens. The private key should be specified to the controller manager with `--service-account-private-key-file` as appropriate. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --service-account-private-key-file argument is set as appropriate. Remediation: By default, RKE2 sets the --service-account-private-key-file argument with the service account key file. No manual remediation needed. 1.3.5 \u00b6 Ensure that the --root-ca-file argument is set as appropriate (Automated) Rationale Processes running within pods that need to contact the API server must verify the API server's serving certificate. Failing to do so could be a subject to man-in-the-middle attacks. Providing the root certificate for the API server's serving certificate to the controller manager with the `--root-ca-file` argument allows the controller manager to inject the trusted bundle into pods so that they can verify TLS connections to the API server. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --root-ca-file argument exists and is set to a certificate bundle file containing the root certificate for the API server's serving certificate Remediation: By default, RKE2 sets the --root-ca-file argument with the root ca file. No manual remediation needed. 1.3.6 \u00b6 Ensure that the RotateKubeletServerCertificate argument is set to true (Automated) Rationale `RotateKubeletServerCertificate` causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Result: Not Applicable Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that RotateKubeletServerCertificateargument exists and is set to true. Remediation: By default, RKE2 implements it's own logic for certificate generation and rotation. 1.3.7 \u00b6 Ensure that the --bind-address argument is set to 127.0.0.1 (Automated) Rationale The Controller Manager API service which runs on port 10252/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --bind-address argument is set to 127.0.0.1. Remediation: By default, RKE2 sets the --bind-address argument to 127.0.0.1 . No manual remediation needed. 1.4 Scheduler \u00b6 This section contains recommendations relating to Scheduler configuration flags 1.4.1 \u00b6 Ensure that the --profiling argument is set to false (Automated) Rationale Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-scheduler | grep -v grep Verify that the --profiling argument is set to false. Remediation: By default, RKE2 sets the --profiling flag parameter to false. No manual remediation needed. 1.4.2 \u00b6 Ensure that the --bind-address argument is set to 127.0.0.1 (Automated) Rationale The Scheduler API service which runs on port 10251/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-scheduler | grep -v grep Verify that the --bind-address argument is set to 127.0.0.1. Remediation: By default, RKE2 sets the --bind-address argument to 127.0.0.1 . No manual remediation needed. 2 Etcd Node Configuration \u00b6 This section covers recommendations for etcd configuration. 2.1 \u00b6 Ensure that the cert-file and key-file fields are set as appropriate (Automated) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit. Result: Not Applicable Audit: Run the below command on the master node. grep -E 'cert-file|key-file' /var/lib/rancher/rke2/server/db/etcd/config Verify that the cert-file and the key-file fields are set as appropriate. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . Server and peer cert and key files are specified. No manual remediation needed. 2.2 \u00b6 Ensure that the client-cert-auth field is set to true (Automated) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service. Result: Not Applicable Audit: Run the below command on the master node. grep 'client-cert-auth' /var/lib/rancher/rke2/server/db/etcd/config Verify that the client-cert-auth field is set to true. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . client-cert-auth is set to true. No manual remediation needed. 2.3 \u00b6 Ensure that the auto-tls field is not set to true (Automated) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service. Result: Pass Audit: Run the below command on the master node. grep 'auto-tls' /var/lib/rancher/rke2/server/db/etcd/config Verify that if the auto-tls field does not exist. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . Within the file, it does not contain the auto-tls argument. No manual remediation needed. 2.4 \u00b6 Ensure that the peer-cert-file and peer-key-file fields are set as appropriate (Automated) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit and also amongst peers in the etcd clusters. Result: Not Applicable Audit: Run the below command on the master node. grep -E 'peer-server-client.crt|peer-server-client.key' /var/lib/rancher/rke2/server/db/etcd/config Verify that the peer-server-client.crt and peer-server-client.key fields are set as appropriate. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . Within the file, the peer-server-client.crt and peer-server-client.key fields are set. No manual remediation needed. 2.5 \u00b6 Ensure that the client-cert-auth field is set to true (Automated) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Result: Not Applicable Audit: Run the below command on the master node. grep 'client-cert-auth' /var/lib/rancher/rke2/server/db/etcd/config Verify that the client-cert-auth field in the peer section is set to true. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . Within the file, the client-cert-auth field is set. No manual remediation needed. 2.6 \u00b6 Ensure that the peer-auto-tls field is not set to true (Automated) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Hence, do not use self- signed certificates for authentication. Result: Pass Audit: Run the below command on the master node. grep 'peer-auto-tls' /var/lib/rancher/rke2/server/db/etcd/config Verify that if the peer-auto-tls field does not exist. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . Within the file, it does not contain the peer-auto-tls field. No manual remediation needed. 2.7 \u00b6 Ensure that a unique Certificate Authority is used for etcd (Manual) Rationale etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. Its access should be restricted to specifically designated clients and peers only. Authentication to etcd is based on whether the certificate presented was issued by a trusted certificate authority. There is no checking of certificate attributes such as common name or subject alternative name. As such, if any attackers were able to gain access to any certificate issued by the trusted certificate authority, they would be able to gain full access to the etcd database. Result: Pass Audit: Run the below command on the master node. # To find the ca file used by etcd: grep 'trusted-ca-file' /var/lib/rancher/rke2/server/db/etcd/config # To find the kube-apiserver process: /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the file referenced by the client-ca-file flag in the apiserver process is different from the file referenced by the trusted-ca-file parameter in the etcd configuration file. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config and the trusted-ca-file parameters in it are set to unique values specific to etcd. No manual remediation needed. 3 Control Plane Configuration \u00b6 3.1 Authentication and Authorization \u00b6 3.1.1 \u00b6 Client certificate authentication should not be used for users (Manual) Rationale With any authentication mechanism the ability to revoke credentials if they are compromised or no longer required, is a key control. Kubernetes client certificate authentication does not allow for this due to a lack of support for certificate revocation. Result: Manual - Operator Dependent Audit: Review user access to the cluster and ensure that users are not making use of Kubernetes client certificate authentication. Remediation: Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of client certificates. 3.2 Logging \u00b6 3.2.1 \u00b6 Ensure that a minimal audit policy is created (Automated) Rationale Logging is an important detective control for all systems, to detect potential unauthorised access. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --audit-policy-file is set. Review the contents of the file specified and ensure that it contains a valid audit policy. Remediation: Create an audit policy file for your cluster. 3.2.2 \u00b6 Ensure that the audit policy covers key security concerns (Manual) Rationale Security audit logs should cover access and modification of key resources in the cluster, to enable them to form an effective part of a security environment. Result: Manual - Operator Dependent Remediation: 4 Worker Node Security Configuration \u00b6 4.1 Worker Node Configuration Files \u00b6 4.1.1 \u00b6 Ensure that the kubelet service file permissions are set to 644 or more restrictive (Automated) Rationale The `kubelet` service file controls various parameters that set the behavior of the kubelet service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Not Applicable Remediation: RKE2 doesn\u2019t launch the kubelet as a service. It is launched and managed by the RKE2 supervisor process. All configuration is passed to it as command line arguments at run time. 4.1.2 \u00b6 Ensure that the kubelet service file ownership is set to root:root (Automated) Rationale The `kubelet` service file controls various parameters that set the behavior of the kubelet service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by `root:root`. Result: Not Applicable Remediation: RKE2 doesn\u2019t launch the kubelet as a service. It is launched and managed by the RKE2 supervisor process. All configuration is passed to it as command line arguments at run time. 4.1.3 \u00b6 Ensure that the proxy kubeconfig file permissions are set to 644 or more restrictive (Manual) Rationale The `kube-proxy` kubeconfig file controls various parameters of the `kube-proxy` service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. It is possible to run `kube-proxy` with the kubeconfig parameters configured as a Kubernetes ConfigMap instead of a file. In this case, there is no proxy kubeconfig file. Result: Pass Audit: Run the below command on the worker node. stat -c %a /var/lib/rancher/rke2/server/manifests/rke2-kube-proxy.yaml 644 Verify that if a file is specified and it exists, the permissions are 644 or more restrictive. Remediation: By derfault, RKE2 creates rke2-kube-proxy.yaml with 644 permissions. No manual remediation needed. 4.1.4 \u00b6 Ensure that the proxy kubeconfig file ownership is set to root:root (Manual) Rationale The kubeconfig file for `kube-proxy` controls various parameters for the `kube-proxy` service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by `root:root`. Result: Pass Audit: Run the below command on the master node. stat -c %U:%G /var/lib/rancher/rke2/server/manifests/rke2-kube-proxy.yaml root:root Verify that if a file is specified and it exists, the permissions are 644 or more restrictive. Remediation: By default, RKE2 creates rke2-kube-proxy.yaml with root:root ownership. No manual remediation needed. 4.1.5 \u00b6 Ensure that the kubelet.conf file permissions are set to 644 or more restrictive (Automated) Rationale The `kubelet.conf` file is the kubeconfig file for the node, and controls various parameters that set the behavior and identity of the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Not Applicable Audit: Run the below command on the worker node. stat -c %a /var/lib/rancher/rke2/agent/kubelet.kubeconfig 644 Remediation: By derfault, RKE2 creates kubelet.kubeconfig with 644 permissions. No manual remediation needed. 4.1.6 \u00b6 Ensure that the kubelet.conf file ownership is set to root:root (Manual) Rationale The `kubelet.conf` file is the kubeconfig file for the node, and controls various parameters that set the behavior and identity of the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by `root:root`. Result: Not Applicable Audit: Run the below command on the master node. stat -c %U:%G /var/lib/rancher/rke2/agent/kubelet.kubeconfig root:root Remediation: By default, RKE2 creates kubelet.kubeconfig with root:root ownership. No manual remediation needed. 4.1.7 \u00b6 Ensure that the certificate authorities file permissions are set to 644 or more restrictive (Manual) Rationale The certificate authorities file controls the authorities used to validate API requests. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Manual - Operator Dependent Audit: Run the below command on the master node. stat -c %a /var/lib/rancher/rke2/server/tls/server-ca.crt 644 Verify that the permissions are 644. Remediation: By default, RKE2 creates /var/lib/rancher/rke2/server/tls/server-ca.crt with 644 permissions. 4.1.8 \u00b6 Ensure that the client certificate authorities file ownership is set to root:root (Automated) Rationale The certificate authorities file controls the authorities used to validate API requests. You should set its file ownership to maintain the integrity of the file. The file should be owned by `root:root`. Result: Pass Audit: Run the below command on the master node. stat -c %U:%G /var/lib/rancher/rke2/server/tls/client-ca.crt root:root Remediation: By default, RKE2 creates /var/lib/rancher/rke2/server/tls/client-ca.crt with root:root ownership. 4.1.9 \u00b6 Ensure that the kubelet configuration file has permissions set to 644 or more restrictive (Automated) Rationale The kubelet reads various parameters, including security settings, from a config file specified by the `--config` argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Not Applicable Remediation: RKE2 doesn\u2019t require or maintain a configuration file for the kubelet process. All configuration is passed to it as command line arguments at run time. 4.1.10 \u00b6 Ensure that the kubelet configuration file ownership is set to root:root (Automated) Rationale The kubelet reads various parameters, including security settings, from a config file specified by the `--config` argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be owned by `root:root`. Result: Not Applicable Remediation: RKE2 doesn\u2019t require or maintain a configuration file for the kubelet process. All configuration is passed to it as command line arguments at run time. 4.2 Kubelet \u00b6 This section contains recommendations for kubelet configuration. 4.2.1 \u00b6 Ensure that the --anonymous-auth argument is set to false (Automated) Rationale When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the Kubelet server. You should rely on authentication to authorize access and disallow anonymous requests. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify that the value for --anonymous-auth is false. Remediation: By default, RKE2 starts kubelet with --anonymous-auth set to false. No manual remediation needed. 4.2.2 \u00b6 Ensure that the --authorization-mode argument is not set to AlwaysAllow (Automated) Rationale Kubelets, by default, allow all authenticated requests (even anonymous ones) without needing explicit authorization checks from the apiserver. You should restrict this behavior and only allow explicitly authorized requests. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify that AlwaysAllow is not present. Remediation: RKE2 starts kubelet with Webhook as the value for the --authorization-mode argument. No manual remediation needed. 4.2.3 \u00b6 Ensure that the --client-ca-file argument is set as appropriate (Automated) Rationale The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet\u2019s port-forwarding functionality. These connections terminate at the kubelet\u2019s HTTPS endpoint. By default, the apiserver does not verify the kubelet\u2019s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Enabling Kubelet certificate authentication ensures that the apiserver could authenticate the Kubelet before submitting any requests. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify that the --client-ca-file argument has a ca file associated. Remediation: By default, RKE2 starts the kubelet process with the --client-ca-file . No manual remediation needed. 4.2.4 \u00b6 Ensure that the --read-only-port argument is set to 0 (Automated) Rationale The Kubelet process provides a read-only API in addition to the main Kubelet API. Unauthenticated access is provided to this read-only API which could possibly retrieve potentially sensitive information about the cluster. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify that the --read-only-port argument is set to 0. Remediation: By default, RKE2 starts the kubelet process with the --read-only-port argument set to 0. 4.2.5 \u00b6 Ensure that the --streaming-connection-idle-timeout argument is not set to 0 (Automated) Rationale Setting idle timeouts ensures that you are protected against Denial-of-Service attacks, inactive connections and running out of ephemeral ports. **Note:** By default, `--streaming-connection-idle-timeout` is set to 4 hours which might be too high for your environment. Setting this as appropriate would additionally ensure that such streaming connections are timed out after serving legitimate use cases. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify that there's nothing returned. Remediation: By default, RKE2 does not set --streaming-connection-idle-timeout when starting kubelet. 4.2.6 \u00b6 Ensure that the --protect-kernel-defaults argument is set to true (Automated) Rationale Kernel parameters are usually tuned and hardened by the system administrators before putting the systems into production. These parameters protect the kernel and the system. Your kubelet kernel defaults that rely on such parameters should be appropriately set to match the desired secured system state. Ignoring this could potentially lead to running pods with undesired kernel behavior. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Remediation: When running with the profile flag set to cis-1.6 , RKE2 starts the kubelet process with the --protect-kernel-defaults argument set to true. 4.2.7 \u00b6 Ensure that the --make-iptables-util-chains argument is set to true (Automated) Rationale Kubelets can automatically manage the required changes to iptables based on how you choose your networking options for the pods. It is recommended to let kubelets manage the changes to iptables. This ensures that the iptables configuration remains in sync with pods networking configuration. Manually configuring iptables with dynamic pod network configuration changes might hamper the communication between pods/containers and to the outside world. You might have iptables rules too restrictive or too open. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify there are no results returned. Remediation: By default, RKE2 does not set the --make-iptables-util-chains argument. No manual remediation needed. 4.2.8 \u00b6 Ensure that the --hostname-override argument is not set (Manual) Rationale Overriding hostnames could potentially break TLS setup between the kubelet and the apiserver. Additionally, with overridden hostnames, it becomes increasingly difficult to associate logs with a particular node and process them for security analytics. Hence, you should setup your kubelet nodes with resolvable FQDNs and avoid overriding the hostnames with IPs. Result: Not Applicable Remediation: RKE2 does set this parameter for each host, but RKE2 also manages all certificates in the cluster. It ensures the hostname-override is included as a subject alternative name (SAN) in the kubelet's certificate. 4.2.9 \u00b6 Ensure that the --event-qps argument is set to 0 or a level which ensures appropriate event capture (Manual) Rationale It is important to capture all events and not restrict event creation. Events are an important source of security information and analytics that ensure that your environment is consistently monitored using the event data. Result: Manual - Operator Dependent Remediation: See CIS Benchmark guide for further details on configuring this. 4.2.10 \u00b6 Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Automated) Rationale Kubelet communication contains sensitive parameters that should remain encrypted in transit. Configure the Kubelets to serve only HTTPS traffic. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify the --tls-cert-file and --tls-private-key-file arguments are present and set appropriately. Remediation: By default, RKE2 sets the --tls-cert-file and --tls-private-key-file arguments when executing the kubelet process. 4.2.11 \u00b6 Ensure that the --rotate-certificates argument is not set to false (Manual) Rationale The `--rotate-certificates` setting causes the kubelet to rotate its client certificates by creating new CSRs as its existing credentials expire. This automated periodic rotation ensures that the there is no downtime due to expired certificates and thus addressing availability in the CIA security triad. **Note:** This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. **Note:**This feature also require the `RotateKubeletClientCertificate` feature gate to be enabled (which is the default since Kubernetes v1.7) Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Remediation: By default, RKE2 implements it's own logic for certificate generation and rotation. 4.2.12 \u00b6 Ensure that the RotateKubeletServerCertificate argument is set to true (Manual) Rationale `RotateKubeletServerCertificate` causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Remediation: By default, RKE2 implements it's own logic for certificate generation and rotation. 4.2.13 \u00b6 Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers (Manual) Rationale TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided. Result: Manual - Operator Dependent Remediation: Configuration of the parameter is dependent on your use case. Please see the CIS Kubernetes Benchmark for suggestions on configuring this for your usecase. 5 Kubernetes Policies \u00b6 5.1 RBAC and Service Accounts \u00b6 5.1.1 \u00b6 Ensure that the cluster-admin role is only used where required (Manual) Rationale Kubernetes provides a set of default roles where RBAC is used. Some of these roles such as `cluster-admin` provide wide-ranging privileges which should only be applied where absolutely necessary. Roles such as `cluster-admin` allow super-user access to perform any action on any resource. When used in a `ClusterRoleBinding`, it gives full control over every resource in the cluster and in all namespaces. When used in a `RoleBinding`, it gives full control over every resource in the rolebinding's namespace, including the namespace itself. Result: Pass Remediation: RKE2 does not make inappropriate use of the cluster-admin role. Operators must audit their workloads of additional usage. See the CIS Benchmark guide for more details. 5.1.2 \u00b6 Minimize access to secrets (Manual) Rationale Inappropriate access to secrets stored within the Kubernetes cluster can allow for an attacker to gain additional access to the Kubernetes cluster or external resources whose credentials are stored as secrets. Result: Manual - Operator Dependent Remediation: RKE2 limits its use of secrets for the system components appropriately, but operators must audit the use of secrets by their workloads. See the CIS Benchmark guide for more details. 5.1.3 \u00b6 Minimize wildcard use in Roles and ClusterRoles (Manual) Rationale The principle of least privilege recommends that users are provided only the access required for their role and nothing more. The use of wildcard rights grants is likely to provide excessive rights to the Kubernetes API. Result: Manual - Operator Dependent Audit: Run the below command on the master node. # Retrieve the roles defined across each namespaces in the cluster and review for wildcards /var/lib/rancher/rke2/bin/kubectl get roles --all-namespaces -o yaml # Retrieve the cluster roles defined in the cluster and review for wildcards /var/lib/rancher/rke2/bin/kubectl get clusterroles -o yaml Verify that there are not wildcards in use. Remediation: Operators should review their workloads for proper role usage. See the CIS Benchmark guide for more details. 5.1.4 \u00b6 Minimize access to create pods (Manual) Rationale The ability to create pods in a cluster opens up possibilities for privilege escalation and should be restricted, where possible. Result: Manual - Operator Dependent Remediation: Operators should review who has access to create pods in their cluster. See the CIS Benchmark guide for more details. 5.1.5 \u00b6 Ensure that default service accounts are not actively used. (Automated) Rationale Kubernetes provides a default service account which is used by cluster workloads where no specific service account is assigned to the pod. Where access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account. The default service account should be configured such that it does not provide a service account token and does not have any explicit rights assignments. Result: Pass. Audit: For each namespace in the cluster, review the rights assigned to the default service account and ensure that it has no roles or cluster roles bound to it apart from the defaults. Additionally ensure that the automountServiceAccountToken: false setting is in place for each default service account. Remediation: Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server. Modify the configuration of each default service account to include this value automountServiceAccountToken: false 5.1.6 \u00b6 Ensure that Service Account Tokens are only mounted where necessary (Manual) Rationale Mounting service account tokens inside pods can provide an avenue for privilege escalation attacks where an attacker is able to compromise a single pod in the cluster. Avoiding mounting these tokens removes this attack avenue. Result: Manual - Operator Dependent Remediation: The pods launched by RKE2 are part of the control plane and generally need access to communicate with the API server, thus this control does not apply to them. Operators should review their workloads and take steps to modify the definition of pods and service accounts which do not need to mount service account tokens to disable it. 5.2 Pod Security Policies \u00b6 5.2.1 \u00b6 Minimize the admission of containers wishing to share the host process ID namespace (Automated) Rationale Privileged containers have access to all Linux Kernel capabilities and devices. A container running with full privileges can do almost everything that the host can do. This flag exists to allow special use-cases, like manipulating the network stack and accessing devices. There should be at least one PodSecurityPolicy (PSP) defined which does not permit privileged containers. If you need to run privileged containers, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl describe psp global-restricted-psp | grep MustRunAsNonRoot Verify that the result is Rule: MustRunAsNonRoot . Remediation: RKE2, when run with the --profile=cis-1.6 argument, will disallow the use of privileged containers. 5.2.2 \u00b6 Minimize the admission of containers wishing to share the host process ID namespace (Automated) Rationale A container running in the host's PID namespace can inspect processes running outside the container. If the container also has access to ptrace capabilities this can be used to escalate privileges outside of the container. There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to share the host PID namespace. If you need to run containers which require hostPID, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp -o json | jq .items [] | jq -r 'select((.spec.hostPID == null) or (.spec.hostPID == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}' Verify that the returned count is 1. Remediation: RKE2 sets the hostPID value to false explicitly for the PSP it creates. When reviewing PSPs, note that the Kubernetes API only displays this field if it is explicitly set to true. No manual remediation is needed. 5.2.3 \u00b6 Minimize the admission of containers wishing to share the host IPC namespace (Automated) Rationale A container running in the host's IPC namespace can use IPC to interact with processes outside the container. There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to share the host IPC namespace. If you have a requirement to containers which require hostIPC, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp -o json | jq .items [] | jq -r 'select((.spec.hostIPC == null) or (.spec.hostIPC == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}' Verify that the returned count is 1. Remediation: RKE2 sets the HostIPC value to false explicitly for the PSP it creates. When reviewing PSPs, note that the Kubernetes API only displays this field if it is explicitly set to true. No manual remediation is needed. 5.2.4 \u00b6 Minimize the admission of containers wishing to share the host network namespace (Automated) Rationale A container running in the host's network namespace could access the local loopback device, and could access network traffic to and from other pods. There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to share the host network namespace. If you have need to run containers which require hostNetwork, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp -o json | jq .items [] | jq -r 'select((.spec.hostNetwork == null) or (.spec.hostNetwork == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}' Verify that the returned count is 1. Remediation: RKE2 sets the HostNetwork value to false explicitly for the PSP it creates. When reviewing PSPs, note that the Kubernetes API only displays this field if it is explicitly set to true. No manual remediation is needed. 5.2.5 \u00b6 Minimize the admission of containers with allowPrivilegeEscalation (Automated) Rationale A container running with the `allowPrivilegeEscalation` flag set to true may have processes that can gain more privileges than their parent. There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to allow privilege escalation. The option exists (and is defaulted to true) to permit setuid binaries to run. If you have need to run containers which use setuid binaries or require privilege escalation, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp -o json | jq .items [] | jq -r 'select((.spec.allowPrivilegeEscalation == null) or (.spec.allowPrivilegeEscalation == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}' Verify that the returned count is 1. Remediation: RKE2 sets the allowPrivilegeEscalation value to false explicitly for the PSP it creates. No manual remediation is needed. 5.2.6 \u00b6 Minimize the admission of root containers (Automated) Rationale Containers may run as any Linux user. Containers which run as the root user, whilst constrained by Container Runtime security features still have a escalated likelihood of container breakout. Ideally, all containers should run as a defined non-UID 0 user. There should be at least one PodSecurityPolicy (PSP) defined which does not permit root users in a container. If you need to run root containers, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp -o json | jq .items [] | jq -r 'select((.spec.allowPrivilegeEscalation == null) or (.spec.allowPrivilegeEscalation == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}' Verify that the returned count is 1. Remediation: RKE2 sets the runAsUser.Rule value to MustRunAsNonRoot in the PodSecurityPolicy that it creates. No manual remediation is needed. 5.2.7 \u00b6 Minimize the admission of containers with the NET_RAW capability (Manual) Rationale Containers run with a default set of capabilities as assigned by the Container Runtime. By default this can include potentially dangerous capabilities. With Docker as the container runtime the NET_RAW capability is enabled which may be misused by malicious containers. Ideally, all containers should drop this capability. There should be at least one PodSecurityPolicy (PSP) defined which prevents containers with the NET_RAW capability from launching. If you need to run containers with this capability, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp global-restricted-psp -o json | jq .spec.requiredDropCapabilities [] Verify the value is \"ALL\" . Remediation: RKE2 sets .spec.requiredDropCapabilities[] to a value of All . No manual remediation needed. 5.2.8 \u00b6 Minimize the admission of containers with added capabilities (Manual) Rationale Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities outside this set can be added to containers which could expose them to risks of container breakout attacks. There should be at least one PodSecurityPolicy (PSP) defined which prevents containers with capabilities beyond the default set from launching. If you need to run containers with additional capabilities, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Manual Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp Verify that there are no PSPs present which have allowedCapabilities set to anything other than an empty array. Remediation: When run with the --profile=cis-1.6 argument RKE2 applies a PodSecurityPolicy that sets requiredDropCapabilities to ALL . No manual remediation needed. 5.2.9 \u00b6 Minimize the admission of containers with capabilities assigned (Manual) Rationale Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities are parts of the rights generally granted on a Linux system to the root user. In many cases applications running in containers do not require any capabilities to operate, so from the perspective of the principal of least privilege use of capabilities should be minimized. Result: Manual Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp Remediation: When run with the --profile=cis-1.6 argument RKE2 applies a PodSecurityPolicy that sets requiredDropCapabilities to ALL . No manual remediation needed. 5.3 Network Policies and CNI \u00b6 5.3.1 \u00b6 Ensure that the CNI in use supports Network Policies (Automated) Rationale Kubernetes network policies are enforced by the CNI plugin in use. As such it is important to ensure that the CNI plugin supports both Ingress and Egress network policies. Result: Pass Audit: Review the documentation of CNI plugin in use by the cluster, and confirm that it supports Ingress and Egress network policies. Remediation: By default, RKE2 use Canal (Calico and Flannel) and fully supports network policies. 5.3.2 \u00b6 Ensure that all Namespaces have Network Policies defined (Automated) Rationale Running different applications on the same Kubernetes cluster creates a risk of one compromised application attacking a neighboring application. Network segmentation is important to ensure that containers can communicate only with those they are supposed to. A network policy is a specification of how selections of pods are allowed to communicate with each other and other network endpoints. Network Policies are namespace scoped. When a network policy is introduced to a given namespace, all traffic not allowed by the policy is denied. However, if there are no network policies in a namespace all traffic will be allowed into and out of the pods in that namespace. Result: Pass Audit: Run the below command on the master node. for i in kube-system kube-public default ; do /var/lib/rancher/rke2/bin/kubectl get networkpolicies -n $i ; done Verify that there are network policies applied to each of the namespaces. Remediation: RKE2, when executed with the --profile=cis-1.6 argument applies a secure network policy that only allows intra-namespace traffic and DNS to kube-system. No manual remediation needed. 5.4 Secrets Management \u00b6 5.4.1 \u00b6 Prefer using secrets as files over secrets as environment variables (Manual) Rationale It is reasonably common for application code to log out its environment (particularly in the event of an error). This will include any secret values passed in as environment variables, so secrets can easily be exposed to any user or entity who has access to the logs. Result: Manual Audit: Run the following command to find references to objects which use environment variables defined from secrets. /var/lib/rancher/rke2/bin/kubectl get all -o jsonpath = '{range .items[?(@..secretKeyRef)]} {.kind} {.metadata.name} {\"\\n\"}{end}' -A Remediation: If possible, rewrite application code to read secrets from mounted secret files, rather than from environment variables. 5.4.2 \u00b6 Consider external secret storage (Manual) Rationale Kubernetes supports secrets as first-class objects, but care needs to be taken to ensure that access to secrets is carefully limited. Using an external secrets provider can ease the management of access to secrets, especially where secrests are used across both Kubernetes and non-Kubernetes environments. Result: Manual Audit: Review your secrets management implementation. Remediation: Refer to the secrets management options offered by your cloud provider or a third-party secrets management solution. 5.5 Extensible Admission Control \u00b6 5.5.1 \u00b6 Configure Image Provenance using ImagePolicyWebhook admission controller (Manual) Rationale Kubernetes supports plugging in provenance rules to accept or reject the images in your deployments. You could configure such rules to ensure that only approved images are deployed in the cluster. Result: Manual Audit: Review the pod definitions in your cluster and verify that image provenance is configured as appropriate. Remediation: Follow the Kubernetes documentation and setup image provenance. 5.6 Omitted \u00b6 The v1.6.1 guide skips 5.6 and goes from 5.5 to 5.7. We are including it here merely for explanation. 5.7 General Policies \u00b6 These policies relate to general cluster management topics, like namespace best practices and policies applied to pod objects in the cluster. 5.7.1 \u00b6 Create administrative boundaries between resources using namespaces (Manual) Rationale Limiting the scope of user permissions can reduce the impact of mistakes or malicious activities. A Kubernetes namespace allows you to partition created resources into logically named groups. Resources created in one namespace can be hidden from other namespaces. By default, each resource created by a user in Kubernetes cluster runs in a default namespace, called default. You can create additional namespaces and attach resources and users to them. You can use Kubernetes Authorization plugins to create policies that segregate access to namespace resources between different users. Result: Manual Audit: Run the below command and review the namespaces created in the cluster. /var/lib/rancher/rke2/bin/kubectl get namespaces Ensure that these namespaces are the ones you need and are adequately administered as per your requirements. Remediation: Follow the documentation and create namespaces for objects in your deployment as you need them. 5.7.2 \u00b6 Ensure that the seccomp profile is set to docker/default in your pod definitions (Manual) Rationale Seccomp (secure computing mode) is used to restrict the set of system calls applications can make, allowing cluster administrators greater control over the security of workloads running in the cluster. Kubernetes disables seccomp profiles by default for historical reasons. You should enable it to ensure that the workloads have restricted actions available within the container. Result: Manual Audit: Review the pod definitions in your cluster. It should create a line as below: annotations : seccomp.security.alpha.kubernetes.io/pod : docker/default Remediation: Review the Kubernetes documentation and if needed, apply a relevant PodSecurityPolicy. 5.7.3 \u00b6 Apply Security Context to Your Pods and Containers (Manual) Rationale A security context defines the operating system security settings (uid, gid, capabilities, SELinux role, etc..) applied to a container. When designing your containers and pods, make sure that you configure the security context for your pods, containers, and volumes. A security context is a property defined in the deployment yaml. It controls the security parameters that will be assigned to the pod/container/volume. There are two levels of security context: pod level security context, and container level security context. Result: Manual Audit: Review the pod definitions in your cluster and verify that you have security contexts defined as appropriate. Remediation: Follow the Kubernetes documentation and apply security contexts to your pods. For a suggested list of security contexts, you may refer to the CIS Security Benchmark. 5.7.4 \u00b6 The default namespace should not be used (Manual) Rationale Resources in a Kubernetes cluster should be segregated by namespace, to allow for security controls to be applied at that level and to make it easier to manage resources. Result: Manual Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get all -n default Verify that there are no resources applied to the default namespace. Remediation: By default, RKE2 does not utilize the default namespace.","title":"CIS v1.6 Self-Assessment Guide"},{"location":"security/cis_self_assessment16/#cis-kubernetes-benchmark-v16-rke2","text":"","title":"CIS Kubernetes Benchmark v1.6 - RKE2"},{"location":"security/cis_self_assessment16/#overview","text":"This document is a companion to the RKE2 security hardening guide. The hardening guide provides prescriptive guidance for hardening a production installation of RKE2, and this benchmark guide is meant to help you evaluate the level of security of the hardened cluster against each control in the CIS Kubernetes benchmark. It is to be used by RKE2 operators, security teams, auditors, and decision makers. This guide is specific to the v1.21 and v1.22 release line of RKE2 and the v1.6.1 release of the CIS Kubernetes Benchmark. For more details about each control, including detailed descriptions and remediations for failing tests, you can refer to the corresponding section of the CIS Kubernetes Benchmark v1.6.1. You can download the benchmark after logging in to CISecurity.org .","title":"Overview"},{"location":"security/cis_self_assessment16/#testing-controls-methodology","text":"Each control in the CIS Kubernetes Benchmark was evaluated against an RKE2 cluster that was configured according to the accompanying hardening guide. Where control audits differ from the original CIS benchmark, the audit commands specific to RKE2 are provided for testing. These are the possible results for each control: Pass - The RKE2 cluster under test passed the audit outlined in the benchmark. Not Applicable - The control is not applicable to RKE2 because of how it is designed to operate. The remediation section will explain why this is so. Manual - Operator Dependent - The control is Manual in the CIS benchmark and it depends on the cluster's use case or some other factor that must be determined by the cluster operator. These controls have been evaluated to ensure RKE2 does not prevent their implementation, but no further configuration or auditing of the cluster under test has been performed.","title":"Testing controls methodology"},{"location":"security/cis_self_assessment16/#controls","text":"","title":"Controls"},{"location":"security/cis_self_assessment16/#1-master-node-security-configuration","text":"","title":"1 Master Node Security Configuration"},{"location":"security/cis_self_assessment16/#11-master-node-configuration-files","text":"","title":"1.1 Master Node Configuration Files"},{"location":"security/cis_self_assessment16/#111","text":"Ensure that the API server pod specification file permissions are set to 644 or more restrictive (Automated) Rationale The API server pod specification file controls various parameters that set the behavior of the API server. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/agent/pod-manifests/kube-apiserver.yaml 644 Remediation: By default, RKE2 creates these files with 644 permissions. No manual remediation needed.","title":"1.1.1"},{"location":"security/cis_self_assessment16/#112","text":"Ensure that the API server pod specification file ownership is set to root:root (Automated) Rationale The API server pod specification file controls various parameters that set the behavior of the API server. You should set its file ownership to maintain the integrity of the file. The file should be owned by `root:root`. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/agent/pod-manifests/kube-apiserver.yaml root:root Remediation: By default, RKE2 creates these files with root:root ownership. No manual remediation needed.","title":"1.1.2"},{"location":"security/cis_self_assessment16/#113","text":"Ensure that the controller manager pod specification file permissions are set to 644 or more restrictive (Automated) Rationale The controller manager pod specification file controls various parameters that set the behavior of the Controller Manager on the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/agent/pod-manifests/kube-controller-manager.yaml 644 Remediation: By default, RKE2 creates these files with 644 permissions. No manual remediation needed.","title":"1.1.3"},{"location":"security/cis_self_assessment16/#114","text":"Ensure that the controller manager pod specification file ownership is set to root:root (Automated) Rationale The controller manager pod specification file controls various parameters that set the behavior of various components of the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/agent/pod-manifests/kube-controller-manager.yaml root:root Remediation: By default, RKE2 creates these files with root:root ownership. No manual remediation needed.","title":"1.1.4"},{"location":"security/cis_self_assessment16/#115","text":"Ensure that the scheduler pod specification file permissions are set to 644 or more restrictive (Automated) Rationale The scheduler pod specification file controls various parameters that set the behavior of the Scheduler service in the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/agent/pod-manifests/kube-scheduler.yaml 644 Remediation: By default, RKE2 creates these files with 644 permissions. No manual remediation needed.","title":"1.1.5"},{"location":"security/cis_self_assessment16/#116","text":"Ensure that the scheduler pod specification file ownership is set to root:root (Automated) Rationale The scheduler pod specification file controls various parameters that set the behavior of the kube-scheduler service in the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/agent/pod-manifests/kube-scheduler.yaml root:root Remediation: By default, RKE2 creates these files with root:root ownership. No manual remediation needed.","title":"1.1.6"},{"location":"security/cis_self_assessment16/#117","text":"Ensure that the etcd pod specification file permissions are set to 644 or more restrictive (Automated) Rationale The etcd pod specification file /var/lib/rancher/rke2/agent/pod-manifests/etcd.yaml controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly-available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/agent/pod-manifests/etcd.yaml 644 Remediation: By default, RKE2 creates these files with 644 permissions. No manual remediation needed.","title":"1.1.7"},{"location":"security/cis_self_assessment16/#118","text":"Ensure that the etcd pod specification file ownership is set to root:root (Automated) Rationale The etcd pod specification file /var/lib/rancher/rke2/agent/pod-manifests/etcd.yaml controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly-available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/agent/pod-manifests/etcd.yaml root:root Remediation: By default, RKE2 creates these files with root:root ownership. No manual remediation needed.","title":"1.1.8"},{"location":"security/cis_self_assessment16/#119","text":"Ensure that the Container Network Interface file permissions are set to 644 or more restrictive (Manual) Rationale Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be writable by only the administrators on the system. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/server/manifests/rke2-canal.yml 644 Remediation: RKE2 deploys the default CNI, Canal, using a Helm chart. The chart is defined as a custom resource in a file with 644 permissions. No manual remediation needed.","title":"1.1.9"},{"location":"security/cis_self_assessment16/#1110","text":"Ensure that the Container Network Interface file ownership is set to root:root (Manual) Rationale Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be owned by root:root. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/manifests/rke2-canal.yml root:root Remediation: RKE2 deploys the default CNI, Canal, using a Helm chart. The chart is defined as a custom resource in a file with root:root ownership. No manual remediation needed.","title":"1.1.10"},{"location":"security/cis_self_assessment16/#1111","text":"Ensure that the etcd data directory permissions are set to 700 or more restrictive (Automated) Rationale etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should not be readable or writable by any group members or the world. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/server/db/etcd 700 Remediation: RKE2 manages the etcd data directory and sets its permissions to 700. No manual remediation needed.","title":"1.1.11"},{"location":"security/cis_self_assessment16/#1112","text":"Ensure that the etcd data directory ownership is set to etcd:etcd (Automated) Rationale etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should be owned by etcd:etcd. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/db/etcd etcd:etcd Remediation: When running RKE2 with the profile flag set to cis-1.6 , RKE2 will refuse to start if the etcd user and group doesn't exist on the host. If it does exist, RKE2 will automatically set the ownership of the etcd data directory to etcd:etcd and ensure the etcd static pod is started with that user and group.","title":"1.1.12"},{"location":"security/cis_self_assessment16/#1113","text":"Ensure that the admin.conf file permissions are set to 644 or more restrictive (Automated) Rationale The admin.conf is the administrator kubeconfig file defining various settings for the administration of the cluster. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. In RKE2, this file is located at `/var/lib/rancher/rke2/server/cred/admin.kubeconfig`. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/server/cred/admin.kubeconfig 644 Remediation: By default, RKE2 creates this file at /var/lib/rancher/rke2/server/cred/admin.kubeconfig and automatically sets its permissions to 644 . No manual remediation needed.","title":"1.1.13"},{"location":"security/cis_self_assessment16/#1114","text":"Ensure that the admin.conf file ownership is set to root:root (Automated) Rationale The admin.conf file contains the admin credentials for the cluster. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. In RKE2, this file is located at `/var/lib/rancher/rke2/server/cred/admin.kubeconfig`. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/cred/admin.kubeconfig root:root Remediation: By default, RKE2 creates this file at stat -c %U:%G /var/lib/rancher/rke2/server/cred/admin.kubeconfig and automatically sets its ownership to root:root .","title":"1.1.14"},{"location":"security/cis_self_assessment16/#1115","text":"Ensure that the scheduler.conf file permissions are set to 644 or more restrictive (Automated) Rationale The scheduler.conf file is the kubeconfig file for the Scheduler. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. In RKE2, this file is located at `/var/lib/rancher/rke2/server/cred/scheduler.kubeconfig`. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig 644 Remediation: By default, RKE2 creates this file at /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig and automatically sets its permissions to 644 . No manual remediation needed.","title":"1.1.15"},{"location":"security/cis_self_assessment16/#1116","text":"Ensure that the scheduler.conf file ownership is set to root:root (Automated) Rationale The scheduler.conf file is the kubeconfig file for the Scheduler. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. In RKE2, this file is located at `/var/lib/rancher/rke2/server/cred/scheduler.kubeconfig`. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig root:root Remediation: By default, RKE2 creates this file at /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig and automatically sets its ownership to root:root .","title":"1.1.16"},{"location":"security/cis_self_assessment16/#1117","text":"Ensure that the controller.kubeconfig file permissions are set to 644 or more restrictive (Automated) Rationale The controller.kubeconfig file is the kubeconfig file for the Scheduler. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. In RKE2, this file is located at `/var/lib/rancher/rke2/server/cred/controller.kubeconfig`. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/server/cred/controller.kubeconfig 644 Remediation: By default, RKE2 creates this file at /var/lib/rancher/rke2/server/cred/controller.kubeconfig and automatically sets its permissions to 644 . No manual remediation needed.","title":"1.1.17"},{"location":"security/cis_self_assessment16/#1118","text":"Ensure that the controller.kubeconfig file ownership is set to root:root (Automated) Rationale The controller.kubeconfig file is the kubeconfig file for the Scheduler. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. In RKE2, this file is located at `/var/lib/rancher/rke2/server/cred/controller.kubeconfig`. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/cred/controller.kubeconfig root:root Remediation: By default, RKE2 creates this file at /var/lib/rancher/rke2/server/cred/controller.kubeconfig and automatically sets its ownership to root:root .","title":"1.1.18"},{"location":"security/cis_self_assessment16/#1119","text":"Ensure that the Kubernetes PKI directory and file ownership is set to root:root (Automated) Rationale Kubernetes makes use of a number of certificates as part of its operation. You should set the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity. The directory and files should be owned by root:root. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/tls root:root Remediation: By default, RKE2 creates the directory and files with the expected ownership of root:root . No manual remediation should be necessary.","title":"1.1.19"},{"location":"security/cis_self_assessment16/#1120","text":"Ensure that the Kubernetes PKI certificate file permissions are set to 644 or more restrictive (Automated) Rationale Kubernetes makes use of a number of certificate files as part of the operation of its components. The permissions on these files should be set to 644 or more restrictive to protect their integrity. Result: Pass Audit: Run the below command on the master node. stat -c %n \\ %a /var/lib/rancher/rke2/server/tls/*.crt Verify that the permissions are 644 or more restrictive. Remediation: By default, RKE2 creates the files with the expected permissions of 644 . No manual remediation is needed.","title":"1.1.20"},{"location":"security/cis_self_assessment16/#1121","text":"Ensure that the Kubernetes PKI key file permissions are set to 600 (Automated) Rationale Kubernetes makes use of a number of key files as part of the operation of its components. The permissions on these files should be set to 600 to protect their integrity and confidentiality. Result: Pass Audit: Run the below command on the master node. stat -c %n \\ %a /var/lib/rancher/rke2/server/tls/*.key Verify that the permissions are 600 or more restrictive. Remediation: By default, RKE2 creates the files with the expected permissions of 600 . No manual remediation is needed.","title":"1.1.21"},{"location":"security/cis_self_assessment16/#12-api-server","text":"This section contains recommendations relating to API server configuration flags","title":"1.2 API Server"},{"location":"security/cis_self_assessment16/#121","text":"Ensure that the --anonymous-auth argument is set to false (Manual) Rationale When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the API server. You should rely on authentication to authorize access and disallow anonymous requests. If you are using RBAC authorization, it is generally considered reasonable to allow anonymous access to the API Server for health checks and discovery purposes, and hence this recommendation is Manual. However, you should consider whether anonymous discovery is an acceptable risk for your purposes. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that --anonymous-auth=false is present. Remediation: By default, RKE2 kube-apiserver is configured to run with this flag and value. No manual remediation is needed.","title":"1.2.1"},{"location":"security/cis_self_assessment16/#122","text":"Ensure that the --basic-auth-file argument is not set (Automated) Rationale Basic authentication uses plaintext credentials for authentication. Currently, the basic authentication credentials last indefinitely, and the password cannot be changed without restarting the API server. The basic authentication is currently supported for convenience. Hence, basic authentication should not be used. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --basic-auth-file argument does not exist. Remediation: By default, RKE2 does not run with basic authentication enabled. No manual remediation is needed.","title":"1.2.2"},{"location":"security/cis_self_assessment16/#123","text":"Ensure that the --token-auth-file parameter is not set (Automated) Rationale The token-based authentication utilizes static tokens to authenticate requests to the apiserver. The tokens are stored in clear-text in a file on the apiserver, and cannot be revoked or rotated without restarting the apiserver. Hence, do not use static token-based authentication. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --token-auth-file argument does not exist. Remediation: By default, RKE2 does not run with basic authentication enabled. No manual remediation is needed.","title":"1.2.3"},{"location":"security/cis_self_assessment16/#124","text":"Ensure that the --kubelet-https argument is set to true (Automated) Rationale Connections from apiserver to kubelets could potentially carry sensitive data such as secrets and keys. It is thus important to use in-transit encryption for any communication between the apiserver and kubelets. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --kubelet-https argument does not exist. Remediation: By default, RKE2 kube-apiserver doesn't run with the --kubelet-https parameter as it runs with TLS. No manual remediation is needed.","title":"1.2.4"},{"location":"security/cis_self_assessment16/#125","text":"Ensure that the --kubelet-client-certificate and --kubelet-client-key arguments are set as appropriate (Automated) Rationale The apiserver, by default, does not authenticate itself to the kubelet's HTTPS endpoints. The requests from the apiserver are treated anonymously. You should set up certificate- based kubelet authentication to ensure that the apiserver authenticates itself to kubelets when submitting requests. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --kubelet-client-certificate and --kubelet-client-key arguments exist and they are set as appropriate. Remediation: By default, RKE2 kube-apiserver is ran with these arguments for secure communication with kubelet. No manual remediation is needed.","title":"1.2.5"},{"location":"security/cis_self_assessment16/#126","text":"Ensure that the --kubelet-certificate-authority argument is set as appropriate (Automated) Rationale The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet\u2019s port-forwarding functionality. These connections terminate at the kubelet\u2019s HTTPS endpoint. By default, the apiserver does not verify the kubelet\u2019s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --kubelet-certificate-authority argument exists and is set as appropriate. Remediation: By default, RKE2 kube-apiserver is ran with this argument for secure communication with kubelet. No manual remediation is needed.","title":"1.2.6"},{"location":"security/cis_self_assessment16/#127","text":"Ensure that the --authorization-mode argument is not set to AlwaysAllow (Automated) Rationale The API Server, can be configured to allow all requests. This mode should not be used on any production cluster. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the argument value doesn't contain AlwaysAllow . Remediation: By default, RKE2 sets Node,RBAC as the parameter to the --authorization-mode argument. No manual remediation is needed.","title":"1.2.7"},{"location":"security/cis_self_assessment16/#128","text":"Ensure that the --authorization-mode argument includes Node (Automated) Rationale The Node authorization mode only allows kubelets to read Secret, ConfigMap, PersistentVolume, and PersistentVolumeClaim objects associated with their nodes. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify Node exists as a parameter to the argument. Remediation: By default, RKE2 sets Node,RBAC as the parameter to the --authorization-mode argument. No manual remediation is needed.","title":"1.2.8"},{"location":"security/cis_self_assessment16/#129","text":"Ensure that the --authorization-mode argument includes RBAC (Automated) Rationale Role Based Access Control (RBAC) allows fine-grained control over the operations that different entities can perform on different objects in the cluster. It is recommended to use the RBAC authorization mode. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify RBAC exists as a parameter to the argument. Remediation: By default, RKE2 sets Node,RBAC as the parameter to the --authorization-mode argument. No manual remediation is needed.","title":"1.2.9"},{"location":"security/cis_self_assessment16/#1210","text":"Ensure that the admission control plugin EventRateLimit is set (Manual) Rationale Using `EventRateLimit` admission control enforces a limit on the number of events that the API Server will accept in a given time slice. A misbehaving workload could overwhelm and DoS the API Server, making it unavailable. This particularly applies to a multi-tenant cluster, where there might be a small percentage of misbehaving tenants which could have a significant impact on the performance of the cluster overall. Hence, it is recommended to limit the rate of events that the API server will accept. Note: This is an Alpha feature in the Kubernetes 1.15 release. Result: Manual - Operator Dependent Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --enable-admission-plugins argument is set to a value that includes EventRateLimit. Remediation: By default, RKE2 only sets NodeRestriction,PodSecurityPolicy as the parameter to the --enable-admission-plugins argument. To configure this, follow the Kubernetes documentation and set the desired limits in a configuration file. Then refer to RKE2's documentation to see how to supply additional api server configuration via the kube-apiserver-arg parameter.","title":"1.2.10"},{"location":"security/cis_self_assessment16/#1211","text":"Ensure that the admission control plugin AlwaysAdmit is not set (Automated) Rationale Setting admission control plugin AlwaysAdmit allows all requests and do not filter any requests. The AlwaysAdmit admission controller was deprecated in Kubernetes v1.13. Its behavior was equivalent to turning off all admission controllers. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that if the --enable-admission-plugins argument is set, its value does not include AlwaysAdmit . Remediation: By default, RKE2 only sets NodeRestriction,PodSecurityPolicy as the parameter to the --enable-admission-plugins argument. No manual remediation needed.","title":"1.2.11"},{"location":"security/cis_self_assessment16/#1212","text":"Ensure that the admission control plugin AlwaysPullImages is set (Manual) Rationale Setting admission control policy to `AlwaysPullImages` forces every new pod to pull the required images every time. In a multi-tenant cluster users can be assured that their private images can only be used by those who have the credentials to pull them. Without this admission control policy, once an image has been pulled to a node, any pod from any user can use it simply by knowing the image\u2019s name, without any authorization check against the image ownership. When this plug-in is enabled, images are always pulled prior to starting containers, which means valid credentials are required. Result: Manual - Operator Dependent Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --enable-admission-plugins argument is set to a value that includes AlwaysPullImages . Remediation: By default, RKE2 only sets NodeRestriction,PodSecurityPolicy as the parameter to the --enable-admission-plugins argument. To configure this, follow the Kubernetes documentation and set the desired limits in a configuration file. Then refer to RKE2's documentation to see how to supply additional api server configuration via the kube-apiserver-arg parameter.","title":"1.2.12"},{"location":"security/cis_self_assessment16/#1213","text":"Ensure that the admission control plugin SecurityContextDeny is set if PodSecurityPolicy is not used (Manual) Rationale SecurityContextDeny can be used to provide a layer of security for clusters which do not have PodSecurityPolicies enabled. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --enable-admission-plugins argument is set to a value that includes SecurityContextDeny , if PodSecurityPolicy is not included. Remediation: By default, RKE2 automatically enables the PodSecurityPolicy admission plugin. Therefore, the SecurityContextDeny plugin need not be enabled. No manual remediation needed.","title":"1.2.13"},{"location":"security/cis_self_assessment16/#1214","text":"Ensure that the admission control plugin ServiceAccount is set (Automated) Rationale When you create a pod, if you do not specify a service account, it is automatically assigned the `default` service account in the same namespace. You should create your own service account and let the API server manage its security tokens. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --disable-admission-plugins argument is set to a value that does not includes ServiceAccount . Remediation: By default, RKE2 does not use this argument. If there's a desire to use this argument, follow the documentation and create ServiceAccount objects as per your environment. Then refer to RKE2's documentation to see how to supply additional api server configuration via the kube-apiserver-arg parameter.","title":"1.2.14"},{"location":"security/cis_self_assessment16/#1215","text":"Ensure that the admission control plugin NamespaceLifecycle is set (Automated) Rationale Setting admission control policy to `NamespaceLifecycle` ensures that objects cannot be created in non-existent namespaces, and that namespaces undergoing termination are not used for creating the new objects. This is recommended to enforce the integrity of the namespace termination process and also for the availability of the newer objects. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --disable-admission-plugins argument is set to a value that does not include NamespaceLifecycle . Remediation: By default, RKE2 does not use this argument. No manual remediation needed.","title":"1.2.15"},{"location":"security/cis_self_assessment16/#1216","text":"Ensure that the admission control plugin PodSecurityPolicy is set (Automated) Rationale A Pod Security Policy is a cluster-level resource that controls the actions that a pod can perform and what it has the ability to access. The `PodSecurityPolicy` objects define a set of conditions that a pod must run with in order to be accepted into the system. Pod Security Policies are comprised of settings and strategies that control the security features a pod has access to and hence this must be used to control pod access permissions. **Note:** When the PodSecurityPolicy admission plugin is in use, there needs to be at least one PodSecurityPolicy in place for ANY pods to be admitted. See section 1.7 for recommendations on PodSecurityPolicy settings. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --enable-admission-plugins argument is set to a value that includes PodSecurityPolicy . Remediation: By default, RKE2 only sets NodeRestriction,PodSecurityPolicy as the parameter to the --enable-admission-plugins argument. No manual remediation needed.","title":"1.2.16"},{"location":"security/cis_self_assessment16/#1217","text":"Ensure that the admission control plugin NodeRestriction is set (Automated) Rationale Using the `NodeRestriction` plug-in ensures that the kubelet is restricted to the `Node` and `Pod` objects that it could modify as defined. Such kubelets will only be allowed to modify their own `Node` API object, and only modify `Pod` API objects that are bound to their node. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Remediation: By default, RKE2 only sets NodeRestriction,PodSecurityPolicy as the parameter to the --enable-admission-plugins argument. No manual remediation needed.","title":"1.2.17"},{"location":"security/cis_self_assessment16/#1218","text":"Ensure that the --insecure-bind-address argument is not set (Automated) Rationale If you bind the apiserver to an insecure address, basically anyone who could connect to it over the insecure port, would have unauthenticated and unencrypted access to your master node. The apiserver doesn't do any authentication checking for insecure binds and traffic to the Insecure API port is not encrpyted, allowing attackers to potentially read sensitive data in transit. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --insecure-bind-address argument does not exist. Remediation: By default, RKE2 explicitly excludes the use of the --insecure-bind-address parameter. No manual remediation is needed.","title":"1.2.18"},{"location":"security/cis_self_assessment16/#1219","text":"Ensure that the --insecure-port argument is set to 0 (Automated) Rationale Setting up the apiserver to serve on an insecure port would allow unauthenticated and unencrypted access to your master node. This would allow attackers who could access this port, to easily take control of the cluster. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --insecure-port argument is set to 0. Remediation: By default, RKE2 starts the kube-apiserver process with this argument's parameter set to 0. No manual remediation is needed.","title":"1.2.19"},{"location":"security/cis_self_assessment16/#1220","text":"Ensure that the --secure-port argument is not set to 0 (Automated) Rationale The secure port is used to serve https with authentication and authorization. If you disable it, no https traffic is served and all traffic is served unencrypted. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --secure-port argument is either not set or is set to an integer value between 1 and 65535. Remediation: By default, RKE2 sets the parameter of 6443 for the --secure-port argument. No manual remediation is needed.","title":"1.2.20"},{"location":"security/cis_self_assessment16/#1221","text":"Ensure that the --profiling argument is set to false (Automated) Rationale Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --profiling argument is set to false. Remediation: By default, RKE2 sets the --profiling flag parameter to false. No manual remediation needed.","title":"1.2.21"},{"location":"security/cis_self_assessment16/#1222","text":"Ensure that the --audit-log-path argument is set (Automated) Rationale Auditing the Kubernetes API Server provides a security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators or other components of the system. Even though currently, Kubernetes provides only basic audit capabilities, it should be enabled. You can enable it by setting an appropriate audit log path. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --audit-log-path argument is set as appropriate. Remediation: By default, RKE2 sets the --audit-log-path argument and parameter. No manual remediation needed.","title":"1.2.22"},{"location":"security/cis_self_assessment16/#1223","text":"Ensure that the --audit-log-maxage argument is set to 30 or as appropriate (Automated) Rationale Retaining logs for at least 30 days ensures that you can go back in time and investigate or correlate any events. Set your audit log retention period to 30 days or as per your business requirements. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --audit-log-maxage argument is set to 30 or as appropriate. Remediation: By default, RKE2 sets the --audit-log-maxage argument parameter to 30. No manual remediation needed.","title":"1.2.23"},{"location":"security/cis_self_assessment16/#1224","text":"Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate (Automated) Rationale Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. For example, if you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --audit-log-maxbackup argument is set to 10 or as appropriate. Remediation: By default, RKE2 sets the --audit-log-maxbackup argument parameter to 10. No manual remediation needed.","title":"1.2.24"},{"location":"security/cis_self_assessment16/#1225","text":"Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate (Automated) Rationale Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. If you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --audit-log-maxsize argument is set to 100 or as appropriate. Remediation: By default, RKE2 sets the --audit-log-maxsize argument parameter to 100. No manual remediation needed.","title":"1.2.25"},{"location":"security/cis_self_assessment16/#1226","text":"Ensure that the --request-timeout argument is set as appropriate (Automated) Rationale Setting global request timeout allows extending the API server request timeout limit to a duration appropriate to the user's connection speed. By default, it is set to 60 seconds which might be problematic on slower connections making cluster resources inaccessible once the data volume for requests exceeds what can be transmitted in 60 seconds. But, setting this timeout limit to be too large can exhaust the API server resources making it prone to Denial-of-Service attack. Hence, it is recommended to set this limit as appropriate and change the default limit of 60 seconds only if needed. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --request-timeout argument is either not set or set to an appropriate value. Remediation: By default, RKE2 does not set the --request-timeout argument. No manual remediation needed.","title":"1.2.26"},{"location":"security/cis_self_assessment16/#1227","text":"Ensure that the --service-account-lookup argument is set to true (Automated) Rationale If `--service-account-lookup` is not enabled, the apiserver only verifies that the authentication token is valid, and does not validate that the service account token mentioned in the request is actually present in etcd. This allows using a service account token even after the corresponding service account is deleted. This is an example of time of check to time of use security issue. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that if the --service-account-lookup argument exists it is set to true. Remediation: By default, RKE2 doesn't set this argument in favor of taking the default effect. No manual remediation needed.","title":"1.2.27"},{"location":"security/cis_self_assessment16/#1228","text":"Ensure that the --service-account-key-file argument is set as appropriate (Automated) Rationale By default, if no `--service-account-key-file` is specified to the apiserver, it uses the private key from the TLS serving certificate to verify service account tokens. To ensure that the keys for service account tokens could be rotated as needed, a separate public/private key pair should be used for signing service account tokens. Hence, the public key should be specified to the apiserver with `--service-account-key-file`. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --service-account-key-file argument exists and is set as appropriate. Remediation: By default, RKE2 sets the --service-account-key-file explicitly. No manual remediation needed.","title":"1.2.28"},{"location":"security/cis_self_assessment16/#1229","text":"Ensure that the --etcd-certfile and --etcd-keyfile arguments are set as appropriate (Automated) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a client certificate and key. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --etcd-certfile and --etcd-keyfile arguments exist and they are set as appropriate. Remediation: By default, RKE2 sets the --etcd-certfile and --etcd-keyfile arguments explicitly. No manual remediation needed.","title":"1.2.29"},{"location":"security/cis_self_assessment16/#1230","text":"Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Automated) Rationale API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --tls-cert-file and --tls-private-key-file arguments exist and they are set as appropriate. Remediation: By default, RKE2 sets the --tls-cert-file and --tls-private-key-file arguments explicitly. No manual remediation needed.","title":"1.2.30"},{"location":"security/cis_self_assessment16/#1231","text":"Ensure that the --client-ca-file argument is set as appropriate (Automated) Rationale API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. If `--client-ca-file` argument is set, any request presenting a client certificate signed by one of the authorities in the `client-ca-file` is authenticated with an identity corresponding to the CommonName of the client certificate. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --client-ca-file argument exists and it is set as appropriate. Remediation: By default, RKE2 sets the --client-ca-file argument explicitly. No manual remediation needed.","title":"1.2.31"},{"location":"security/cis_self_assessment16/#1232","text":"Ensure that the --etcd-cafile argument is set as appropriate (Automated) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a SSL Certificate Authority file. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --etcd-cafile argument exists and it is set as appropriate. Remediation: By default, RKE2 sets the --etcd-cafile argument explicitly. No manual remediation needed.","title":"1.2.32"},{"location":"security/cis_self_assessment16/#1233","text":"Ensure that the --encryption-provider-config argument is set as appropriate (Automated) Rationale etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted at rest to avoid any disclosures. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --encryption-provider-config argument is set to a EncryptionConfigfile. Additionally, ensure that the EncryptionConfigfile has all the desired resources covered especially any secrets. Remediation: By default, RKE2 sets the --encryption-provider-config argument explicitly. No manual remediation needed. RKE2's default encryption provider config file is located at /var/lib/rancher/rke2/server/cred/encryption-config.json and is configured to encrypt secrets.","title":"1.2.33"},{"location":"security/cis_self_assessment16/#1234","text":"Ensure that encryption providers are appropriately configured (Automated) Rationale Where `etcd` encryption is used, it is important to ensure that the appropriate set of encryption providers is used. Currently, the `aescbc`, `kms` and `secretbox` are likely to be appropriate options. Result: Pass Remediation: Follow the Kubernetes documentation and configure a EncryptionConfig file. In this file, choose aescbc , kms or secretbox as the encryption provider. Audit: Run the below command on the master node. grep aescbc /var/lib/rancher/rke2/server/cred/encryption-config.json Run the below command on the master node. Verify that aescbc is set as the encryption provider for all the desired resources. Remediation By default, RKE2 sets the argument --encryption-provider-config and parameter. The contents of the config file indicates the use of aescbc. No manual remediation needed.","title":"1.2.34"},{"location":"security/cis_self_assessment16/#1235","text":"Ensure that the API Server only makes use of Strong Cryptographic Ciphers (Manual) Rationale TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided. Result: Manual - Operator Dependent Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --tls-cipher-suites argument is set as outlined in the remediation procedure below. Remediation: By default, RKE2 explicitly doesn't set this flag. No manual remediation needed.","title":"1.2.35"},{"location":"security/cis_self_assessment16/#13-controller-manager","text":"","title":"1.3 Controller Manager"},{"location":"security/cis_self_assessment16/#131","text":"Ensure that the --terminated-pod-gc-threshold argument is set as appropriate (Manual) Rationale Garbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. The current setting for garbage collection is 12,500 terminated pods which might be too high for your system to sustain. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection. Result: Manual - Operator Dependent Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --terminated-pod-gc-threshold argument is set as appropriate. Remediation: By default, RKE2 sets the --terminated-pod-gc-threshold argument with a value of 1000. No manual remediation needed.","title":"1.3.1"},{"location":"security/cis_self_assessment16/#132","text":"Ensure that the --profiling argument is set to false (Automated) Rationale Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --profiling argument is set to false. Remediation: By default, RKE2 sets the --profiling flag parameter to false. No manual remediation needed.","title":"1.3.2"},{"location":"security/cis_self_assessment16/#133","text":"Ensure that the --use-service-account-credentials argument is set to true (Automated) Rationale The controller manager creates a service account per controller in the `kube-system` namespace, generates a credential for it, and builds a dedicated API client with that service account credential for each controller loop to use. Setting the `--use-service-account-credentials` to `true` runs each control loop within the controller manager using a separate service account credential. When used in combination with RBAC, this ensures that the control loops run with the minimum permissions required to perform their intended tasks. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --use-service-account-credentials argument is set to true. Remediation: By default, RKE2 sets the --use-service-account-credentials argument to true. No manual remediation needed.","title":"1.3.3"},{"location":"security/cis_self_assessment16/#134","text":"Ensure that the --service-account-private-key-file argument is set as appropriate (Automated) Rationale To ensure that keys for service account tokens can be rotated as needed, a separate public/private key pair should be used for signing service account tokens. The private key should be specified to the controller manager with `--service-account-private-key-file` as appropriate. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --service-account-private-key-file argument is set as appropriate. Remediation: By default, RKE2 sets the --service-account-private-key-file argument with the service account key file. No manual remediation needed.","title":"1.3.4"},{"location":"security/cis_self_assessment16/#135","text":"Ensure that the --root-ca-file argument is set as appropriate (Automated) Rationale Processes running within pods that need to contact the API server must verify the API server's serving certificate. Failing to do so could be a subject to man-in-the-middle attacks. Providing the root certificate for the API server's serving certificate to the controller manager with the `--root-ca-file` argument allows the controller manager to inject the trusted bundle into pods so that they can verify TLS connections to the API server. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --root-ca-file argument exists and is set to a certificate bundle file containing the root certificate for the API server's serving certificate Remediation: By default, RKE2 sets the --root-ca-file argument with the root ca file. No manual remediation needed.","title":"1.3.5"},{"location":"security/cis_self_assessment16/#136","text":"Ensure that the RotateKubeletServerCertificate argument is set to true (Automated) Rationale `RotateKubeletServerCertificate` causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Result: Not Applicable Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that RotateKubeletServerCertificateargument exists and is set to true. Remediation: By default, RKE2 implements it's own logic for certificate generation and rotation.","title":"1.3.6"},{"location":"security/cis_self_assessment16/#137","text":"Ensure that the --bind-address argument is set to 127.0.0.1 (Automated) Rationale The Controller Manager API service which runs on port 10252/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --bind-address argument is set to 127.0.0.1. Remediation: By default, RKE2 sets the --bind-address argument to 127.0.0.1 . No manual remediation needed.","title":"1.3.7"},{"location":"security/cis_self_assessment16/#14-scheduler","text":"This section contains recommendations relating to Scheduler configuration flags","title":"1.4 Scheduler"},{"location":"security/cis_self_assessment16/#141","text":"Ensure that the --profiling argument is set to false (Automated) Rationale Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-scheduler | grep -v grep Verify that the --profiling argument is set to false. Remediation: By default, RKE2 sets the --profiling flag parameter to false. No manual remediation needed.","title":"1.4.1"},{"location":"security/cis_self_assessment16/#142","text":"Ensure that the --bind-address argument is set to 127.0.0.1 (Automated) Rationale The Scheduler API service which runs on port 10251/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-scheduler | grep -v grep Verify that the --bind-address argument is set to 127.0.0.1. Remediation: By default, RKE2 sets the --bind-address argument to 127.0.0.1 . No manual remediation needed.","title":"1.4.2"},{"location":"security/cis_self_assessment16/#2-etcd-node-configuration","text":"This section covers recommendations for etcd configuration.","title":"2 Etcd Node Configuration"},{"location":"security/cis_self_assessment16/#21","text":"Ensure that the cert-file and key-file fields are set as appropriate (Automated) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit. Result: Not Applicable Audit: Run the below command on the master node. grep -E 'cert-file|key-file' /var/lib/rancher/rke2/server/db/etcd/config Verify that the cert-file and the key-file fields are set as appropriate. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . Server and peer cert and key files are specified. No manual remediation needed.","title":"2.1"},{"location":"security/cis_self_assessment16/#22","text":"Ensure that the client-cert-auth field is set to true (Automated) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service. Result: Not Applicable Audit: Run the below command on the master node. grep 'client-cert-auth' /var/lib/rancher/rke2/server/db/etcd/config Verify that the client-cert-auth field is set to true. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . client-cert-auth is set to true. No manual remediation needed.","title":"2.2"},{"location":"security/cis_self_assessment16/#23","text":"Ensure that the auto-tls field is not set to true (Automated) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service. Result: Pass Audit: Run the below command on the master node. grep 'auto-tls' /var/lib/rancher/rke2/server/db/etcd/config Verify that if the auto-tls field does not exist. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . Within the file, it does not contain the auto-tls argument. No manual remediation needed.","title":"2.3"},{"location":"security/cis_self_assessment16/#24","text":"Ensure that the peer-cert-file and peer-key-file fields are set as appropriate (Automated) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit and also amongst peers in the etcd clusters. Result: Not Applicable Audit: Run the below command on the master node. grep -E 'peer-server-client.crt|peer-server-client.key' /var/lib/rancher/rke2/server/db/etcd/config Verify that the peer-server-client.crt and peer-server-client.key fields are set as appropriate. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . Within the file, the peer-server-client.crt and peer-server-client.key fields are set. No manual remediation needed.","title":"2.4"},{"location":"security/cis_self_assessment16/#25","text":"Ensure that the client-cert-auth field is set to true (Automated) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Result: Not Applicable Audit: Run the below command on the master node. grep 'client-cert-auth' /var/lib/rancher/rke2/server/db/etcd/config Verify that the client-cert-auth field in the peer section is set to true. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . Within the file, the client-cert-auth field is set. No manual remediation needed.","title":"2.5"},{"location":"security/cis_self_assessment16/#26","text":"Ensure that the peer-auto-tls field is not set to true (Automated) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Hence, do not use self- signed certificates for authentication. Result: Pass Audit: Run the below command on the master node. grep 'peer-auto-tls' /var/lib/rancher/rke2/server/db/etcd/config Verify that if the peer-auto-tls field does not exist. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . Within the file, it does not contain the peer-auto-tls field. No manual remediation needed.","title":"2.6"},{"location":"security/cis_self_assessment16/#27","text":"Ensure that a unique Certificate Authority is used for etcd (Manual) Rationale etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. Its access should be restricted to specifically designated clients and peers only. Authentication to etcd is based on whether the certificate presented was issued by a trusted certificate authority. There is no checking of certificate attributes such as common name or subject alternative name. As such, if any attackers were able to gain access to any certificate issued by the trusted certificate authority, they would be able to gain full access to the etcd database. Result: Pass Audit: Run the below command on the master node. # To find the ca file used by etcd: grep 'trusted-ca-file' /var/lib/rancher/rke2/server/db/etcd/config # To find the kube-apiserver process: /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the file referenced by the client-ca-file flag in the apiserver process is different from the file referenced by the trusted-ca-file parameter in the etcd configuration file. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config and the trusted-ca-file parameters in it are set to unique values specific to etcd. No manual remediation needed.","title":"2.7"},{"location":"security/cis_self_assessment16/#3-control-plane-configuration","text":"","title":"3 Control Plane Configuration"},{"location":"security/cis_self_assessment16/#31-authentication-and-authorization","text":"","title":"3.1 Authentication and Authorization"},{"location":"security/cis_self_assessment16/#311","text":"Client certificate authentication should not be used for users (Manual) Rationale With any authentication mechanism the ability to revoke credentials if they are compromised or no longer required, is a key control. Kubernetes client certificate authentication does not allow for this due to a lack of support for certificate revocation. Result: Manual - Operator Dependent Audit: Review user access to the cluster and ensure that users are not making use of Kubernetes client certificate authentication. Remediation: Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of client certificates.","title":"3.1.1"},{"location":"security/cis_self_assessment16/#32-logging","text":"","title":"3.2 Logging"},{"location":"security/cis_self_assessment16/#321","text":"Ensure that a minimal audit policy is created (Automated) Rationale Logging is an important detective control for all systems, to detect potential unauthorised access. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --audit-policy-file is set. Review the contents of the file specified and ensure that it contains a valid audit policy. Remediation: Create an audit policy file for your cluster.","title":"3.2.1"},{"location":"security/cis_self_assessment16/#322","text":"Ensure that the audit policy covers key security concerns (Manual) Rationale Security audit logs should cover access and modification of key resources in the cluster, to enable them to form an effective part of a security environment. Result: Manual - Operator Dependent Remediation:","title":"3.2.2"},{"location":"security/cis_self_assessment16/#4-worker-node-security-configuration","text":"","title":"4 Worker Node Security Configuration"},{"location":"security/cis_self_assessment16/#41-worker-node-configuration-files","text":"","title":"4.1 Worker Node Configuration Files"},{"location":"security/cis_self_assessment16/#411","text":"Ensure that the kubelet service file permissions are set to 644 or more restrictive (Automated) Rationale The `kubelet` service file controls various parameters that set the behavior of the kubelet service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Not Applicable Remediation: RKE2 doesn\u2019t launch the kubelet as a service. It is launched and managed by the RKE2 supervisor process. All configuration is passed to it as command line arguments at run time.","title":"4.1.1"},{"location":"security/cis_self_assessment16/#412","text":"Ensure that the kubelet service file ownership is set to root:root (Automated) Rationale The `kubelet` service file controls various parameters that set the behavior of the kubelet service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by `root:root`. Result: Not Applicable Remediation: RKE2 doesn\u2019t launch the kubelet as a service. It is launched and managed by the RKE2 supervisor process. All configuration is passed to it as command line arguments at run time.","title":"4.1.2"},{"location":"security/cis_self_assessment16/#413","text":"Ensure that the proxy kubeconfig file permissions are set to 644 or more restrictive (Manual) Rationale The `kube-proxy` kubeconfig file controls various parameters of the `kube-proxy` service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. It is possible to run `kube-proxy` with the kubeconfig parameters configured as a Kubernetes ConfigMap instead of a file. In this case, there is no proxy kubeconfig file. Result: Pass Audit: Run the below command on the worker node. stat -c %a /var/lib/rancher/rke2/server/manifests/rke2-kube-proxy.yaml 644 Verify that if a file is specified and it exists, the permissions are 644 or more restrictive. Remediation: By derfault, RKE2 creates rke2-kube-proxy.yaml with 644 permissions. No manual remediation needed.","title":"4.1.3"},{"location":"security/cis_self_assessment16/#414","text":"Ensure that the proxy kubeconfig file ownership is set to root:root (Manual) Rationale The kubeconfig file for `kube-proxy` controls various parameters for the `kube-proxy` service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by `root:root`. Result: Pass Audit: Run the below command on the master node. stat -c %U:%G /var/lib/rancher/rke2/server/manifests/rke2-kube-proxy.yaml root:root Verify that if a file is specified and it exists, the permissions are 644 or more restrictive. Remediation: By default, RKE2 creates rke2-kube-proxy.yaml with root:root ownership. No manual remediation needed.","title":"4.1.4"},{"location":"security/cis_self_assessment16/#415","text":"Ensure that the kubelet.conf file permissions are set to 644 or more restrictive (Automated) Rationale The `kubelet.conf` file is the kubeconfig file for the node, and controls various parameters that set the behavior and identity of the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Not Applicable Audit: Run the below command on the worker node. stat -c %a /var/lib/rancher/rke2/agent/kubelet.kubeconfig 644 Remediation: By derfault, RKE2 creates kubelet.kubeconfig with 644 permissions. No manual remediation needed.","title":"4.1.5"},{"location":"security/cis_self_assessment16/#416","text":"Ensure that the kubelet.conf file ownership is set to root:root (Manual) Rationale The `kubelet.conf` file is the kubeconfig file for the node, and controls various parameters that set the behavior and identity of the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by `root:root`. Result: Not Applicable Audit: Run the below command on the master node. stat -c %U:%G /var/lib/rancher/rke2/agent/kubelet.kubeconfig root:root Remediation: By default, RKE2 creates kubelet.kubeconfig with root:root ownership. No manual remediation needed.","title":"4.1.6"},{"location":"security/cis_self_assessment16/#417","text":"Ensure that the certificate authorities file permissions are set to 644 or more restrictive (Manual) Rationale The certificate authorities file controls the authorities used to validate API requests. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Manual - Operator Dependent Audit: Run the below command on the master node. stat -c %a /var/lib/rancher/rke2/server/tls/server-ca.crt 644 Verify that the permissions are 644. Remediation: By default, RKE2 creates /var/lib/rancher/rke2/server/tls/server-ca.crt with 644 permissions.","title":"4.1.7"},{"location":"security/cis_self_assessment16/#418","text":"Ensure that the client certificate authorities file ownership is set to root:root (Automated) Rationale The certificate authorities file controls the authorities used to validate API requests. You should set its file ownership to maintain the integrity of the file. The file should be owned by `root:root`. Result: Pass Audit: Run the below command on the master node. stat -c %U:%G /var/lib/rancher/rke2/server/tls/client-ca.crt root:root Remediation: By default, RKE2 creates /var/lib/rancher/rke2/server/tls/client-ca.crt with root:root ownership.","title":"4.1.8"},{"location":"security/cis_self_assessment16/#419","text":"Ensure that the kubelet configuration file has permissions set to 644 or more restrictive (Automated) Rationale The kubelet reads various parameters, including security settings, from a config file specified by the `--config` argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Not Applicable Remediation: RKE2 doesn\u2019t require or maintain a configuration file for the kubelet process. All configuration is passed to it as command line arguments at run time.","title":"4.1.9"},{"location":"security/cis_self_assessment16/#4110","text":"Ensure that the kubelet configuration file ownership is set to root:root (Automated) Rationale The kubelet reads various parameters, including security settings, from a config file specified by the `--config` argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be owned by `root:root`. Result: Not Applicable Remediation: RKE2 doesn\u2019t require or maintain a configuration file for the kubelet process. All configuration is passed to it as command line arguments at run time.","title":"4.1.10"},{"location":"security/cis_self_assessment16/#42-kubelet","text":"This section contains recommendations for kubelet configuration.","title":"4.2 Kubelet"},{"location":"security/cis_self_assessment16/#421","text":"Ensure that the --anonymous-auth argument is set to false (Automated) Rationale When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the Kubelet server. You should rely on authentication to authorize access and disallow anonymous requests. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify that the value for --anonymous-auth is false. Remediation: By default, RKE2 starts kubelet with --anonymous-auth set to false. No manual remediation needed.","title":"4.2.1"},{"location":"security/cis_self_assessment16/#422","text":"Ensure that the --authorization-mode argument is not set to AlwaysAllow (Automated) Rationale Kubelets, by default, allow all authenticated requests (even anonymous ones) without needing explicit authorization checks from the apiserver. You should restrict this behavior and only allow explicitly authorized requests. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify that AlwaysAllow is not present. Remediation: RKE2 starts kubelet with Webhook as the value for the --authorization-mode argument. No manual remediation needed.","title":"4.2.2"},{"location":"security/cis_self_assessment16/#423","text":"Ensure that the --client-ca-file argument is set as appropriate (Automated) Rationale The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet\u2019s port-forwarding functionality. These connections terminate at the kubelet\u2019s HTTPS endpoint. By default, the apiserver does not verify the kubelet\u2019s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Enabling Kubelet certificate authentication ensures that the apiserver could authenticate the Kubelet before submitting any requests. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify that the --client-ca-file argument has a ca file associated. Remediation: By default, RKE2 starts the kubelet process with the --client-ca-file . No manual remediation needed.","title":"4.2.3"},{"location":"security/cis_self_assessment16/#424","text":"Ensure that the --read-only-port argument is set to 0 (Automated) Rationale The Kubelet process provides a read-only API in addition to the main Kubelet API. Unauthenticated access is provided to this read-only API which could possibly retrieve potentially sensitive information about the cluster. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify that the --read-only-port argument is set to 0. Remediation: By default, RKE2 starts the kubelet process with the --read-only-port argument set to 0.","title":"4.2.4"},{"location":"security/cis_self_assessment16/#425","text":"Ensure that the --streaming-connection-idle-timeout argument is not set to 0 (Automated) Rationale Setting idle timeouts ensures that you are protected against Denial-of-Service attacks, inactive connections and running out of ephemeral ports. **Note:** By default, `--streaming-connection-idle-timeout` is set to 4 hours which might be too high for your environment. Setting this as appropriate would additionally ensure that such streaming connections are timed out after serving legitimate use cases. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify that there's nothing returned. Remediation: By default, RKE2 does not set --streaming-connection-idle-timeout when starting kubelet.","title":"4.2.5"},{"location":"security/cis_self_assessment16/#426","text":"Ensure that the --protect-kernel-defaults argument is set to true (Automated) Rationale Kernel parameters are usually tuned and hardened by the system administrators before putting the systems into production. These parameters protect the kernel and the system. Your kubelet kernel defaults that rely on such parameters should be appropriately set to match the desired secured system state. Ignoring this could potentially lead to running pods with undesired kernel behavior. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Remediation: When running with the profile flag set to cis-1.6 , RKE2 starts the kubelet process with the --protect-kernel-defaults argument set to true.","title":"4.2.6"},{"location":"security/cis_self_assessment16/#427","text":"Ensure that the --make-iptables-util-chains argument is set to true (Automated) Rationale Kubelets can automatically manage the required changes to iptables based on how you choose your networking options for the pods. It is recommended to let kubelets manage the changes to iptables. This ensures that the iptables configuration remains in sync with pods networking configuration. Manually configuring iptables with dynamic pod network configuration changes might hamper the communication between pods/containers and to the outside world. You might have iptables rules too restrictive or too open. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify there are no results returned. Remediation: By default, RKE2 does not set the --make-iptables-util-chains argument. No manual remediation needed.","title":"4.2.7"},{"location":"security/cis_self_assessment16/#428","text":"Ensure that the --hostname-override argument is not set (Manual) Rationale Overriding hostnames could potentially break TLS setup between the kubelet and the apiserver. Additionally, with overridden hostnames, it becomes increasingly difficult to associate logs with a particular node and process them for security analytics. Hence, you should setup your kubelet nodes with resolvable FQDNs and avoid overriding the hostnames with IPs. Result: Not Applicable Remediation: RKE2 does set this parameter for each host, but RKE2 also manages all certificates in the cluster. It ensures the hostname-override is included as a subject alternative name (SAN) in the kubelet's certificate.","title":"4.2.8"},{"location":"security/cis_self_assessment16/#429","text":"Ensure that the --event-qps argument is set to 0 or a level which ensures appropriate event capture (Manual) Rationale It is important to capture all events and not restrict event creation. Events are an important source of security information and analytics that ensure that your environment is consistently monitored using the event data. Result: Manual - Operator Dependent Remediation: See CIS Benchmark guide for further details on configuring this.","title":"4.2.9"},{"location":"security/cis_self_assessment16/#4210","text":"Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Automated) Rationale Kubelet communication contains sensitive parameters that should remain encrypted in transit. Configure the Kubelets to serve only HTTPS traffic. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify the --tls-cert-file and --tls-private-key-file arguments are present and set appropriately. Remediation: By default, RKE2 sets the --tls-cert-file and --tls-private-key-file arguments when executing the kubelet process.","title":"4.2.10"},{"location":"security/cis_self_assessment16/#4211","text":"Ensure that the --rotate-certificates argument is not set to false (Manual) Rationale The `--rotate-certificates` setting causes the kubelet to rotate its client certificates by creating new CSRs as its existing credentials expire. This automated periodic rotation ensures that the there is no downtime due to expired certificates and thus addressing availability in the CIA security triad. **Note:** This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. **Note:**This feature also require the `RotateKubeletClientCertificate` feature gate to be enabled (which is the default since Kubernetes v1.7) Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Remediation: By default, RKE2 implements it's own logic for certificate generation and rotation.","title":"4.2.11"},{"location":"security/cis_self_assessment16/#4212","text":"Ensure that the RotateKubeletServerCertificate argument is set to true (Manual) Rationale `RotateKubeletServerCertificate` causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Remediation: By default, RKE2 implements it's own logic for certificate generation and rotation.","title":"4.2.12"},{"location":"security/cis_self_assessment16/#4213","text":"Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers (Manual) Rationale TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided. Result: Manual - Operator Dependent Remediation: Configuration of the parameter is dependent on your use case. Please see the CIS Kubernetes Benchmark for suggestions on configuring this for your usecase.","title":"4.2.13"},{"location":"security/cis_self_assessment16/#5-kubernetes-policies","text":"","title":"5 Kubernetes Policies"},{"location":"security/cis_self_assessment16/#51-rbac-and-service-accounts","text":"","title":"5.1 RBAC and Service Accounts"},{"location":"security/cis_self_assessment16/#511","text":"Ensure that the cluster-admin role is only used where required (Manual) Rationale Kubernetes provides a set of default roles where RBAC is used. Some of these roles such as `cluster-admin` provide wide-ranging privileges which should only be applied where absolutely necessary. Roles such as `cluster-admin` allow super-user access to perform any action on any resource. When used in a `ClusterRoleBinding`, it gives full control over every resource in the cluster and in all namespaces. When used in a `RoleBinding`, it gives full control over every resource in the rolebinding's namespace, including the namespace itself. Result: Pass Remediation: RKE2 does not make inappropriate use of the cluster-admin role. Operators must audit their workloads of additional usage. See the CIS Benchmark guide for more details.","title":"5.1.1"},{"location":"security/cis_self_assessment16/#512","text":"Minimize access to secrets (Manual) Rationale Inappropriate access to secrets stored within the Kubernetes cluster can allow for an attacker to gain additional access to the Kubernetes cluster or external resources whose credentials are stored as secrets. Result: Manual - Operator Dependent Remediation: RKE2 limits its use of secrets for the system components appropriately, but operators must audit the use of secrets by their workloads. See the CIS Benchmark guide for more details.","title":"5.1.2"},{"location":"security/cis_self_assessment16/#513","text":"Minimize wildcard use in Roles and ClusterRoles (Manual) Rationale The principle of least privilege recommends that users are provided only the access required for their role and nothing more. The use of wildcard rights grants is likely to provide excessive rights to the Kubernetes API. Result: Manual - Operator Dependent Audit: Run the below command on the master node. # Retrieve the roles defined across each namespaces in the cluster and review for wildcards /var/lib/rancher/rke2/bin/kubectl get roles --all-namespaces -o yaml # Retrieve the cluster roles defined in the cluster and review for wildcards /var/lib/rancher/rke2/bin/kubectl get clusterroles -o yaml Verify that there are not wildcards in use. Remediation: Operators should review their workloads for proper role usage. See the CIS Benchmark guide for more details.","title":"5.1.3"},{"location":"security/cis_self_assessment16/#514","text":"Minimize access to create pods (Manual) Rationale The ability to create pods in a cluster opens up possibilities for privilege escalation and should be restricted, where possible. Result: Manual - Operator Dependent Remediation: Operators should review who has access to create pods in their cluster. See the CIS Benchmark guide for more details.","title":"5.1.4"},{"location":"security/cis_self_assessment16/#515","text":"Ensure that default service accounts are not actively used. (Automated) Rationale Kubernetes provides a default service account which is used by cluster workloads where no specific service account is assigned to the pod. Where access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account. The default service account should be configured such that it does not provide a service account token and does not have any explicit rights assignments. Result: Pass. Audit: For each namespace in the cluster, review the rights assigned to the default service account and ensure that it has no roles or cluster roles bound to it apart from the defaults. Additionally ensure that the automountServiceAccountToken: false setting is in place for each default service account. Remediation: Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server. Modify the configuration of each default service account to include this value automountServiceAccountToken: false","title":"5.1.5"},{"location":"security/cis_self_assessment16/#516","text":"Ensure that Service Account Tokens are only mounted where necessary (Manual) Rationale Mounting service account tokens inside pods can provide an avenue for privilege escalation attacks where an attacker is able to compromise a single pod in the cluster. Avoiding mounting these tokens removes this attack avenue. Result: Manual - Operator Dependent Remediation: The pods launched by RKE2 are part of the control plane and generally need access to communicate with the API server, thus this control does not apply to them. Operators should review their workloads and take steps to modify the definition of pods and service accounts which do not need to mount service account tokens to disable it.","title":"5.1.6"},{"location":"security/cis_self_assessment16/#52-pod-security-policies","text":"","title":"5.2 Pod Security Policies"},{"location":"security/cis_self_assessment16/#521","text":"Minimize the admission of containers wishing to share the host process ID namespace (Automated) Rationale Privileged containers have access to all Linux Kernel capabilities and devices. A container running with full privileges can do almost everything that the host can do. This flag exists to allow special use-cases, like manipulating the network stack and accessing devices. There should be at least one PodSecurityPolicy (PSP) defined which does not permit privileged containers. If you need to run privileged containers, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl describe psp global-restricted-psp | grep MustRunAsNonRoot Verify that the result is Rule: MustRunAsNonRoot . Remediation: RKE2, when run with the --profile=cis-1.6 argument, will disallow the use of privileged containers.","title":"5.2.1"},{"location":"security/cis_self_assessment16/#522","text":"Minimize the admission of containers wishing to share the host process ID namespace (Automated) Rationale A container running in the host's PID namespace can inspect processes running outside the container. If the container also has access to ptrace capabilities this can be used to escalate privileges outside of the container. There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to share the host PID namespace. If you need to run containers which require hostPID, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp -o json | jq .items [] | jq -r 'select((.spec.hostPID == null) or (.spec.hostPID == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}' Verify that the returned count is 1. Remediation: RKE2 sets the hostPID value to false explicitly for the PSP it creates. When reviewing PSPs, note that the Kubernetes API only displays this field if it is explicitly set to true. No manual remediation is needed.","title":"5.2.2"},{"location":"security/cis_self_assessment16/#523","text":"Minimize the admission of containers wishing to share the host IPC namespace (Automated) Rationale A container running in the host's IPC namespace can use IPC to interact with processes outside the container. There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to share the host IPC namespace. If you have a requirement to containers which require hostIPC, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp -o json | jq .items [] | jq -r 'select((.spec.hostIPC == null) or (.spec.hostIPC == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}' Verify that the returned count is 1. Remediation: RKE2 sets the HostIPC value to false explicitly for the PSP it creates. When reviewing PSPs, note that the Kubernetes API only displays this field if it is explicitly set to true. No manual remediation is needed.","title":"5.2.3"},{"location":"security/cis_self_assessment16/#524","text":"Minimize the admission of containers wishing to share the host network namespace (Automated) Rationale A container running in the host's network namespace could access the local loopback device, and could access network traffic to and from other pods. There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to share the host network namespace. If you have need to run containers which require hostNetwork, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp -o json | jq .items [] | jq -r 'select((.spec.hostNetwork == null) or (.spec.hostNetwork == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}' Verify that the returned count is 1. Remediation: RKE2 sets the HostNetwork value to false explicitly for the PSP it creates. When reviewing PSPs, note that the Kubernetes API only displays this field if it is explicitly set to true. No manual remediation is needed.","title":"5.2.4"},{"location":"security/cis_self_assessment16/#525","text":"Minimize the admission of containers with allowPrivilegeEscalation (Automated) Rationale A container running with the `allowPrivilegeEscalation` flag set to true may have processes that can gain more privileges than their parent. There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to allow privilege escalation. The option exists (and is defaulted to true) to permit setuid binaries to run. If you have need to run containers which use setuid binaries or require privilege escalation, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp -o json | jq .items [] | jq -r 'select((.spec.allowPrivilegeEscalation == null) or (.spec.allowPrivilegeEscalation == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}' Verify that the returned count is 1. Remediation: RKE2 sets the allowPrivilegeEscalation value to false explicitly for the PSP it creates. No manual remediation is needed.","title":"5.2.5"},{"location":"security/cis_self_assessment16/#526","text":"Minimize the admission of root containers (Automated) Rationale Containers may run as any Linux user. Containers which run as the root user, whilst constrained by Container Runtime security features still have a escalated likelihood of container breakout. Ideally, all containers should run as a defined non-UID 0 user. There should be at least one PodSecurityPolicy (PSP) defined which does not permit root users in a container. If you need to run root containers, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp -o json | jq .items [] | jq -r 'select((.spec.allowPrivilegeEscalation == null) or (.spec.allowPrivilegeEscalation == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}' Verify that the returned count is 1. Remediation: RKE2 sets the runAsUser.Rule value to MustRunAsNonRoot in the PodSecurityPolicy that it creates. No manual remediation is needed.","title":"5.2.6"},{"location":"security/cis_self_assessment16/#527","text":"Minimize the admission of containers with the NET_RAW capability (Manual) Rationale Containers run with a default set of capabilities as assigned by the Container Runtime. By default this can include potentially dangerous capabilities. With Docker as the container runtime the NET_RAW capability is enabled which may be misused by malicious containers. Ideally, all containers should drop this capability. There should be at least one PodSecurityPolicy (PSP) defined which prevents containers with the NET_RAW capability from launching. If you need to run containers with this capability, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp global-restricted-psp -o json | jq .spec.requiredDropCapabilities [] Verify the value is \"ALL\" . Remediation: RKE2 sets .spec.requiredDropCapabilities[] to a value of All . No manual remediation needed.","title":"5.2.7"},{"location":"security/cis_self_assessment16/#528","text":"Minimize the admission of containers with added capabilities (Manual) Rationale Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities outside this set can be added to containers which could expose them to risks of container breakout attacks. There should be at least one PodSecurityPolicy (PSP) defined which prevents containers with capabilities beyond the default set from launching. If you need to run containers with additional capabilities, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Manual Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp Verify that there are no PSPs present which have allowedCapabilities set to anything other than an empty array. Remediation: When run with the --profile=cis-1.6 argument RKE2 applies a PodSecurityPolicy that sets requiredDropCapabilities to ALL . No manual remediation needed.","title":"5.2.8"},{"location":"security/cis_self_assessment16/#529","text":"Minimize the admission of containers with capabilities assigned (Manual) Rationale Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities are parts of the rights generally granted on a Linux system to the root user. In many cases applications running in containers do not require any capabilities to operate, so from the perspective of the principal of least privilege use of capabilities should be minimized. Result: Manual Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp Remediation: When run with the --profile=cis-1.6 argument RKE2 applies a PodSecurityPolicy that sets requiredDropCapabilities to ALL . No manual remediation needed.","title":"5.2.9"},{"location":"security/cis_self_assessment16/#53-network-policies-and-cni","text":"","title":"5.3 Network Policies and CNI"},{"location":"security/cis_self_assessment16/#531","text":"Ensure that the CNI in use supports Network Policies (Automated) Rationale Kubernetes network policies are enforced by the CNI plugin in use. As such it is important to ensure that the CNI plugin supports both Ingress and Egress network policies. Result: Pass Audit: Review the documentation of CNI plugin in use by the cluster, and confirm that it supports Ingress and Egress network policies. Remediation: By default, RKE2 use Canal (Calico and Flannel) and fully supports network policies.","title":"5.3.1"},{"location":"security/cis_self_assessment16/#532","text":"Ensure that all Namespaces have Network Policies defined (Automated) Rationale Running different applications on the same Kubernetes cluster creates a risk of one compromised application attacking a neighboring application. Network segmentation is important to ensure that containers can communicate only with those they are supposed to. A network policy is a specification of how selections of pods are allowed to communicate with each other and other network endpoints. Network Policies are namespace scoped. When a network policy is introduced to a given namespace, all traffic not allowed by the policy is denied. However, if there are no network policies in a namespace all traffic will be allowed into and out of the pods in that namespace. Result: Pass Audit: Run the below command on the master node. for i in kube-system kube-public default ; do /var/lib/rancher/rke2/bin/kubectl get networkpolicies -n $i ; done Verify that there are network policies applied to each of the namespaces. Remediation: RKE2, when executed with the --profile=cis-1.6 argument applies a secure network policy that only allows intra-namespace traffic and DNS to kube-system. No manual remediation needed.","title":"5.3.2"},{"location":"security/cis_self_assessment16/#54-secrets-management","text":"","title":"5.4 Secrets Management"},{"location":"security/cis_self_assessment16/#541","text":"Prefer using secrets as files over secrets as environment variables (Manual) Rationale It is reasonably common for application code to log out its environment (particularly in the event of an error). This will include any secret values passed in as environment variables, so secrets can easily be exposed to any user or entity who has access to the logs. Result: Manual Audit: Run the following command to find references to objects which use environment variables defined from secrets. /var/lib/rancher/rke2/bin/kubectl get all -o jsonpath = '{range .items[?(@..secretKeyRef)]} {.kind} {.metadata.name} {\"\\n\"}{end}' -A Remediation: If possible, rewrite application code to read secrets from mounted secret files, rather than from environment variables.","title":"5.4.1"},{"location":"security/cis_self_assessment16/#542","text":"Consider external secret storage (Manual) Rationale Kubernetes supports secrets as first-class objects, but care needs to be taken to ensure that access to secrets is carefully limited. Using an external secrets provider can ease the management of access to secrets, especially where secrests are used across both Kubernetes and non-Kubernetes environments. Result: Manual Audit: Review your secrets management implementation. Remediation: Refer to the secrets management options offered by your cloud provider or a third-party secrets management solution.","title":"5.4.2"},{"location":"security/cis_self_assessment16/#55-extensible-admission-control","text":"","title":"5.5 Extensible Admission Control"},{"location":"security/cis_self_assessment16/#551","text":"Configure Image Provenance using ImagePolicyWebhook admission controller (Manual) Rationale Kubernetes supports plugging in provenance rules to accept or reject the images in your deployments. You could configure such rules to ensure that only approved images are deployed in the cluster. Result: Manual Audit: Review the pod definitions in your cluster and verify that image provenance is configured as appropriate. Remediation: Follow the Kubernetes documentation and setup image provenance.","title":"5.5.1"},{"location":"security/cis_self_assessment16/#56-omitted","text":"The v1.6.1 guide skips 5.6 and goes from 5.5 to 5.7. We are including it here merely for explanation.","title":"5.6 Omitted"},{"location":"security/cis_self_assessment16/#57-general-policies","text":"These policies relate to general cluster management topics, like namespace best practices and policies applied to pod objects in the cluster.","title":"5.7 General Policies"},{"location":"security/cis_self_assessment16/#571","text":"Create administrative boundaries between resources using namespaces (Manual) Rationale Limiting the scope of user permissions can reduce the impact of mistakes or malicious activities. A Kubernetes namespace allows you to partition created resources into logically named groups. Resources created in one namespace can be hidden from other namespaces. By default, each resource created by a user in Kubernetes cluster runs in a default namespace, called default. You can create additional namespaces and attach resources and users to them. You can use Kubernetes Authorization plugins to create policies that segregate access to namespace resources between different users. Result: Manual Audit: Run the below command and review the namespaces created in the cluster. /var/lib/rancher/rke2/bin/kubectl get namespaces Ensure that these namespaces are the ones you need and are adequately administered as per your requirements. Remediation: Follow the documentation and create namespaces for objects in your deployment as you need them.","title":"5.7.1"},{"location":"security/cis_self_assessment16/#572","text":"Ensure that the seccomp profile is set to docker/default in your pod definitions (Manual) Rationale Seccomp (secure computing mode) is used to restrict the set of system calls applications can make, allowing cluster administrators greater control over the security of workloads running in the cluster. Kubernetes disables seccomp profiles by default for historical reasons. You should enable it to ensure that the workloads have restricted actions available within the container. Result: Manual Audit: Review the pod definitions in your cluster. It should create a line as below: annotations : seccomp.security.alpha.kubernetes.io/pod : docker/default Remediation: Review the Kubernetes documentation and if needed, apply a relevant PodSecurityPolicy.","title":"5.7.2"},{"location":"security/cis_self_assessment16/#573","text":"Apply Security Context to Your Pods and Containers (Manual) Rationale A security context defines the operating system security settings (uid, gid, capabilities, SELinux role, etc..) applied to a container. When designing your containers and pods, make sure that you configure the security context for your pods, containers, and volumes. A security context is a property defined in the deployment yaml. It controls the security parameters that will be assigned to the pod/container/volume. There are two levels of security context: pod level security context, and container level security context. Result: Manual Audit: Review the pod definitions in your cluster and verify that you have security contexts defined as appropriate. Remediation: Follow the Kubernetes documentation and apply security contexts to your pods. For a suggested list of security contexts, you may refer to the CIS Security Benchmark.","title":"5.7.3"},{"location":"security/cis_self_assessment16/#574","text":"The default namespace should not be used (Manual) Rationale Resources in a Kubernetes cluster should be segregated by namespace, to allow for security controls to be applied at that level and to make it easier to manage resources. Result: Manual Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get all -n default Verify that there are no resources applied to the default namespace. Remediation: By default, RKE2 does not utilize the default namespace.","title":"5.7.4"},{"location":"security/fips_support/","text":"FIPS 140-2 is a U.S. Federal Government security standard used to approve cryptographic modules. This document explains how RKE2 is built with FIPS validated cryptographic libraries. Use of FIPS Compatible Go compiler. \u00b6 The Go compiler in use can be found here . Each component of the system is built with the version of this compiler that matches the same standard Go compiler version that would be used otherwise. This version of Go replaces the standard Go crypto libraries with the FIPS validated BoringCrypto module. See GoBoring's readme for more details. This module has been revalidated as the Rancher Kubernetes Cryptographic Library in order to ensure support on a wider range of systems. FIPS Support in Cluster Components \u00b6 Most of the components of the RKE2 system are statically compiled with the GoBoring Go compiler implementation. RKE2, from a component perspective, is broken up in a number of sections. The list below contains the sections and associated components. Kubernetes API Server Controller Manager Scheduler Kubelet Kube Proxy Metric Server Kubectl Helm Charts Flannel Calico CoreDNS Runtime \u00b6 To ensure that all aspects of the system architecture are using FIPS 140-2 compliant algorithm implementations, the RKE2 runtime contains utilities statically compiled with the FIPS enabled Go compiler for FIPS 140-2 compliance. This ensures that all levels of the stack are compliant from Kubernetes daemons to container orchestration mechanics. etcd containerd containerd-shim containerd-shim-runc-v1 containerd-shim-runc-v2 ctr crictl runc CNI \u00b6 As of v1.21.2, RKE2 supports selecting a different CNI via the --cni flag and comes bundled with several CNIs including Canal (default), Calico, Cilium, and Multus. Of these, only Canal (the default) is rebuilt for FIPS compliance. Ingress \u00b6 RKE2 ships with NGNIX as its default ingress provider. As of v1.21+, this component is FIPS compliant. There are two primary sub-components for NGINX ingress: controller - responsible for monitoring/updating Kubernetes resources and configuring the server accordingly server - responsible for accepting and routing traffic The controller is written in Go and as such is compiled using our FIPS compatible Go compiler . The server is written in C and also requires OpenSSL to function properly. As such, it leverages a FIPS-validated version of OpenSSL to achieve FIPS compliance.","title":"FIPS 140-2 Enablement"},{"location":"security/fips_support/#use-of-fips-compatible-go-compiler","text":"The Go compiler in use can be found here . Each component of the system is built with the version of this compiler that matches the same standard Go compiler version that would be used otherwise. This version of Go replaces the standard Go crypto libraries with the FIPS validated BoringCrypto module. See GoBoring's readme for more details. This module has been revalidated as the Rancher Kubernetes Cryptographic Library in order to ensure support on a wider range of systems.","title":"Use of FIPS Compatible Go compiler."},{"location":"security/fips_support/#fips-support-in-cluster-components","text":"Most of the components of the RKE2 system are statically compiled with the GoBoring Go compiler implementation. RKE2, from a component perspective, is broken up in a number of sections. The list below contains the sections and associated components. Kubernetes API Server Controller Manager Scheduler Kubelet Kube Proxy Metric Server Kubectl Helm Charts Flannel Calico CoreDNS","title":"FIPS Support in Cluster Components"},{"location":"security/fips_support/#runtime","text":"To ensure that all aspects of the system architecture are using FIPS 140-2 compliant algorithm implementations, the RKE2 runtime contains utilities statically compiled with the FIPS enabled Go compiler for FIPS 140-2 compliance. This ensures that all levels of the stack are compliant from Kubernetes daemons to container orchestration mechanics. etcd containerd containerd-shim containerd-shim-runc-v1 containerd-shim-runc-v2 ctr crictl runc","title":"Runtime"},{"location":"security/fips_support/#cni","text":"As of v1.21.2, RKE2 supports selecting a different CNI via the --cni flag and comes bundled with several CNIs including Canal (default), Calico, Cilium, and Multus. Of these, only Canal (the default) is rebuilt for FIPS compliance.","title":"CNI"},{"location":"security/fips_support/#ingress","text":"RKE2 ships with NGNIX as its default ingress provider. As of v1.21+, this component is FIPS compliant. There are two primary sub-components for NGINX ingress: controller - responsible for monitoring/updating Kubernetes resources and configuring the server accordingly server - responsible for accepting and routing traffic The controller is written in Go and as such is compiled using our FIPS compatible Go compiler . The server is written in C and also requires OpenSSL to function properly. As such, it leverages a FIPS-validated version of OpenSSL to achieve FIPS compliance.","title":"Ingress"},{"location":"security/hardening_guide/","text":"CIS Hardening Guide \u00b6 This document provides prescriptive guidance for hardening a production installation of RKE2. It outlines the configurations and controls required to address Kubernetes benchmark controls from the Center for Internet Security (CIS). For more details about evaluating a hardened cluster against the official CIS benchmark, refer to the CIS Benchmark Rancher Self-Assessment Guide v1.6 . RKE2 is designed to be \"hardened by default\" and pass the majority of the Kubernetes CIS controls without modification. There are a few notable exceptions to this that require manual intervention to fully pass the CIS Benchmark: RKE2 will not modify the host operating system. Therefore, you, the operator, must make a few host-level modifications. Certain CIS policy controls for PodSecurityPolicies and NetworkPolicies will restrict the functionality of the cluster. You must opt into having RKE2 configuring these out of the box. To help ensure these above requirements are met, RKE2 can be started with the profile flag set to cis-1.6 . This flag generally does two things: Checks that host-level requirements have been met. If they haven't, RKE2 will exit with a fatal error describing the unmet requirements. Configures runtime pod security policies and network policies that allow the cluster to pass associated controls. Note The profile's flag only valid values are cis-1.5 or cis-1.6 . It accepts a string value to allow for other profiles in the future. Note The self-assessment guide for CIS v1.5 ( cis-1.5 ) was removed from this documentation, since this version is applicable only to Kubernetes v1.15 that is not supported anymore. The profile, however, is still available in RKE2. The following section outlines the specific actions that are taken when the profile flag is set to cis-1.6 . Host-level requirements \u00b6 There are two areas of host-level requirements: kernel parameters and etcd process/directory configuration. These are outlined in this section. Ensure protect-kernel-defaults is set \u00b6 This is a kubelet flag that will cause the kubelet to exit if the required kernel parameters are unset or are set to values that are different from the kubelet's defaults. When the profile flag is set, RKE2 will set the flag to true . Note protect-kernel-defaults is exposed as a top-level flag for RKE2. If you have set profile to \"cis-1.x\" and protect-kernel-defaults to false explicitly, RKE2 will exit with an error. RKE2 will also check the same kernel parameters that the kubelet does and exit with an error following the same rules as the kubelet. This is done as a convenience to help the operator more quickly and easily identify what kernel parameters are violating the kubelet defaults. Ensure etcd is configured properly \u00b6 The CIS Benchmark requires that the etcd data directory be owned by the etcd user and group. This implicitly requires the etcd process to be ran as the host-level etcd user. To achieve this, RKE2 takes several steps when started with a valid \"cis-1.x\" profile: Check that the etcd user and group exists on the host. If they don't, exit with an error. Create etcd's data directory with etcd as the user and group owner. Ensure the etcd process is ran as the etcd user and group by setting the etcd static pod's SecurityContext appropriately. Setting up hosts \u00b6 This section gives you the commands necessary to configure your host to meet the above requirements. Set kernel parameters \u00b6 When RKE2 is installed, it creates a sysctl config file to set the required parameters appropriately. However, it does not automatically configures the host to use this configuration. You must do this manually. The location of the config file depends on the installation method used. If RKE2 was installed via RPM, YUM, or DNF (the default on OSes that use RPMs, such as CentOS), run the following commands: sudo cp -f /usr/share/rke2/rke2-cis-sysctl.conf /etc/sysctl.d/60-rke2-cis.conf sudo systemctl restart systemd-sysctl If RKE2 was installed via the tarball (the default on OSes that do not use RPMs, such as Ubuntu), run the following commands: sudo cp -f /usr/local/share/rke2/rke2-cis-sysctl.conf /etc/sysctl.d/60-rke2-cis.conf sudo systemctl restart systemd-sysctl If your system lacks the systemd-sysctl.service and/or the /etc/sysctl.d directory, you will want to make sure the sysctls are applied at boot by running the following command during start-up: sudo sysctl -p /usr/local/share/rke2/rke2-cis-sysctl.conf Please perform this step only on fresh installations, before actually using RKE2 to deploy Kubernetes. Many Kubernetes components, including CNI plugins, set up their own sysctls. Restarting the systemd-sysctl service on a running Kubernetes cluster can result in unexpected side-effects. Create the etcd user \u00b6 On some Linux distributions, the useradd command will not create a group. The -U flag is included below to account for that. This flag tells useradd to create a group with the same name as the user. sudo useradd -r -c \"etcd user\" -s /sbin/nologin -M etcd -U Kubernetes runtime requirements \u00b6 The runtime requirements to pass the CIS Benchmark are centered around pod security and network policies. These are outlined in this section. PodSecurityPolicies \u00b6 RKE2 always runs with the PodSecurityPolicy admission controller turned on. However, when it is not started with a valid \"cis-1.x\" profile, RKE2 will put an unrestricted policy in place that allows Kubernetes to run as though the PodSecurityPolicy admission controller was not enabled. When ran with a valid \"cis-1.x\" profile, RKE2 will put a much more restrictive set of policies in place. These policies meet the requirements outlined in section 5.2 of the CIS Benchmark. Note The Kubernetes control plane components and critical additions such as CNI, DNS, and Ingress are ran as pods in the kube-system namespace. Therefore, this namespace will have a policy that is less restrictive so that these components can run properly. NetworkPolicies \u00b6 When ran with a valid \"cis-1.x\" profile, RKE2 will put NetworkPolicies in place that passes the CIS Benchmark for Kubernetes' built-in namespaces. These namespaces are: kube-system , kube-public , kube-node-lease , and default . The NetworkPolicy used will only allow pods within the same namespace to talk to each other. The notable exception to this is that it allows DNS requests to be resolved. Note Operators must manage network policies as normal for additional namespaces that are created. Configure default service account \u00b6 Set automountServiceAccountToken to false for default service accounts Kubernetes provides a default service account which is used by cluster workloads where no specific service account is assigned to the pod. Where access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account. The default service account should be configured such that it does not provide a service account token and does not have any explicit rights assignments. For each namespace including default and kube-system on a standard RKE2 install, the default service account must include this value: automountServiceAccountToken : false For namespaces created by the cluster operator, the following script and configuration file can be used to configure the default service account. The configuration below must be saved to a file called account_update.yaml . apiVersion : v1 kind : ServiceAccount metadata : name : default automountServiceAccountToken : false Create a bash script file called account_update.sh . Be sure to sudo chmod +x account_update.sh so the script has execute permissions. #!/bin/bash -e for namespace in $( kubectl get namespaces -A -o = jsonpath = \"{.items[*]['metadata.name']}\" ) ; do echo -n \"Patching namespace $namespace - \" kubectl patch serviceaccount default -n ${ namespace } -p \" $( cat account_update.yaml ) \" done Execute this script to apply the account_update.yaml configuration to default service account in all namespaces. API Server audit configuration \u00b6 CIS requirements 1.2.22 to 1.2.25 are related to configuring audit logs for the API Server. When RKE2 is started with the profile flag set to cis-1.6 , it will automatically configure hardened --audit-log- parameters in the API Server to pass those CIS checks. RKE2's default audit policy is configured to not log requests in the API Server. This is done to allow cluster operators flexibility to customize an audit policy that suits their auditing requirements and needs, as these are specific to each users' environment and policies. A default audit policy is created by RKE2 when started with the profile flag set to cis-1.6 . The policy is defined in /etc/rancher/rke2/audit-policy.yaml . apiVersion : audit.k8s.io/v1 kind : Policy metadata : creationTimestamp : null rules : - level : None To start logging requests to the API Server, at least level parameter must be modified, for example, to Metadata . Detailed information about policy configuration for the API server can be found in the Kubernetes documentation . After adapting the audit policy, RKE2 must be restarted to load the new configuration. sudo systemctl restart rke2-server.service API Server audit logs will be written to /var/lib/rancher/rke2/server/logs/audit.log . Known issues \u00b6 The following are controls that RKE2 currently does not pass. Each gap will be explained and whether it can be passed through manual operator intervention or if it will be addressed in a future release. Control 1.1.12 \u00b6 Ensure that the etcd data directory ownership is set to etcd:etcd . Rationale etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should be owned by etcd:etcd . Remediation This can be remediated by creating an etcd user and group as described above. Control 5.1.5 \u00b6 Ensure that default service accounts are not actively used Rationale Kubernetes provides a default service account which is used by cluster workloads where no specific service account is assigned to the pod. Where access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account. The default service account should be configured such that it does not provide a service account token and does not have any explicit rights assignments. This can be remediated by updating the automountServiceAccountToken field to false for the default service account in each namespace. Remediation You can manually update this field on service accounts in your cluster to pass the control as described above. Control 5.3.2 \u00b6 Ensure that all Namespaces have Network Policies defined Rationale Running different applications on the same Kubernetes cluster creates a risk of one compromised application attacking a neighboring application. Network segmentation is important to ensure that containers can communicate only with those they are supposed to. A network policy is a specification of how selections of pods are allowed to communicate with each other and other network endpoints. Network Policies are namespace scoped. When a network policy is introduced to a given namespace, all traffic not allowed by the policy is denied. However, if there are no network policies in a namespace all traffic will be allowed into and out of the pods in that namespace. Remediation This can be remediated by setting profile: \"cis-1.6\" in RKE2 configuration file /etc/rancher/rke2/config.yaml . An example can be found below. RKE2 configuration \u00b6 Below is the minimum necessary configuration needed for hardening RKE2 to pass CIS v1.6 hardened profile rke2-cis-1.6-profile-hardened available in Rancher. secrets-encryption : \"true\" profile : \"cis-1.6\" # CIS 4.2.6, 5.2.1, 5.2.8, 5.2.9, 5.3.2 The configuration file must be named config.yaml and placed in /etc/rancher/rke2 . The directory needs to be created prior to installing RKE2. Conclusion \u00b6 If you have followed this guide, your RKE2 cluster will be configured to pass the CIS Kubernetes Benchmark. You can review our CIS Benchmark Self-Assessment Guide v1.6 to understand how we verified each of the benchmarks and how you can do the same on your cluster.","title":"CIS Hardening Guide"},{"location":"security/hardening_guide/#cis-hardening-guide","text":"This document provides prescriptive guidance for hardening a production installation of RKE2. It outlines the configurations and controls required to address Kubernetes benchmark controls from the Center for Internet Security (CIS). For more details about evaluating a hardened cluster against the official CIS benchmark, refer to the CIS Benchmark Rancher Self-Assessment Guide v1.6 . RKE2 is designed to be \"hardened by default\" and pass the majority of the Kubernetes CIS controls without modification. There are a few notable exceptions to this that require manual intervention to fully pass the CIS Benchmark: RKE2 will not modify the host operating system. Therefore, you, the operator, must make a few host-level modifications. Certain CIS policy controls for PodSecurityPolicies and NetworkPolicies will restrict the functionality of the cluster. You must opt into having RKE2 configuring these out of the box. To help ensure these above requirements are met, RKE2 can be started with the profile flag set to cis-1.6 . This flag generally does two things: Checks that host-level requirements have been met. If they haven't, RKE2 will exit with a fatal error describing the unmet requirements. Configures runtime pod security policies and network policies that allow the cluster to pass associated controls. Note The profile's flag only valid values are cis-1.5 or cis-1.6 . It accepts a string value to allow for other profiles in the future. Note The self-assessment guide for CIS v1.5 ( cis-1.5 ) was removed from this documentation, since this version is applicable only to Kubernetes v1.15 that is not supported anymore. The profile, however, is still available in RKE2. The following section outlines the specific actions that are taken when the profile flag is set to cis-1.6 .","title":"CIS Hardening Guide"},{"location":"security/hardening_guide/#host-level-requirements","text":"There are two areas of host-level requirements: kernel parameters and etcd process/directory configuration. These are outlined in this section.","title":"Host-level requirements"},{"location":"security/hardening_guide/#ensure-protect-kernel-defaults-is-set","text":"This is a kubelet flag that will cause the kubelet to exit if the required kernel parameters are unset or are set to values that are different from the kubelet's defaults. When the profile flag is set, RKE2 will set the flag to true . Note protect-kernel-defaults is exposed as a top-level flag for RKE2. If you have set profile to \"cis-1.x\" and protect-kernel-defaults to false explicitly, RKE2 will exit with an error. RKE2 will also check the same kernel parameters that the kubelet does and exit with an error following the same rules as the kubelet. This is done as a convenience to help the operator more quickly and easily identify what kernel parameters are violating the kubelet defaults.","title":"Ensure protect-kernel-defaults is set"},{"location":"security/hardening_guide/#ensure-etcd-is-configured-properly","text":"The CIS Benchmark requires that the etcd data directory be owned by the etcd user and group. This implicitly requires the etcd process to be ran as the host-level etcd user. To achieve this, RKE2 takes several steps when started with a valid \"cis-1.x\" profile: Check that the etcd user and group exists on the host. If they don't, exit with an error. Create etcd's data directory with etcd as the user and group owner. Ensure the etcd process is ran as the etcd user and group by setting the etcd static pod's SecurityContext appropriately.","title":"Ensure etcd is configured properly"},{"location":"security/hardening_guide/#setting-up-hosts","text":"This section gives you the commands necessary to configure your host to meet the above requirements.","title":"Setting up hosts"},{"location":"security/hardening_guide/#set-kernel-parameters","text":"When RKE2 is installed, it creates a sysctl config file to set the required parameters appropriately. However, it does not automatically configures the host to use this configuration. You must do this manually. The location of the config file depends on the installation method used. If RKE2 was installed via RPM, YUM, or DNF (the default on OSes that use RPMs, such as CentOS), run the following commands: sudo cp -f /usr/share/rke2/rke2-cis-sysctl.conf /etc/sysctl.d/60-rke2-cis.conf sudo systemctl restart systemd-sysctl If RKE2 was installed via the tarball (the default on OSes that do not use RPMs, such as Ubuntu), run the following commands: sudo cp -f /usr/local/share/rke2/rke2-cis-sysctl.conf /etc/sysctl.d/60-rke2-cis.conf sudo systemctl restart systemd-sysctl If your system lacks the systemd-sysctl.service and/or the /etc/sysctl.d directory, you will want to make sure the sysctls are applied at boot by running the following command during start-up: sudo sysctl -p /usr/local/share/rke2/rke2-cis-sysctl.conf Please perform this step only on fresh installations, before actually using RKE2 to deploy Kubernetes. Many Kubernetes components, including CNI plugins, set up their own sysctls. Restarting the systemd-sysctl service on a running Kubernetes cluster can result in unexpected side-effects.","title":"Set kernel parameters"},{"location":"security/hardening_guide/#create-the-etcd-user","text":"On some Linux distributions, the useradd command will not create a group. The -U flag is included below to account for that. This flag tells useradd to create a group with the same name as the user. sudo useradd -r -c \"etcd user\" -s /sbin/nologin -M etcd -U","title":"Create the etcd user"},{"location":"security/hardening_guide/#kubernetes-runtime-requirements","text":"The runtime requirements to pass the CIS Benchmark are centered around pod security and network policies. These are outlined in this section.","title":"Kubernetes runtime requirements"},{"location":"security/hardening_guide/#podsecuritypolicies","text":"RKE2 always runs with the PodSecurityPolicy admission controller turned on. However, when it is not started with a valid \"cis-1.x\" profile, RKE2 will put an unrestricted policy in place that allows Kubernetes to run as though the PodSecurityPolicy admission controller was not enabled. When ran with a valid \"cis-1.x\" profile, RKE2 will put a much more restrictive set of policies in place. These policies meet the requirements outlined in section 5.2 of the CIS Benchmark. Note The Kubernetes control plane components and critical additions such as CNI, DNS, and Ingress are ran as pods in the kube-system namespace. Therefore, this namespace will have a policy that is less restrictive so that these components can run properly.","title":"PodSecurityPolicies"},{"location":"security/hardening_guide/#networkpolicies","text":"When ran with a valid \"cis-1.x\" profile, RKE2 will put NetworkPolicies in place that passes the CIS Benchmark for Kubernetes' built-in namespaces. These namespaces are: kube-system , kube-public , kube-node-lease , and default . The NetworkPolicy used will only allow pods within the same namespace to talk to each other. The notable exception to this is that it allows DNS requests to be resolved. Note Operators must manage network policies as normal for additional namespaces that are created.","title":"NetworkPolicies"},{"location":"security/hardening_guide/#configure-default-service-account","text":"Set automountServiceAccountToken to false for default service accounts Kubernetes provides a default service account which is used by cluster workloads where no specific service account is assigned to the pod. Where access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account. The default service account should be configured such that it does not provide a service account token and does not have any explicit rights assignments. For each namespace including default and kube-system on a standard RKE2 install, the default service account must include this value: automountServiceAccountToken : false For namespaces created by the cluster operator, the following script and configuration file can be used to configure the default service account. The configuration below must be saved to a file called account_update.yaml . apiVersion : v1 kind : ServiceAccount metadata : name : default automountServiceAccountToken : false Create a bash script file called account_update.sh . Be sure to sudo chmod +x account_update.sh so the script has execute permissions. #!/bin/bash -e for namespace in $( kubectl get namespaces -A -o = jsonpath = \"{.items[*]['metadata.name']}\" ) ; do echo -n \"Patching namespace $namespace - \" kubectl patch serviceaccount default -n ${ namespace } -p \" $( cat account_update.yaml ) \" done Execute this script to apply the account_update.yaml configuration to default service account in all namespaces.","title":"Configure default service account"},{"location":"security/hardening_guide/#api-server-audit-configuration","text":"CIS requirements 1.2.22 to 1.2.25 are related to configuring audit logs for the API Server. When RKE2 is started with the profile flag set to cis-1.6 , it will automatically configure hardened --audit-log- parameters in the API Server to pass those CIS checks. RKE2's default audit policy is configured to not log requests in the API Server. This is done to allow cluster operators flexibility to customize an audit policy that suits their auditing requirements and needs, as these are specific to each users' environment and policies. A default audit policy is created by RKE2 when started with the profile flag set to cis-1.6 . The policy is defined in /etc/rancher/rke2/audit-policy.yaml . apiVersion : audit.k8s.io/v1 kind : Policy metadata : creationTimestamp : null rules : - level : None To start logging requests to the API Server, at least level parameter must be modified, for example, to Metadata . Detailed information about policy configuration for the API server can be found in the Kubernetes documentation . After adapting the audit policy, RKE2 must be restarted to load the new configuration. sudo systemctl restart rke2-server.service API Server audit logs will be written to /var/lib/rancher/rke2/server/logs/audit.log .","title":"API Server audit configuration"},{"location":"security/hardening_guide/#known-issues","text":"The following are controls that RKE2 currently does not pass. Each gap will be explained and whether it can be passed through manual operator intervention or if it will be addressed in a future release.","title":"Known issues"},{"location":"security/hardening_guide/#control-1112","text":"Ensure that the etcd data directory ownership is set to etcd:etcd . Rationale etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should be owned by etcd:etcd . Remediation This can be remediated by creating an etcd user and group as described above.","title":"Control  1.1.12"},{"location":"security/hardening_guide/#control-515","text":"Ensure that default service accounts are not actively used Rationale Kubernetes provides a default service account which is used by cluster workloads where no specific service account is assigned to the pod. Where access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account. The default service account should be configured such that it does not provide a service account token and does not have any explicit rights assignments. This can be remediated by updating the automountServiceAccountToken field to false for the default service account in each namespace. Remediation You can manually update this field on service accounts in your cluster to pass the control as described above.","title":"Control 5.1.5"},{"location":"security/hardening_guide/#control-532","text":"Ensure that all Namespaces have Network Policies defined Rationale Running different applications on the same Kubernetes cluster creates a risk of one compromised application attacking a neighboring application. Network segmentation is important to ensure that containers can communicate only with those they are supposed to. A network policy is a specification of how selections of pods are allowed to communicate with each other and other network endpoints. Network Policies are namespace scoped. When a network policy is introduced to a given namespace, all traffic not allowed by the policy is denied. However, if there are no network policies in a namespace all traffic will be allowed into and out of the pods in that namespace. Remediation This can be remediated by setting profile: \"cis-1.6\" in RKE2 configuration file /etc/rancher/rke2/config.yaml . An example can be found below.","title":"Control 5.3.2"},{"location":"security/hardening_guide/#rke2-configuration","text":"Below is the minimum necessary configuration needed for hardening RKE2 to pass CIS v1.6 hardened profile rke2-cis-1.6-profile-hardened available in Rancher. secrets-encryption : \"true\" profile : \"cis-1.6\" # CIS 4.2.6, 5.2.1, 5.2.8, 5.2.9, 5.3.2 The configuration file must be named config.yaml and placed in /etc/rancher/rke2 . The directory needs to be created prior to installing RKE2.","title":"RKE2 configuration"},{"location":"security/hardening_guide/#conclusion","text":"If you have followed this guide, your RKE2 cluster will be configured to pass the CIS Kubernetes Benchmark. You can review our CIS Benchmark Self-Assessment Guide v1.6 to understand how we verified each of the benchmarks and how you can do the same on your cluster.","title":"Conclusion"},{"location":"security/policies/","text":"This document describes how RKE2 configures PodSecurityPolicies and NetworkPolicies in order to be secure-by-default while also providing operators with maximum configuration flexibility. Pod Security Policies \u00b6 RKE2 can be ran with or without the profile: cis-1.6 configuration parameter. This will cause it to apply different PodSecurityPolicies (PSPs) at start-up. If running with the cis-1.6 profile, RKE2 will apply a restrictive policy called global-restricted-psp to all namespaces except kube-system . The kube-system namespace needs a less restrictive policy named system-unrestricted-psp in order to launch critical components. If running without the cis-1.6 profile, RKE2 will apply a completely unrestricted policy called global-unrestricted-psp , which is the equivalent of running without the PSP admission controller enabled. RKE2 will put these policies in place upon initial startup, but will not modify them after that, unless explicitly triggered by the cluster operator as described below. This is to allow the operator to fully control the PSPs without RKE2's defaults adding interference. The creation and application of the PSPs are controlled by the presence or absence of certain annotations on the kube-system namespace. These map directly to the PSPs which can be created and are: psp.rke2.io/global-restricted psp.rke2.io/system-unrestricted psp.rke2.io/global-unrestricted The following logic is performed at startup for the policies and their annotations: If the annotation exists, RKE2 continues without further action. If the annotation doesn't exist, RKE2 checks to see if the associated policy exists and if so, deletes and recreates it, along with adding the annotation to the namespace. In the case of the global-unrestricted-psp , the policy is not recreated. This is to account for moving between CIS and non-CIS modes without making the cluster less secure. At the time of creating a policy, cluster roles and cluster role bindings are also created to ensure the appropriate policies are put into use by default. So, after the initial start-up, operators can modify or delete RKE2's policies and RKE2 will respect those changes. Additionally, to \"reset\" a policy, an operator just needs to delete the associated annotation from the kube-system namespace and restart RKE2. The policies are outlined below, starting with the most restrictive global-restricted PSP. apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : name : global-restricted-psp spec : privileged : false # CIS - 5.2.1 allowPrivilegeEscalation : false # CIS - 5.2.5 requiredDropCapabilities : # CIS - 5.2.7/8/9 - ALL volumes : - 'configMap' - 'emptyDir' - 'projected' - 'secret' - 'downwardAPI' - 'persistentVolumeClaim' hostNetwork : false # CIS - 5.2.4 hostIPC : false # CIS - 5.2.3 hostPID : false # CIS - 5.2.2 runAsUser : rule : 'MustRunAsNonRoot' # CIS - 5.2.6 seLinux : rule : 'RunAsAny' supplementalGroups : rule : 'MustRunAs' ranges : - min : 1 max : 65535 fsGroup : rule : 'MustRunAs' ranges : - min : 1 max : 65535 readOnlyRootFilesystem : false If RKE2 is started in non CIS mode, annotations are checked like above however the resulting application of pod security policies is a permissive one. See below. apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : name : global-unrestricted-psp spec : privileged : true allowPrivilegeEscalation : true allowedCapabilities : - '*' volumes : - '*' hostNetwork : true hostPorts : - min : 0 max : 65535 hostIPC : true hostPID : true runAsUser : rule : 'RunAsAny' seLinux : rule : 'RunAsAny' supplementalGroups : rule : 'RunAsAny' fsGroup : rule : 'RunAsAny' In both cases, the \"system unrestricted policy\" is applied. See below. apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : name : system-unrestricted-psp spec : privileged : true allowPrivilegeEscalation : true allowedCapabilities : - '*' volumes : - '*' hostNetwork : true hostPorts : - min : 0 max : 65535 hostIPC : true hostPID : true runAsUser : rule : 'RunAsAny' seLinux : rule : 'RunAsAny' supplementalGroups : rule : 'RunAsAny' fsGroup : rule : 'RunAsAny' To view the pod security policies currently deployed on your system, run the below command: kubectl get psp -A Network Policies \u00b6 When RKE2 is run with the profile: cis-1.6 parameter, it will apply 2 network policies to the kube-system , kube-public , and default namespaces and applies associated annotations. The same logic applies to these policies and annotations as the PSPs. On start, the annotations for each namespace are checked for existence and if they exist, RKE2 takes no action. If the annotation doesn't exist, RKE2 checks to see if the policy exists and if it does, recreates it. The first policy applied is to restrict network traffic to only the namespace itself. See below. apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : managedFields : - apiVersion : networking.k8s.io/v1 fieldsType : FieldsV1 fieldsV1 : f:spec : f:ingress : {} f:policyTypes : {} name : default-network-policy namespace : default spec : ingress : - from : - podSelector : {} podSelector : {} policyTypes : - Ingress The second policy applied is to the kube-system namespace and allows for DNS traffic. See below. apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : managedFields : - apiVersion : networking.k8s.io/v1 fieldsV1 : f:spec : f:ingress : {} f:podSelector : f:matchLabels : f:policyTypes : {} name : default-network-dns-policy namespace : kube-system spec : ingress : - ports : - port : 53 protocol : TCP - port : 53 protocol : UDP podSelector : matchLabels : policyTypes : - Ingress RKE2 applies the default-network-policy policy and np.rke2.io annotation to all built-in namespaces. The kube-system namespace additionally gets the default-network-dns-policy policy and np.rke2.io/dns annotation applied to it. To view the network policies currently deployed on your system, run the below command: kubectl get networkpolicies -A","title":"Default Policy Configuration"},{"location":"security/policies/#pod-security-policies","text":"RKE2 can be ran with or without the profile: cis-1.6 configuration parameter. This will cause it to apply different PodSecurityPolicies (PSPs) at start-up. If running with the cis-1.6 profile, RKE2 will apply a restrictive policy called global-restricted-psp to all namespaces except kube-system . The kube-system namespace needs a less restrictive policy named system-unrestricted-psp in order to launch critical components. If running without the cis-1.6 profile, RKE2 will apply a completely unrestricted policy called global-unrestricted-psp , which is the equivalent of running without the PSP admission controller enabled. RKE2 will put these policies in place upon initial startup, but will not modify them after that, unless explicitly triggered by the cluster operator as described below. This is to allow the operator to fully control the PSPs without RKE2's defaults adding interference. The creation and application of the PSPs are controlled by the presence or absence of certain annotations on the kube-system namespace. These map directly to the PSPs which can be created and are: psp.rke2.io/global-restricted psp.rke2.io/system-unrestricted psp.rke2.io/global-unrestricted The following logic is performed at startup for the policies and their annotations: If the annotation exists, RKE2 continues without further action. If the annotation doesn't exist, RKE2 checks to see if the associated policy exists and if so, deletes and recreates it, along with adding the annotation to the namespace. In the case of the global-unrestricted-psp , the policy is not recreated. This is to account for moving between CIS and non-CIS modes without making the cluster less secure. At the time of creating a policy, cluster roles and cluster role bindings are also created to ensure the appropriate policies are put into use by default. So, after the initial start-up, operators can modify or delete RKE2's policies and RKE2 will respect those changes. Additionally, to \"reset\" a policy, an operator just needs to delete the associated annotation from the kube-system namespace and restart RKE2. The policies are outlined below, starting with the most restrictive global-restricted PSP. apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : name : global-restricted-psp spec : privileged : false # CIS - 5.2.1 allowPrivilegeEscalation : false # CIS - 5.2.5 requiredDropCapabilities : # CIS - 5.2.7/8/9 - ALL volumes : - 'configMap' - 'emptyDir' - 'projected' - 'secret' - 'downwardAPI' - 'persistentVolumeClaim' hostNetwork : false # CIS - 5.2.4 hostIPC : false # CIS - 5.2.3 hostPID : false # CIS - 5.2.2 runAsUser : rule : 'MustRunAsNonRoot' # CIS - 5.2.6 seLinux : rule : 'RunAsAny' supplementalGroups : rule : 'MustRunAs' ranges : - min : 1 max : 65535 fsGroup : rule : 'MustRunAs' ranges : - min : 1 max : 65535 readOnlyRootFilesystem : false If RKE2 is started in non CIS mode, annotations are checked like above however the resulting application of pod security policies is a permissive one. See below. apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : name : global-unrestricted-psp spec : privileged : true allowPrivilegeEscalation : true allowedCapabilities : - '*' volumes : - '*' hostNetwork : true hostPorts : - min : 0 max : 65535 hostIPC : true hostPID : true runAsUser : rule : 'RunAsAny' seLinux : rule : 'RunAsAny' supplementalGroups : rule : 'RunAsAny' fsGroup : rule : 'RunAsAny' In both cases, the \"system unrestricted policy\" is applied. See below. apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : name : system-unrestricted-psp spec : privileged : true allowPrivilegeEscalation : true allowedCapabilities : - '*' volumes : - '*' hostNetwork : true hostPorts : - min : 0 max : 65535 hostIPC : true hostPID : true runAsUser : rule : 'RunAsAny' seLinux : rule : 'RunAsAny' supplementalGroups : rule : 'RunAsAny' fsGroup : rule : 'RunAsAny' To view the pod security policies currently deployed on your system, run the below command: kubectl get psp -A","title":"Pod Security Policies"},{"location":"security/policies/#network-policies","text":"When RKE2 is run with the profile: cis-1.6 parameter, it will apply 2 network policies to the kube-system , kube-public , and default namespaces and applies associated annotations. The same logic applies to these policies and annotations as the PSPs. On start, the annotations for each namespace are checked for existence and if they exist, RKE2 takes no action. If the annotation doesn't exist, RKE2 checks to see if the policy exists and if it does, recreates it. The first policy applied is to restrict network traffic to only the namespace itself. See below. apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : managedFields : - apiVersion : networking.k8s.io/v1 fieldsType : FieldsV1 fieldsV1 : f:spec : f:ingress : {} f:policyTypes : {} name : default-network-policy namespace : default spec : ingress : - from : - podSelector : {} podSelector : {} policyTypes : - Ingress The second policy applied is to the kube-system namespace and allows for DNS traffic. See below. apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : managedFields : - apiVersion : networking.k8s.io/v1 fieldsV1 : f:spec : f:ingress : {} f:podSelector : f:matchLabels : f:policyTypes : {} name : default-network-dns-policy namespace : kube-system spec : ingress : - ports : - port : 53 protocol : TCP - port : 53 protocol : UDP podSelector : matchLabels : policyTypes : - Ingress RKE2 applies the default-network-policy policy and np.rke2.io annotation to all built-in namespaces. The kube-system namespace additionally gets the default-network-dns-policy policy and np.rke2.io/dns annotation applied to it. To view the network policies currently deployed on your system, run the below command: kubectl get networkpolicies -A","title":"Network Policies"},{"location":"security/secrets_encryption/","text":"Secrets Encryption Config \u00b6 RKE2 supports encrypting Secrets at rest, and will do the following automatically: Generate an AES-CBC key Generate an encryption config file with the generated key: { \"kind\" : \"EncryptionConfiguration\" , \"apiVersion\" : \"apiserver.config.k8s.io/v1\" , \"resources\" : [ { \"resources\" : [ \"secrets\" ], \"providers\" : [ { \"aescbc\" : { \"keys\" : [ { \"name\" : \"aescbckey\" , \"secret\" : \"xxxxxxxxxxxxxxxxxxx\" } ] } }, { \"identity\" : {} } ] } ] } Pass the config to the Kubernetes APIServer as encryption-provider-config Once enabled any created secret will be encrypted with this key. Note that if you disable encryption then any encrypted secrets will not be readable until you enable encryption again using the same key. Secrets Encryption Tool \u00b6 Available as of v1.21.8+rke2r1 RKE2 contains a utility subcommand secrets-encrypt , which allows administrators to perform the following tasks: Adding new encryption keys Rotating and deleting encryption keys Reencrypting secrets Warning: Failure to follow proper procedure when rotating secrets encryption keys can cause permanent data loss. Proceed with caution. Single-Server Encryption Key Rotation \u00b6 To rotate secrets encryption keys on a single-node cluster: Prepare: rke2 secrets-encrypt prepare Restart the kube-apiserver pod: # Get the kube-apiserver container ID export CONTAINER_RUNTIME_ENDPOINT=\"unix:///var/run/k3s/containerd/containerd.sock\" crictl ps --name kube-apiserver # Stop the pod crictl stop <CONTAINER_ID> Rotate: rke2 secrets-encrypt rotate Restart the kube-apiserver pod again Reencrypt: rke2 secrets-encrypt reencrypt Multi-Server Encryption Key Rotation \u00b6 To rotate secrets encryption keys on HA setups: Note: In this example, 3 servers are used to for a HA cluster, referred to as S1, S2, S3. While not required, it is recommended that you pick one server node from which to run the secrets-encrypt commands. Prepare on S1 rke2 secrets-encrypt prepare Sequentially Restart S1, S2, S3 systemctl restart rke2-server.service Wait for the systemctl command to return before restarting the next server. Rotate on S1 rke2 secrets-encrypt rotate Sequentially Restart S1, S2, S3 Reencrypt on S1 rke2 secrets-encrypt reencrypt Wait until reencryption is finished, either via server logs journalctl -u rke2-server or via rke2 secrets-encrypt status . The status will return reencrypt_finished when done. Sequentially Restart S1, S2, S3 Secrets Encryption Status \u00b6 The secrets-encrypt status subcommand displays information about the current status of secrets encryption on the node. An example of the command on a single-server node: $ rke2 secrets-encrypt status Encryption Status: Enabled Current Rotation Stage: start Server Encryption Hashes: All hashes match Active Key Type Name ------ -------- ---- * AES-CBC aescbckey Another example on HA cluster, after rotating the keys, but before restarting the servers: $ rke2 secrets-encrypt status Encryption Status: Enabled Current Rotation Stage: rotate Server Encryption Hashes: hash does not match between node-1 and node-2 Active Key Type Name ------ -------- ---- * AES-CBC aescbckey-2021-12-10T22:54:38Z AES-CBC aescbckey Details on each section are as follows: Encryption Status : Displayed whether secrets encryption is disabled or enabled on the node Current Rotation Stage : Indicates the current rotation stage on the node. Stages are: start , prepare , rotate , reencrypt_request , reencrypt_active , reencrypt_finished Server Encryption Hashes : Useful for HA clusters, this indicates whether all servers are on the same stage with their local files. This can be used to identify whether a restart of servers is required before proceeding to the next stage. In the HA example above, node-1 and node-2 have different hashes, indicating that they currently do not have the same encryption configuration. Restarting the servers will sync up their configuration. Key Table : Summarizes information about the secrets encryption keys found on the node. Active : The \"*\" indicates which, if any, of the keys are currently used for secrets encryption. An active key is used by Kubernetes to encrypt any new secrets. Key Type : RKE2 only supports the AES-CBC key type. Find more info here. Name : Name of the encryption key.","title":"Secrets Encryption"},{"location":"security/secrets_encryption/#secrets-encryption-config","text":"RKE2 supports encrypting Secrets at rest, and will do the following automatically: Generate an AES-CBC key Generate an encryption config file with the generated key: { \"kind\" : \"EncryptionConfiguration\" , \"apiVersion\" : \"apiserver.config.k8s.io/v1\" , \"resources\" : [ { \"resources\" : [ \"secrets\" ], \"providers\" : [ { \"aescbc\" : { \"keys\" : [ { \"name\" : \"aescbckey\" , \"secret\" : \"xxxxxxxxxxxxxxxxxxx\" } ] } }, { \"identity\" : {} } ] } ] } Pass the config to the Kubernetes APIServer as encryption-provider-config Once enabled any created secret will be encrypted with this key. Note that if you disable encryption then any encrypted secrets will not be readable until you enable encryption again using the same key.","title":"Secrets Encryption Config"},{"location":"security/secrets_encryption/#secrets-encryption-tool","text":"Available as of v1.21.8+rke2r1 RKE2 contains a utility subcommand secrets-encrypt , which allows administrators to perform the following tasks: Adding new encryption keys Rotating and deleting encryption keys Reencrypting secrets Warning: Failure to follow proper procedure when rotating secrets encryption keys can cause permanent data loss. Proceed with caution.","title":"Secrets Encryption Tool"},{"location":"security/secrets_encryption/#single-server-encryption-key-rotation","text":"To rotate secrets encryption keys on a single-node cluster: Prepare: rke2 secrets-encrypt prepare Restart the kube-apiserver pod: # Get the kube-apiserver container ID export CONTAINER_RUNTIME_ENDPOINT=\"unix:///var/run/k3s/containerd/containerd.sock\" crictl ps --name kube-apiserver # Stop the pod crictl stop <CONTAINER_ID> Rotate: rke2 secrets-encrypt rotate Restart the kube-apiserver pod again Reencrypt: rke2 secrets-encrypt reencrypt","title":"Single-Server Encryption Key Rotation"},{"location":"security/secrets_encryption/#multi-server-encryption-key-rotation","text":"To rotate secrets encryption keys on HA setups: Note: In this example, 3 servers are used to for a HA cluster, referred to as S1, S2, S3. While not required, it is recommended that you pick one server node from which to run the secrets-encrypt commands. Prepare on S1 rke2 secrets-encrypt prepare Sequentially Restart S1, S2, S3 systemctl restart rke2-server.service Wait for the systemctl command to return before restarting the next server. Rotate on S1 rke2 secrets-encrypt rotate Sequentially Restart S1, S2, S3 Reencrypt on S1 rke2 secrets-encrypt reencrypt Wait until reencryption is finished, either via server logs journalctl -u rke2-server or via rke2 secrets-encrypt status . The status will return reencrypt_finished when done. Sequentially Restart S1, S2, S3","title":"Multi-Server Encryption Key Rotation"},{"location":"security/secrets_encryption/#secrets-encryption-status","text":"The secrets-encrypt status subcommand displays information about the current status of secrets encryption on the node. An example of the command on a single-server node: $ rke2 secrets-encrypt status Encryption Status: Enabled Current Rotation Stage: start Server Encryption Hashes: All hashes match Active Key Type Name ------ -------- ---- * AES-CBC aescbckey Another example on HA cluster, after rotating the keys, but before restarting the servers: $ rke2 secrets-encrypt status Encryption Status: Enabled Current Rotation Stage: rotate Server Encryption Hashes: hash does not match between node-1 and node-2 Active Key Type Name ------ -------- ---- * AES-CBC aescbckey-2021-12-10T22:54:38Z AES-CBC aescbckey Details on each section are as follows: Encryption Status : Displayed whether secrets encryption is disabled or enabled on the node Current Rotation Stage : Indicates the current rotation stage on the node. Stages are: start , prepare , rotate , reencrypt_request , reencrypt_active , reencrypt_finished Server Encryption Hashes : Useful for HA clusters, this indicates whether all servers are on the same stage with their local files. This can be used to identify whether a restart of servers is required before proceeding to the next stage. In the HA example above, node-1 and node-2 have different hashes, indicating that they currently do not have the same encryption configuration. Restarting the servers will sync up their configuration. Key Table : Summarizes information about the secrets encryption keys found on the node. Active : The \"*\" indicates which, if any, of the keys are currently used for secrets encryption. An active key is used by Kubernetes to encrypt any new secrets. Key Type : RKE2 only supports the AES-CBC key type. Find more info here. Name : Name of the encryption key.","title":"Secrets Encryption Status"},{"location":"security/selinux/","text":"RKE2 can be run on SELinux-enabled systems which is the default when installed on CentOS/RHEL 7 & 8. The policy supporting this is a specialization of the container-selinux policy for containerd. It accounts for the non-standard location(s) which containerd is installed and places persistent and ephemeral state. Custom Context Labels \u00b6 RKE2 runs control-plane services as static pods which require access to multiple container_var_lib_t locations. The etcd container must be able to read-write under /var/lib/rancher/rke2/server/db and read, along with kube-apiserver , kube-controller-manager , and kube-scheduler , from /var/lib/rancher/rke2/server/tls . To make this work without over-privileging, e.g., spc_t , the RKE2 SELinux policy introduces the rke2_service_db_t and rke2_service_t context labels for read-write and read-only access, respectively. These labels will only be applied to the RKE2 control-plane static pods. Configuration \u00b6 RKE2 support for SELinux amounts to a single configuration item, the --selinux boolean flag. This is a pass-through to the enable_selinux boolean in the cri section of the containerd/cri toml . If RKE2 was installed via tarball then SELinux will not be enabled without additional configuration. The recommended method to configure such is via an entry in the RKE2 config.yaml , e.g.: # /etc/rancher/rke2/config.yaml is the default location selinux : true This is equivalent to passing the --selinux flag to rke2 server or rke2 agent command-line or setting the RKE2_SELINUX=true environment variable.","title":"SELinux"},{"location":"security/selinux/#custom-context-labels","text":"RKE2 runs control-plane services as static pods which require access to multiple container_var_lib_t locations. The etcd container must be able to read-write under /var/lib/rancher/rke2/server/db and read, along with kube-apiserver , kube-controller-manager , and kube-scheduler , from /var/lib/rancher/rke2/server/tls . To make this work without over-privileging, e.g., spc_t , the RKE2 SELinux policy introduces the rke2_service_db_t and rke2_service_t context labels for read-write and read-only access, respectively. These labels will only be applied to the RKE2 control-plane static pods.","title":"Custom Context Labels"},{"location":"security/selinux/#configuration","text":"RKE2 support for SELinux amounts to a single configuration item, the --selinux boolean flag. This is a pass-through to the enable_selinux boolean in the cri section of the containerd/cri toml . If RKE2 was installed via tarball then SELinux will not be enabled without additional configuration. The recommended method to configure such is via an entry in the RKE2 config.yaml , e.g.: # /etc/rancher/rke2/config.yaml is the default location selinux : true This is equivalent to passing the --selinux flag to rke2 server or rke2 agent command-line or setting the RKE2_SELINUX=true environment variable.","title":"Configuration"},{"location":"upgrade/automated_upgrade/","text":"Automated Upgrades \u00b6 Overview \u00b6 You can manage rke2 cluster upgrades using Rancher's system-upgrade-controller. This is a Kubernetes-native approach to cluster upgrades. It leverages a custom resource definition (CRD) , the plan , and a controller that schedules upgrades based on the configured plans. A plan defines upgrade policies and requirements. This documentation will provide plans with defaults appropriate for upgrading a rke2 cluster. For more advanced plan configuration options, please review the CRD . The controller schedules upgrades by monitoring plans and selecting nodes to run upgrade jobs on. A plan defines which nodes should be upgraded through a label selector . When a job has run to completion successfully, the controller will label the node on which it ran accordingly. Note: The upgrade job that is launched must be highly privileged. It is configured with the following: Host IPC , NET , and PID namespaces The CAP_SYS_BOOT capability Host root mounted at /host with read and write permissions For more details on the design and architecture of the system-upgrade-controller or its integration with rke2, see the following Git repositories: system-upgrade-controller rke2-upgrade To automate upgrades in this manner you must: Install the system-upgrade-controller into your cluster Configure plans Install the system-upgrade-controller \u00b6 The system-upgrade-controller can be installed as a deployment into your cluster. The deployment requires a service-account, clusterRoleBinding, and a configmap. To install these components, run the following command: kubectl apply -f https://github.com/rancher/system-upgrade-controller/releases/download/v0.9.1/system-upgrade-controller.yaml The controller can be configured and customized via the previously mentioned configmap, but the controller must be redeployed for the changes to be applied. Configure plans \u00b6 It is recommended that you minimally create two plans: a plan for upgrading server (master / control-plane) nodes and a plan for upgrading agent (worker) nodes. As needed, you can create additional plans to control the rollout of the upgrade across nodes. The following two example plans will upgrade your cluster to rke2 v1.23.1+rke2r2. Once the plans are created, the controller will pick them up and begin to upgrade your cluster. # Server plan apiVersion: upgrade.cattle.io/v1 kind: Plan metadata: name: server-plan namespace: system-upgrade labels: rke2-upgrade: server spec: concurrency: 1 nodeSelector: matchExpressions: - {key: rke2-upgrade, operator: Exists} - {key: rke2-upgrade, operator: NotIn, values: [\"disabled\", \"false\"]} # When using k8s version 1.19 or older, swap control-plane with master - {key: node-role.kubernetes.io/control-plane, operator: In, values: [\"true\"]} serviceAccountName: system-upgrade tolerations: - key: CriticalAddonsOnly operator: Exists cordon: true # drain: # force: true upgrade: image: rancher/rke2-upgrade version: v1.23.1-rke2r2 --- # Agent plan apiVersion: upgrade.cattle.io/v1 kind: Plan metadata: name: agent-plan namespace: system-upgrade labels: rke2-upgrade: agent spec: concurrency: 2 nodeSelector: matchExpressions: - {key: rke2-upgrade, operator: Exists} - {key: rke2-upgrade, operator: NotIn, values: [\"disabled\", \"false\"]} # When using k8s version 1.19 or older, swap control-plane with master - {key: node-role.kubernetes.io/control-plane, operator: NotIn, values: [\"true\"]} prepare: args: - prepare - server-plan image: rancher/rke2-upgrade serviceAccountName: system-upgrade cordon: true drain: force: true upgrade: image: rancher/rke2-upgrade version: v1.23.1-rke2r2 There are a few important things to call out regarding these plans: The plans must be created in the same namespace where the controller was deployed. The concurrency field indicates how many nodes can be upgraded at the same time. The server-plan targets server nodes by specifying a label selector that selects nodes with the node-role.kubernetes.io/control-plane label ( node-role.kubernetes.io/master for 1.19 or older). The agent-plan targets agent nodes by specifying a label selector that select nodes without that label. Optionally, additional labels can be included, like in the example above, which requires label \"rke2-upgrade\" to exist and not have the value \"disabled\" or \"false\". The prepare step in the agent-plan will cause upgrade jobs for that plan to wait for the server-plan to complete before they execute. Both plans have the version field set to v1.23.1+rke2r2. Alternatively, you can omit the version field and set the channel field to a URL that resolves to a release of rke2. This will cause the controller to monitor that URL and upgrade the cluster any time it resolves to a new release. This works well with the release channels . Thus, you can configure your plans with the following channel to ensure your cluster is always automatically upgraded to the newest stable release of rke2: apiVersion: upgrade.cattle.io/v1 kind: Plan ... spec: ... channel: https://update.rke2.io/v1-release/channels/stable As stated, the upgrade will begin as soon as the controller detects that a plan was created. Updating a plan will cause the controller to re-evaluate the plan and determine if another upgrade is needed. You can monitor the progress of an upgrade by viewing the plan and jobs via kubectl: kubectl -n system-upgrade get plans -o yaml kubectl -n system-upgrade get jobs -o yaml","title":"Automated Upgrades"},{"location":"upgrade/automated_upgrade/#automated-upgrades","text":"","title":"Automated Upgrades"},{"location":"upgrade/automated_upgrade/#overview","text":"You can manage rke2 cluster upgrades using Rancher's system-upgrade-controller. This is a Kubernetes-native approach to cluster upgrades. It leverages a custom resource definition (CRD) , the plan , and a controller that schedules upgrades based on the configured plans. A plan defines upgrade policies and requirements. This documentation will provide plans with defaults appropriate for upgrading a rke2 cluster. For more advanced plan configuration options, please review the CRD . The controller schedules upgrades by monitoring plans and selecting nodes to run upgrade jobs on. A plan defines which nodes should be upgraded through a label selector . When a job has run to completion successfully, the controller will label the node on which it ran accordingly. Note: The upgrade job that is launched must be highly privileged. It is configured with the following: Host IPC , NET , and PID namespaces The CAP_SYS_BOOT capability Host root mounted at /host with read and write permissions For more details on the design and architecture of the system-upgrade-controller or its integration with rke2, see the following Git repositories: system-upgrade-controller rke2-upgrade To automate upgrades in this manner you must: Install the system-upgrade-controller into your cluster Configure plans","title":"Overview"},{"location":"upgrade/automated_upgrade/#install-the-system-upgrade-controller","text":"The system-upgrade-controller can be installed as a deployment into your cluster. The deployment requires a service-account, clusterRoleBinding, and a configmap. To install these components, run the following command: kubectl apply -f https://github.com/rancher/system-upgrade-controller/releases/download/v0.9.1/system-upgrade-controller.yaml The controller can be configured and customized via the previously mentioned configmap, but the controller must be redeployed for the changes to be applied.","title":"Install the system-upgrade-controller"},{"location":"upgrade/automated_upgrade/#configure-plans","text":"It is recommended that you minimally create two plans: a plan for upgrading server (master / control-plane) nodes and a plan for upgrading agent (worker) nodes. As needed, you can create additional plans to control the rollout of the upgrade across nodes. The following two example plans will upgrade your cluster to rke2 v1.23.1+rke2r2. Once the plans are created, the controller will pick them up and begin to upgrade your cluster. # Server plan apiVersion: upgrade.cattle.io/v1 kind: Plan metadata: name: server-plan namespace: system-upgrade labels: rke2-upgrade: server spec: concurrency: 1 nodeSelector: matchExpressions: - {key: rke2-upgrade, operator: Exists} - {key: rke2-upgrade, operator: NotIn, values: [\"disabled\", \"false\"]} # When using k8s version 1.19 or older, swap control-plane with master - {key: node-role.kubernetes.io/control-plane, operator: In, values: [\"true\"]} serviceAccountName: system-upgrade tolerations: - key: CriticalAddonsOnly operator: Exists cordon: true # drain: # force: true upgrade: image: rancher/rke2-upgrade version: v1.23.1-rke2r2 --- # Agent plan apiVersion: upgrade.cattle.io/v1 kind: Plan metadata: name: agent-plan namespace: system-upgrade labels: rke2-upgrade: agent spec: concurrency: 2 nodeSelector: matchExpressions: - {key: rke2-upgrade, operator: Exists} - {key: rke2-upgrade, operator: NotIn, values: [\"disabled\", \"false\"]} # When using k8s version 1.19 or older, swap control-plane with master - {key: node-role.kubernetes.io/control-plane, operator: NotIn, values: [\"true\"]} prepare: args: - prepare - server-plan image: rancher/rke2-upgrade serviceAccountName: system-upgrade cordon: true drain: force: true upgrade: image: rancher/rke2-upgrade version: v1.23.1-rke2r2 There are a few important things to call out regarding these plans: The plans must be created in the same namespace where the controller was deployed. The concurrency field indicates how many nodes can be upgraded at the same time. The server-plan targets server nodes by specifying a label selector that selects nodes with the node-role.kubernetes.io/control-plane label ( node-role.kubernetes.io/master for 1.19 or older). The agent-plan targets agent nodes by specifying a label selector that select nodes without that label. Optionally, additional labels can be included, like in the example above, which requires label \"rke2-upgrade\" to exist and not have the value \"disabled\" or \"false\". The prepare step in the agent-plan will cause upgrade jobs for that plan to wait for the server-plan to complete before they execute. Both plans have the version field set to v1.23.1+rke2r2. Alternatively, you can omit the version field and set the channel field to a URL that resolves to a release of rke2. This will cause the controller to monitor that URL and upgrade the cluster any time it resolves to a new release. This works well with the release channels . Thus, you can configure your plans with the following channel to ensure your cluster is always automatically upgraded to the newest stable release of rke2: apiVersion: upgrade.cattle.io/v1 kind: Plan ... spec: ... channel: https://update.rke2.io/v1-release/channels/stable As stated, the upgrade will begin as soon as the controller detects that a plan was created. Updating a plan will cause the controller to re-evaluate the plan and determine if another upgrade is needed. You can monitor the progress of an upgrade by viewing the plan and jobs via kubectl: kubectl -n system-upgrade get plans -o yaml kubectl -n system-upgrade get jobs -o yaml","title":"Configure plans"},{"location":"upgrade/basic_upgrade/","text":"Upgrade Basics \u00b6 You can upgrade rke2 by using the installation script, or by manually installing the binary of the desired version. Note: Upgrade the server nodes first, one at a time. Once all servers have been upgraded, you may then upgrade agent nodes. Release Channels \u00b6 Upgrades performed via the installation script or using our automated upgrades feature can be tied to different release channels. Currently, the latest channel is the only available channel. Once we have more releases and need to distinguish between the most recent release and the most stable release, we will add a stable channel and set it as the default. For an exhaustive and up-to-date list of channels, you can visit the rke2 channel service API . For more technical details on how channels work, you can see the channelserver project . Upgrade rke2 Using the Installation Script \u00b6 To upgrade rke2 from an older version you can re-run the installation script using the same flags, for example: curl -sfL https://get.rke2.io | sh - This will upgrade to the most recent version in the stable channel by default. If you want to upgrade to the most recent version in a specific channel (such as latest) you can specify the channel: curl -sfL https://get.rke2.io | INSTALL_RKE2_CHANNEL = latest sh - If you want to upgrade to a specific version you can run the following command: curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION = vX.Y.Z+rke2rN sh - Remember to restart the rke2 process after installing: # Server nodes: systemctl restart rke2-server # Agent nodes: systemctl restart rke2-agent Manually Upgrade rke2 Using the Binary \u00b6 Or to manually upgrade rke2: Download the desired version of the rke2 binary from releases Copy the downloaded binary to /usr/local/bin/rke2 for tarball installed rke2, and /usr/bin for rpm installed rke2 Stop the old rke2 binary Launch the new rke2 binary Restarting rke2 \u00b6 Restarting rke2 is supported by the installation script for systemd. systemd To restart servers manually: sudo systemctl restart rke2-server To restart agents manually: sudo systemctl restart rke2-agent","title":"Upgrade Basics"},{"location":"upgrade/basic_upgrade/#upgrade-basics","text":"You can upgrade rke2 by using the installation script, or by manually installing the binary of the desired version. Note: Upgrade the server nodes first, one at a time. Once all servers have been upgraded, you may then upgrade agent nodes.","title":"Upgrade Basics"},{"location":"upgrade/basic_upgrade/#release-channels","text":"Upgrades performed via the installation script or using our automated upgrades feature can be tied to different release channels. Currently, the latest channel is the only available channel. Once we have more releases and need to distinguish between the most recent release and the most stable release, we will add a stable channel and set it as the default. For an exhaustive and up-to-date list of channels, you can visit the rke2 channel service API . For more technical details on how channels work, you can see the channelserver project .","title":"Release Channels"},{"location":"upgrade/basic_upgrade/#upgrade-rke2-using-the-installation-script","text":"To upgrade rke2 from an older version you can re-run the installation script using the same flags, for example: curl -sfL https://get.rke2.io | sh - This will upgrade to the most recent version in the stable channel by default. If you want to upgrade to the most recent version in a specific channel (such as latest) you can specify the channel: curl -sfL https://get.rke2.io | INSTALL_RKE2_CHANNEL = latest sh - If you want to upgrade to a specific version you can run the following command: curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION = vX.Y.Z+rke2rN sh - Remember to restart the rke2 process after installing: # Server nodes: systemctl restart rke2-server # Agent nodes: systemctl restart rke2-agent","title":"Upgrade rke2 Using the Installation Script"},{"location":"upgrade/basic_upgrade/#manually-upgrade-rke2-using-the-binary","text":"Or to manually upgrade rke2: Download the desired version of the rke2 binary from releases Copy the downloaded binary to /usr/local/bin/rke2 for tarball installed rke2, and /usr/bin for rpm installed rke2 Stop the old rke2 binary Launch the new rke2 binary","title":"Manually Upgrade rke2 Using the Binary"},{"location":"upgrade/basic_upgrade/#restarting-rke2","text":"Restarting rke2 is supported by the installation script for systemd. systemd To restart servers manually: sudo systemctl restart rke2-server To restart agents manually: sudo systemctl restart rke2-agent","title":"Restarting rke2"},{"location":"upgrade/upgrade/","text":"Upgrading RKE2 Clusters \u00b6 This section describes how to upgrade your rke2 cluster. Upgrade basics describes several techniques for upgrading your cluster manually. It can also be used as a basis for upgrading through third-party Infrastructure-as-Code tools like Terraform . Automated upgrades describes how to perform Kubernetes-native automated upgrades using Rancher's system-upgrade-controller .","title":"Overview"},{"location":"upgrade/upgrade/#upgrading-rke2-clusters","text":"This section describes how to upgrade your rke2 cluster. Upgrade basics describes several techniques for upgrading your cluster manually. It can also be used as a basis for upgrading through third-party Infrastructure-as-Code tools like Terraform . Automated upgrades describes how to perform Kubernetes-native automated upgrades using Rancher's system-upgrade-controller .","title":"Upgrading RKE2 Clusters"}]}